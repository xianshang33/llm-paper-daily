<h2 align='center'>llm-paper-daily æ—¥å¸¸è®ºæ–‡ç²¾é€‰</h2>
<div align='center'>

[![Status](https://img.shields.io/badge/status-Update_01.18_13:05-success.svg)]() [![ç®€ä½“ä¸­æ–‡ badge](https://img.shields.io/badge/%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87-Simplified%20Chinese-blue)](./README.md) [![English badge](https://img.shields.io/badge/%E8%8B%B1%E6%96%87-English-blue)](./README_en.md) 

</div>

æ¬¢è¿æ¥åˆ° **llm-paper-daily**! è¿™æ˜¯ä¸€ä¸ªè·å–æœ€æ–°ç ”ç©¶è®ºæ–‡çš„æ¯æ—¥æ›´æ–°å’Œåˆ†ç±»çš„å¹³å°ã€‚å¸Œæœ›ä¸ºçˆ±å¥½è€…æä¾› LLM ç ”ç©¶çš„å‰æ²¿èµ„è®¯ï¼Œè®©æ‚¨æ›´è½»æ¾åœ°äº†è§£è¯¥é¢†åŸŸçš„æœ€æ–°å‘å±•ã€‚

ğŸ“š **æ¯æ—¥æ›´æ–°:** ä»“åº“æ¯å¤©ä¼šå¸¦æ¥æœ€æ–°çš„ LLM ç ”ç©¶ï¼Œå¹¶é™„æœ‰arxivåœ°å€ã€ç›¸å…³ git ä»“åº“å’ŒåŸºäº GPT-4 çš„ç®€å•æ€»ç»“

ğŸ’ **åˆ†ç±»æ‘˜è¦:** å°†æ¯ç¯‡è®ºæ–‡åˆ†ç±»åˆ°å¦‚æ¨ç†ã€ä»£ç†ã€æ£€ç´¢ã€åº”ç”¨ã€é¢„è®­ç»ƒä¸æŒ‡ä»¤å¾®è°ƒç­‰ä¸åŒéƒ¨åˆ†ï¼Œå¸®åŠ©æ‚¨èƒ½è½»æ¾å¯¼èˆªå¹¶å‘ç°ç›¸å…³çš„ç ”ç©¶

ğŸŒˆ **å¾é›†è´¡çŒ®:** æ¬¢è¿å¤§å®¶åšå‡ºè´¡çŒ®ï¼å¸Œæœ›æ‚¨å¯ä»¥ ğŸ™Œ å°†æ”¶é›†çš„å¥½æ–‡å’Œå…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„åˆ†ç±»æ–‡ç« æäº¤ **Pull Requests**

## ç›®å½•
- [æœ€æ–°è®ºæ–‡(å«æ€»ç»“)](#æœ€æ–°è®ºæ–‡)
- [åˆ†ç±»](#åˆ†ç±»)
  - [ğŸ’¡ Reasoning](#Reasoning)
  - [ğŸ¤– Agent](#Agent)
  - [ğŸ¦‰ Knowledge and Retrieval](#Knowledge-and-Retrieval)
  - [ğŸ‘©â€ğŸ« Alignment and Hallucination](#Alignment-and-Hallucination)
  - [ğŸ¨ Application](#Application)
  - [ğŸ“ Pre-training and Instruction Fine-tuning](#Pre-training-and-Instruction-Fine-tuning)
  - [ğŸ“„ Survey](#Survey)
  - [ğŸ† Top Conferences](#Top-Conferences)

<details>
  <summary>æŸ¥çœ‹æ›´æ–°æ–‡ç«  &nbsp;&nbsp;<sub>æ›´æ–°æ—¶é—´: 01æœˆ18æ—¥ 13:05</sub></summary>
<br>

- Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges 
- SpecGen: Automated Generation of Formal Program Specifications via Large Language Models 
- ReFT: Reasoning with Reinforced Fine-Tuning 
- LLMs for Relational Reasoning: How Far are We? 
- Vlogger: Make Your Dream A Vlog 
</details>

## æœ€æ–°è®ºæ–‡
### 01æœˆ

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>01-17</span> | **Vlogger: Make Your Dream A Vlog**<br><sub>æœºæ„: Shanghai Jiao Tong University, Shanghai AI Laboratory, Shenzhen Institute of Advanced Technology Chinese Academy of Sciences<br>æœ¬è®ºæ–‡é€šè¿‡ä»‹ç»Vloggerç³»ç»Ÿï¼Œå±•ç¤ºäº†ä¸€ä¸ªåˆ›æ–°çš„åŠæ³•å°†LLMsåº”ç”¨äºè§†é¢‘åšå®¢çš„ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œä»è€Œå…‹æœäº†ç”Ÿæˆåˆ†é’Ÿçº§è¿è´¯è§†é¢‘å†…å®¹çš„æŒ‘æˆ˜ï¼Œå¹¶å–å¾—äº†ä¼˜å¼‚çš„å®éªŒç»“æœã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.09414v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.09414.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/zhuangshaobin/Vlogger)</div> |
| <span style='display: inline-block; width: 42px;'>01-17</span> | **LLMs for Relational Reasoning: How Far are We?**<br><sub>æœºæ„: Continental-NTU Corporate Lab, Nanyang Technological University, Singapore<br>æœ¬è®ºæ–‡ä¸»è¦æ¢è®¨äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…³ç³»æ¨ç†æ–¹é¢çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚é€šè¿‡å…¨é¢çš„è¯„ä¼°ï¼ŒåŒ…æ‹¬æ–°æå‡ºçš„æµ‹è¯•æ–¹æ³•å’Œè¯„ä¼°æ¨¡å—ï¼Œå‘ç°LLMsè™½ç„¶åœ¨æŸäº›å…³ç³»æ¨ç†ä»»åŠ¡ä¸Šè¡¨ç°ä¸é”™ï¼Œä½†ä¸ä¸“é—¨ä¸ºé€»è¾‘æ¨ç†è®¾è®¡çš„æ¨¡å‹ç›¸æ¯”ï¼Œå…¶æ€§èƒ½ç›¸å¯¹è¾ƒå·®ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.09042v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.09042.md)  |
| <span style='display: inline-block; width: 42px;'>01-17</span> | **ReFT: Reasoning with Reinforced Fine-Tuning**<br><sub>æœºæ„: ByteDance Research<br>ReFTé€šè¿‡åˆ©ç”¨å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–éå¯å¾®ç›®æ ‡ï¼Œæ˜¾è‘—æé«˜äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•°å­¦é—®é¢˜æ±‚è§£ä»»åŠ¡ä¸­çš„æ€§èƒ½å’Œæ³›åŒ–èƒ½åŠ›ã€‚å®ƒè¶…è¶Šäº†ä¼ ç»Ÿçš„ç›‘ç£å¼å­¦ä¹ æ–¹æ³•ï¼Œå±•ç°äº†åœ¨æ›´å¤æ‚æ¨ç†ä»»åŠ¡ä¸­çš„æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08967v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.08967.md)  |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline**<br><sub>æœºæ„: Alibaba Group  <br>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°å­¦æ¨ç†æ•°æ®é›†ï¼Œè¯¥æ•°æ®é›†ä¸Pythonä»£ç è§£é‡Šå™¨ç›¸ç»“åˆï¼Œé€šè¿‡æ”¹è¿›æ•°æ®é›†å¹¶å®æ–½ç‰¹å®šå¾®è°ƒæµç¨‹æ˜¾è‘—æé«˜äº†LLMåœ¨æ•°å­¦é—®é¢˜æ±‚è§£ä»»åŠ¡ä¸Šçš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08190v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.0819.md)  |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation**<br><sub>æœºæ„: Johns Hopkins University, Microsoft<br>æœ¬è®ºæ–‡æå‡ºäº†CPOï¼Œä¸€ä¸ªæ–°çš„LLMå¾®è°ƒæ–¹æ³•ï¼Œæœ‰æ•ˆè§£å†³äº†SFTåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­å­˜åœ¨çš„ç“¶é¢ˆï¼Œå®ç°äº†åœ¨èµ„æºæ¶ˆè€—æå°‘çš„æƒ…å†µä¸‹æ˜¾è‘—æå‡ä¸­ç­‰è§„æ¨¡LLMç¿»è¯‘æ¨¡å‹çš„æ€§èƒ½ï¼Œæœ€ç»ˆä¸æœ€å…ˆè¿›çš„çŠ¶æ€è‰ºæœ¯ç¿»è¯‘ç³»ç»Ÿé½å¤´å¹¶è¿›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08417v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.08417.md)  |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models**<br><sub>æœºæ„:  Zhejiang University<br>DoraemonGPTæ˜¯ä¸€ä¸ªLLMé©±åŠ¨çš„æ™ºèƒ½ä½“ï¼Œé€šè¿‡ç¬¦å·è®°å¿†å’Œå·¥å…·é›†æ¥ç†è§£å¹¶è§£ç­”æ¶‰åŠåŠ¨æ€è§†é¢‘çš„å¤æ‚é—®é¢˜ã€‚å…¶é‡‡ç”¨äº†MCTSè§„åˆ’å™¨ä¼˜åŒ–å›ç­”çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œèƒ½å¤Ÿåœ¨çœŸå®ä¸–ç•Œåœºæ™¯ä¸­å¤„ç†æ›´ä¸ºå¤æ‚çš„ä»»åŠ¡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08392v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.08392.md)  |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models**<br><sub>æœºæ„: Tencent AI Lab<br>æœ¬æ–‡æ·±å…¥åˆ†æäº†LLMsåœ¨æœºå™¨ç¿»è¯‘ä»»åŠ¡ä¸­é¢†åŸŸä¸åŒ¹é…é—®é¢˜ï¼Œå¹¶å®éªŒäº†ä¸åŒæ•°é‡çš„å¹³è¡Œæ•°æ®å¯¹LLMsç¿»è¯‘èƒ½åŠ›çš„å½±å“ï¼Œå±•ç°å‡ºLLMsåœ¨å¤„ç†è¿™äº›æŒ‘æˆ˜ä¸­çš„æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08350v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.0835.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/pangjh3/LLM4MT)</div> |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture**<br><sub>æœºæ„: Microsoft<br>æœ¬æ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å†œä¸šæ•°æ®ä¸Šç”Ÿæˆé—®ç­”å¯¹çš„æ€§èƒ½ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæ–°çš„ç”Ÿæˆç®¡é“ï¼Œæœ‰æ•ˆåœ°ä½¿ç”¨äº†RAGå’Œå¾®è°ƒæŠ€æœ¯å¢å¼ºLLMçš„åº”ç”¨åœºæ™¯ï¼Œæ‹“å±•äº†LLMåœ¨ç‰¹å®šè¡Œä¸šçš„åº”ç”¨æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08406v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.08406.md)  |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **SpecGen: Automated Generation of Formal Program Specifications via Large Language Models**<br><sub>æœºæ„: Nanjing University, Nanyang Technological University, Singapore Management University<br>è®ºæ–‡æå‡ºäº† SpecGenï¼Œä¸€ä¸ªç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹å’Œå¯å‘å¼é€‰æ‹©ç­–ç•¥çš„ç¨‹åºå½¢å¼åŒ–è§„èŒƒè‡ªåŠ¨ç”ŸæˆæŠ€æœ¯ã€‚é€šè¿‡æ¯”è¾ƒä¸ç°æœ‰å·¥å…·å’Œçº¯ LLM æ–¹æ³•ï¼ŒSpecGen è¡¨ç°å‡ºæ›´é«˜æ•ˆå’Œå‡†ç¡®çš„ç”Ÿæˆè§„èŒƒçš„èƒ½åŠ›ï¼Œå¹¶ä¸”æå‡ºäº†æ•°æ®é›†ä¿ƒè¿›åç»­ç ”ç©¶ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08807v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.08807.md)  |
| <span style='display: inline-block; width: 42px;'>01-15</span> | **MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models**<br><sub>æœºæ„: Microsoft Research India<br>è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šè¯­è¨€ä»»åŠ¡ä¸Šé€šè¿‡å‚æ•°é«˜æ•ˆå¾®è°ƒåçš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨ä½èµ„æºè¯­è¨€å’Œè‹±è¯­ä»»åŠ¡ä¸Šã€‚ç ”ç©¶å±•ç¤ºäº†PEFTçš„æ½œåŠ›ï¼ŒåŒæ—¶æŒ‡å‡ºäº†æœªæ¥å·¥ä½œçš„ä¸€äº›å¯èƒ½æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07598v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.07598.md)  |
| <span style='display: inline-block; width: 42px;'>01-15</span> | **The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey**<br><sub>æœºæ„: Technology Innovation Institute UAE, Islamic University of Technology Bangladesh, Stanford University, Amazon GenAI, AI Institute University of South Carolina<br>æœ¬è®ºæ–‡æ˜¯å…³äºLLMsä¸Šä¸‹æ–‡é•¿åº¦æ‰©å±•æŠ€æœ¯çš„è¯¦ç»†è°ƒç ”ã€‚å®ƒä¸ºç ”ç©¶äººå‘˜æä¾›äº†è¯¥é¢†åŸŸçš„ç°æœ‰ç­–ç•¥å’ŒæŒ‘æˆ˜çš„æœ‰ç»„ç»‡æ¦‚è§ˆï¼Œå¹¶é¼“åŠ±äº†å¯¹æœªæ¥å‘å±•çš„è®¨è®ºã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07872v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.07872.md)  |
| <span style='display: inline-block; width: 42px;'>01-15</span> | **A Study on Large Language Models' Limitations in Multiple-Choice Question Answering**<br><sub>æœºæ„: David R. Cheriton School of Computer Science<br>è¯¥è®ºæ–‡é’ˆå¯¹LLMsåœ¨MCQä»»åŠ¡ä¸­çš„é™åˆ¶è¿›è¡Œäº†ç ”ç©¶ï¼ŒæŒ‡å‡ºå¤šæ•°æ¨¡å‹åœ¨æ­¤ç±»ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³ã€‚è®ºæ–‡è¿˜å‘ç°æ¨¡å‹çš„å›ç­”å¾€å¾€ä¾èµ–äºé€‰é¡¹é¡ºåºï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„è¯„ä¼°æ–¹æ³•æ¥æ’é™¤è¿™äº›åè§ã€‚è®ºæ–‡æ¨èåœ¨ä½¿ç”¨MCQè¯„ä¼°LLMsæ—¶è¦æ ¼å¤–å°å¿ƒï¼Œå¹¶æµ‹è¯•æ¨¡å‹æ˜¯å¦çœŸæ­£ç†è§£äº†ä»»åŠ¡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07955v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.07955.md)  |
| <span style='display: inline-block; width: 42px;'>01-14</span> | **Small LLMs Are Weak Tool Learners: A Multi-LLM Agent**<br><sub>æœºæ„: Sun Yat-sen University, Alibaba Group<br>ç ”ç©¶è¡¨æ˜å°å‹LLMåœ¨ä½œä¸ºå·¥å…·å­¦ä¹ è€…æ–¹é¢è¾ƒä¸ºè–„å¼±ï¼Œé€šè¿‡å¼•å…¥Î±-UMiå¤šLLMæ¡†æ¶æ¥æ„å»ºæ€§èƒ½æ›´ä¼˜çš„LLMä»£ç†ï¼Œæå‡ºäº†å¿…è¦çš„åŒé˜¶æ®µå¾®è°ƒç­–ç•¥ï¼Œå¹¶æ·±å…¥åˆ†æäº†æ•°æ®è§„æ¨¡æ³•åˆ™ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07324v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.07324.md)  |
| <span style='display: inline-block; width: 42px;'>01-13</span> | **Bridging the Preference Gap between Retrievers and LLMs**<br><sub>æœ¬è®ºæ–‡ä»‹ç»äº†BGMæ¡†æ¶ä»¥è§£å†³æ£€ç´¢å™¨å’ŒLLMsä¹‹é—´çš„"åå¥½å·®"é—®é¢˜ï¼Œé€šè¿‡ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—ï¼ˆseq2seqï¼‰çš„æ¡¥æ¨¡å‹ç»“åˆSLå’ŒRLçš„è®­ç»ƒæ–¹æ¡ˆï¼Œä¼˜åŒ–äº†æ£€ç´¢ä¿¡æ¯ä»¥æ»¡è¶³LLMsçš„åå¥½ï¼Œæ”¹è¿›äº†å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡çš„è¡¨ç°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06954v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06954.md)  |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **TestSpark: IntelliJ IDEA's Ultimate Test Generation Companion**<br><sub>æœºæ„: JetBrains Research, Delft University of Technology<br>è®ºæ–‡æå‡ºäº†TestSparkæ’ä»¶ï¼Œå®ƒç»“åˆäº†åŸºäºæœç´¢çš„è½¯ä»¶æµ‹è¯•ç”Ÿæˆå’ŒåŸºäºè¯­è¨€æ¨¡å‹çš„æµ‹è¯•ç”Ÿæˆæ–¹æ³•ï¼Œåœ¨IntelliJ IDEAä¸­æé«˜äº†å•å…ƒæµ‹è¯•çš„ç”Ÿæˆå’Œé›†æˆæ•ˆç‡ï¼ŒåŒæ—¶è§£å†³äº†LLMç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å¯ç¼–è¯‘æ€§çš„é—®é¢˜ã€‚æ’ä»¶çš„å¼€æºç‰¹æ€§ä½¿å…¶æˆä¸ºè¿æ¥è½¯ä»¶å¼€å‘è€…å’Œç ”ç©¶è€…çš„æ¡¥æ¢ï¼Œæœ‰åŠ©äºæµ‹è¯•ç”ŸæˆæŠ€æœ¯çš„å®ç”¨æ€§è¿›æ­¥ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06580v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.0658.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/JetBrains-Research/TestSpark)</div> |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation**<br><sub>æœºæ„: Nanyang Technological University, Fudan University<br>è¯¥è®ºæ–‡æˆåŠŸæå‡ºäº†ä¸€ç§æ–°æ–¹æ³•TOOLGENï¼Œé€šè¿‡é›†æˆè‡ªåŠ¨å®Œæˆå·¥å…·åˆ°ä»“åº“çº§ä»£ç ç”Ÿæˆä¸­çš„LLMsï¼Œè§£å†³äº†ä¾èµ–æ€§é—®é¢˜ï¼Œæé«˜äº†ä»£ç ç”Ÿæˆçš„è´¨é‡å’ŒæˆåŠŸç‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06391v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06391.md)  |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs**<br><sub>æœºæ„: Virginia Tech, Renmin University of China, UC Davis<br>æœ¬è®ºæ–‡æå‡ºäº†å°†LLMsè§†ä¸ºå…·å¤‡ç±»äººæ²Ÿé€šèƒ½åŠ›çš„å®ä½“ï¼Œåˆ©ç”¨äº†ä¸€ä¸ªæ–°çš„è§†è§’æ¥ç ”ç©¶AIå®‰å…¨é—®é¢˜ã€‚é€šè¿‡å°†åå¤šå¹´çš„ç¤¾ä¼šç§‘å­¦ç ”ç©¶åº”ç”¨äºAIå®‰å…¨ï¼Œåˆ¶å®šäº†ä¸€ä¸ªè¯´æœæŠ€å·§åˆ†ç±»æ³•ï¼Œå¹¶é€šè¿‡åˆ›å»ºçš„å·¥å…·è‡ªåŠ¨ç”Ÿæˆäº†å¯¹æŠ—æ€§æç¤ºã€‚ç»“æœè¡¨æ˜ï¼Œè¯´æœæŠ€å·§å¯ä»¥æœ‰æ•ˆåœ°å¢å¼ºæœ‰é£é™©è¡Œä¸ºè¢«LLMsæ‰§è¡Œçš„å¯èƒ½æ€§ï¼ŒåŒæ—¶æ­ç¤ºäº†å½“å‰é˜²å¾¡æ‰‹æ®µåœ¨åº”å¯¹è¿™ç±»ç­–ç•¥æ—¶çš„ä¸è¶³ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06373v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06373.md)  |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape**<br><sub>æœºæ„: Tsinghua University, University of Maryland, Beijing Xicheng Educational Research Institute  <br>æœ¬æ–‡çš„ç ”ç©¶å±•ç°äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ•™è‚²é¢†åŸŸä¸­ï¼Œç‰¹åˆ«æ˜¯åœ¨AESç³»ç»Ÿä¸­çš„æ½œåŠ›ã€‚LLMsä¸ä»…èƒ½å¤Ÿè‡ªåŠ¨åŒ–è¯„åˆ†è¿‡ç¨‹ï¼Œè¿˜èƒ½å¤Ÿé€šè¿‡ç”Ÿæˆåé¦ˆæ¥å¢å¼ºäººç±»è¯„åˆ†è€…çš„è¡¨ç°ã€‚è¿™ä¸ä»…æ˜¯æŠ€æœ¯ä¸Šçš„è¿›æ­¥ï¼Œæ›´ä¸ºæœªæ¥çš„äººå·¥æ™ºèƒ½è¾…åŠ©æ•™è‚²å’Œäººå·¥æ™ºèƒ½ä¸äººç±»çš„é«˜æ•ˆåä½œæä¾›äº†å®è´µè§è§£ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06431v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06431.md)  |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation**<br><sub>æœºæ„: Tianyu Zheng, Shuyue Guo, Xingwei Qu, Jiawei Guo, Weixu Zhang, Xinrun Du, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu, Ge Zhang<br>è¿™ç¯‡è®ºæ–‡æå‡ºäº†Kunç­–ç•¥ï¼Œè§£å†³äº†ä¸­æ–‡å¤§å‹è¯­è¨€æ¨¡å‹æŒ‡ä»¤å¾®è°ƒä¸­å­˜åœ¨çš„æ•°æ®ä¸€è‡´æ€§é—®é¢˜ï¼Œé€šè¿‡APè¿‡ç¨‹å’Œæ–°çš„æ•°æ®ç”Ÿæˆæ–¹æ³•ï¼Œå‡å°‘äº†å¯¹äººå·¥æ ‡æ³¨çš„ä¾èµ–ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼ŒKunç­–ç•¥åœ¨åˆ›å»ºé«˜è´¨é‡æ•°æ®é›†æ–¹é¢å…·æœ‰æ˜æ˜¾ä¼˜åŠ¿ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06477v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06477.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/Zheng0428/COIG-Kun)</div> |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding**<br><sub>æœºæ„: Tsinghua University, Zhipu AI<br>é€šè¿‡å®æ–½APARï¼Œè¯¥ç ”ç©¶æˆåŠŸæé«˜äº†LLMsåœ¨å†…å­˜å—é™åœºæ™¯å’Œé«˜ååç‡åœºæ™¯ä¸‹çš„è§£ç æ•ˆç‡å’Œç”Ÿæˆé€Ÿåº¦ï¼ŒåŒæ—¶ä¿æŒäº†ç”Ÿæˆè´¨é‡ï¼Œä¸ºå¤§è¯­è¨€æ¨¡å‹çš„éƒ¨ç½²æä¾›äº†ä¸€ç§æ–°çš„é«˜æ•ˆç­–ç•¥ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06761v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06761.md)  |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models**<br><sub>æœºæ„: University of Washington Seattle, University of Wisconsin-Madison, Stanford University<br>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå®éªŒè®¾è®¡æ¡†æ¶ï¼Œä¸ºäº†æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç›‘ç£å¼å¾®è°ƒï¼ˆSFTï¼‰è¿‡ç¨‹ä¸­çš„æ ‡ç­¾æ•ˆç‡ã€‚å®ƒå±•ç¤ºäº†å®éªŒè®¾è®¡æŠ€æœ¯å¯ä»¥åœ¨ç»´æŒä½è®¡ç®—æˆæœ¬çš„åŒæ—¶ï¼Œå¤§å¹…æé«˜æ ‡ç­¾æ•ˆç‡ï¼Œåœ¨ä¸€äº›ä»»åŠ¡ä¸­ä¸éšæœºé‡‡æ ·ç›¸æ¯”èŠ‚çœäº†50%çš„æ³¨é‡Šæˆæœ¬ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06692v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06692.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion**<br><sub>æœºæ„: Tsinghua Shenzhen International Graduate School Tsinghua University, School of Computer Science Peking University, Baidu Inc.<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œæ—¶é—´çŸ¥è¯†å›¾è°±å®Œæˆçš„æ–¹æ³•ï¼Œé€šè¿‡é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•å’Œç»“åˆç»“æ„ä¿¡æ¯çš„å†å²æ•°æ®å¢å¼ºï¼Œæé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›å’Œæ€§èƒ½ã€‚å®éªŒæ˜¾ç¤ºè¯¥æ–¹æ³•æœ‰æ•ˆåœ°æå‡äº†æ—¶é—´çŸ¥è¯†å›¾è°±é¢„æµ‹çš„ç²¾åº¦ï¼Œè¾¾åˆ°äº†æœ€å…ˆè¿›çš„ç»“æœã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06072v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06072.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase**<br><sub>æœºæ„: LAIR Lab Lehigh University, Huazhong University of Science and Technology  <br>æœ¬æ–‡å®šä¹‰äº†æ··åˆåœºæ™¯ä¸­çš„æ··åˆæ–‡æœ¬ï¼ˆmixcaseï¼‰ï¼Œæ„å»ºäº†MIXSETæ•°æ®é›†ï¼Œå¹¶æå‡ºäº†é€šå‘è§£å†³æ··åˆæ–‡æœ¬æ£€æµ‹é—®é¢˜çš„è§è§£å’Œæ–¹å‘ã€‚ç ”ç©¶å‘ç°ï¼Œç°æœ‰çš„æ£€æµ‹å™¨åœ¨è¯†åˆ«æ··åˆæ–‡æœ¬æ–¹é¢å­˜åœ¨ä¸è¶³ï¼Œè¿™æå‡ºäº†åˆ¶å®šæ›´ç»†ç²’åº¦æ£€æµ‹å™¨çš„ç´§è¿«éœ€æ±‚ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05952v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.05952.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/Dongping-Chen/MixSet)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint**<br><sub>æœºæ„: Gaoling School of Artificial Intelligence, Renmin University of China; School of Information, Renmin University of China; Kuaishou Technology, Beijing China.<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„RLæ–¹æ³•ï¼Œåä¸ºRLMECï¼Œé€šè¿‡ç”Ÿæˆå¼å¥–åŠ±æ¨¡å‹å’Œæœ€å°ç¼–è¾‘æœºåˆ¶ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹åœ¨RLè®­ç»ƒè¿‡ç¨‹ä¸­å®ç°æ›´ç²¾ç»†çš„ç›‘ç£å’Œè®­ç»ƒçš„ç¨³å®šæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06081v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06081.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/RUCAIBox/RLMEC)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models**<br><sub>æœºæ„: Google Research, Tel Aviv University<br>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºPatchscopesçš„æ¡†æ¶ï¼Œæä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•å»è§£é‡Šå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰éšè—è¡¨ç¤ºä¸­ç¼–ç çš„ä¿¡æ¯ï¼Œå¹¶ä¸”èƒ½å¤Ÿçº æ­£å¤šæ­¥æ¨ç†é”™è¯¯ã€‚Patchscopesä½œä¸ºä¸€ç§é€šç”¨çš„å¯é…ç½®æ¡†æ¶ï¼Œä¸ä»…ç»Ÿä¸€äº†ç°æœ‰çš„è§£é‡Šå·¥å…·ï¼Œå¹¶è§£å†³äº†å®ƒä»¬è‡ªèº«çš„ä¸€äº›ä¸è¶³ï¼ŒåŒæ—¶ä¹Ÿå¼€è¾Ÿäº†æ–°çš„ç ”ç©¶å’Œåº”ç”¨å¯èƒ½æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06102v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06102.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems**<br><sub>æœºæ„: Zhongguancun Laboratory, Tsinghua University, Institute of Information Engineering Chinese Academy of Sciences<br>æœ¬æ–‡ä¸ºå¤§è¯­è¨€æ¨¡å‹ç³»ç»Ÿä¸­çš„é£é™©åˆ†ç±»ã€ç¼“è§£æªæ–½ä»¥åŠè¯„ä¼°æ ‡å‡†æä¾›äº†å…¨é¢çš„æ¦‚è¿°ï¼Œæå‡ºäº†ä¸€ä¸ªæ–°çš„ç³»ç»ŸåŒ–åˆ†ç±»æ¡†æ¶ï¼Œå¸®åŠ©å¼€å‘è€…æ›´å…¨é¢åœ°ç†è§£å’Œå¤„ç†LLMç³»ç»Ÿçš„æ½œåœ¨é£é™©ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05778v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.05778.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning**<br><sub>æœºæ„: Qatar Computing Research Institute <br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„ã€ç”¨äºæ”¹å–„LLMsåœ¨ä¸Šä¸‹æ–‡æ¨ç†èƒ½åŠ›çš„å•ä»£ç†åŒæ­¥æç¤ºæ¡†æ¶â€”â€”Evidence to Generate (E2G)ã€‚é€šè¿‡è¦æ±‚LLMsåœ¨ç”Ÿæˆç­”æ¡ˆçš„åŒæ—¶æä¾›è¯æ®ä¸è§£é‡Šï¼ŒE2Gèƒ½å¤Ÿå‡å°‘é”™è¯¯æ¨ç†å¹¶æé«˜æ¨¡å‹åœ¨å¤„ç†å„ç§æ¨ç†ä»»åŠ¡æ—¶çš„å‡†ç¡®åº¦ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒE2Gæ–¹æ³•åœ¨å¤šä¸ªæƒ…å¢ƒå¯†é›†å‹è¯­è¨€ä»»åŠ¡ä¸­è¡¨ç°å‡ºè¾ƒCoTæ›´å¥½çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05787v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.05787.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **TOFU: A Task of Fictitious Unlearning for LLMs**<br><sub>æœºæ„: Carnegie Mellon University<br>æ–‡ç« ä¸ºLLMé—å¿˜é—®é¢˜æä¾›äº†æ–°çš„æ•°æ®é›†å’Œè¯„ä¼°æœºåˆ¶ï¼ŒTOFUä»»åŠ¡å±•ç¤ºäº†ç°æœ‰é—å¿˜æŠ€æœ¯çš„ä¸è¶³ï¼Œé¼“åŠ±äº†ç›¸ç»§è€Œæ¥çš„æ”¹è¿›å’Œç ”ç©¶å·¥ä½œã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06121v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06121.md)  |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction**<br><sub>æœºæ„: Fudan University, Microsoft Research Asia, Zhejiang University<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºEASYTOOLçš„æ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡ç®€åŒ–å’Œç»Ÿä¸€å·¥å…·æ–‡æ¡£çš„æŒ‡ä»¤æ¥æé«˜LLMåŸºç¡€ä»£ç†åœ¨å·¥å…·ä½¿ç”¨æ–¹é¢çš„è¡¨ç°ï¼Œè§£å†³äº†ç°æœ‰å·¥å…·ä½¿ç”¨ä¸­çš„ä¸ä¸€è‡´æ€§ã€å†—ä½™æ€§å’Œä¸å®Œæ•´æ€§é—®é¢˜ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06201v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.06201.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/microsoft/JARVIS)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models**<br><sub>æœºæ„: Johns Hopkins University<br>æ­¤ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡ä½¿ç”¨ç®€æ´çš„æ€ç»´é“¾æç¤ºï¼ˆCCoTï¼‰ï¼Œåœ¨å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¯ä»¥å¤§å¹…å‡å°‘æ–‡æœ¬è¾“å‡ºçš„é•¿åº¦ï¼Œè€Œä¸ä¼šå½±å“è§£å†³é—®é¢˜çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05618v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.05618.md)  |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks**<br><sub>InfiAgent-DABenchæä¾›äº†ä¸€ä¸ªæ–°é¢–çš„è¯„ä¼°åŸºå‡†ï¼Œè¿™ä¸ä»…æœ‰åŠ©äºè¡¡é‡æ™ºèƒ½ä»£ç†åœ¨æ•°æ®åˆ†æä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼ŒåŒæ—¶ä¹Ÿæ˜¯æ¢ç´¢å¦‚ä½•æ”¹è¿›å’Œä¼˜åŒ–LLMåœ¨è¿™ä¸€ç‰¹å®šé¢†åŸŸåº”ç”¨çš„é‡è¦ä¸€æ­¥ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05507v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.05507.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/InfiAgent/InfiAgent)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security**<br><sub>æœºæ„: Tsinghua University, Xiaomi AI Lab<br>è¯¥è®ºæ–‡ä½œä¸ºä¸€é¡¹è°ƒç ”å·¥ä½œï¼Œä»‹ç»äº†ä¸ªäººLLMä»£ç†çš„ç°çŠ¶ã€æŒ‘æˆ˜å’Œæœªæ¥è¶‹åŠ¿ï¼Œå¹¶æå‡ºäº†ä¸€ç§é€šç”¨çš„ç³»ç»Ÿæ¶æ„å’Œæ™ºèƒ½æ°´å¹³å®šä¹‰ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05459v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.05459.md)  |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **CASA: Causality-driven Argument Sufficiency Assessment**<br><sub>æœºæ„: Peking University<br>æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªåŸºäºLLMsçš„é›¶æ ·æœ¬å› æœé©±åŠ¨è®ºè¯å……åˆ†æ€§è¯„ä¼°æ¡†æ¶ï¼ˆCASAï¼‰ï¼ŒæˆåŠŸåº”å¯¹äº†æ— è§‚æµ‹æ•°æ®ä¸‹è®ºè¯å……åˆ†æ€§é‡åŒ–å’Œå¹²é¢„çš„éš¾é¢˜ï¼Œå¹¶åœ¨å®é™…åº”ç”¨ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05249v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.05249.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/xxxiaol/CASA)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing**<br><sub>æœºæ„: Google Research<br>è®ºæ–‡æˆåŠŸåœ°æå‡ºäº†ä¸€ä¸ªæ–°çš„åŸºäºå†…å­˜çš„è½¬æ¢å™¨æ–¹æ³•ï¼Œé€šè¿‡å­˜å‚¨é©±é€ç­–ç•¥å’ŒATTENDREå±‚ï¼Œæœ‰æ•ˆåœ°å‡å°‘å†…å­˜éœ€æ±‚å¹¶æ”¯æŒåŒå‘æ³¨æ„åŠ›ï¼Œåœ¨é•¿åºåˆ—å¤„ç†ä¸Šè¡¨ç°å‡ºä¸ä¼ ç»Ÿæ–¹æ³•ç›¸å½“çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04881v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.04881.md)  |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis**<br><sub>æœºæ„: Renmin University of China, Beijing Key Laboratory of Big Data Management and Analysis Methods, Meituan Group<br>è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªåä¸ºProLLM4Recçš„æ¡†æ¶ï¼Œç³»ç»Ÿåœ°åˆ†æäº†åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä½œä¸ºæ¨èç³»ç»Ÿçš„åŸºç¡€æ¨¡å‹ï¼Œå¹¶é€šè¿‡å®éªŒæµ‹è¯•äº†ä¸åŒæƒ…å†µä¸‹å¯¹LLMsçš„å½±å“ã€‚é€šè¿‡å®è¯åˆ†æï¼Œæ€»ç»“äº†å¯¹æœªæ¥ç ”ç©¶çš„å¯å‘æ€§å‘ç°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04997v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.04997.md)  |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk**<br><sub>æœºæ„: AWS AI Labs<br>è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è‡ªæˆ‘å¯¹è¯ç”Ÿæˆè®­ç»ƒæ•°æ®çš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•æœ‰æ½œåŠ›æ”¹è¿›ä»»åŠ¡å¯¼å‘å¯¹è¯ä»£ç†çš„æ€§èƒ½ã€‚å°½ç®¡å­˜åœ¨ä¸€äº›é™åˆ¶ï¼Œç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå½“é€‰æ‹©é«˜è´¨é‡å¯¹è¯ä½œä¸ºè®­ç»ƒæ•°æ®æ—¶ï¼Œå¯ä»¥æœ‰æ•ˆæé«˜æ¨¡å‹çš„æ€§èƒ½ã€‚è¿™è¯æ˜äº†åœ¨æ­£ç¡®çš„è®¾ç½®ä¸‹ï¼Œé€šè¿‡è‡ªæˆ‘ç”Ÿæˆæ•°æ®è¿›è¡Œå¾®è°ƒçš„è¯­è¨€æ¨¡å‹ç¡®å®æœ‰æ½œåŠ›å®ç°è‡ªæˆ‘æ”¹è¿›ï¼Œå¹¶æˆä¸ºæ›´å¥½çš„ä»»åŠ¡å¯¼å‘å¯¹è¯ä»£ç†ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05033v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.05033.md)  |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **AUTOACT: Automatic Agent Learning from Scratch via Self-Planning**<br><sub>æœºæ„: Zhejiang University, Alibaba Group<br>è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåä¸ºAUTOACTçš„æ¡†æ¶ï¼Œå®ƒé€šè¿‡è‡ªæˆ‘æŒ‡å¯¼å’Œè‡ªæˆ‘è§„åˆ’å®ç°è¯­è¨€ä»£ç†çš„è‡ªåŠ¨å­¦ä¹ ï¼Œä»¥åº”å¯¹ä»é›¶å¼€å§‹å­¦ä¹ æ–°ä»»åŠ¡çš„æŒ‘æˆ˜ã€‚è¯¥æ¡†æ¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæœ‰æ•ˆçš„æ•°æ®æ‰©å……æ–¹æ³•å’Œé«˜æ•ˆç‡çš„è‡ªåŠ¨ä»£ç†å­¦ä¹ è¿‡ç¨‹ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05268v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.05268.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/zjunlp/AutoAct)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Leveraging Print Debugging to Improve Code Generation in Large Language Models**<br><sub>æœºæ„: Zhejiang University, ByteDance<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ©ç”¨print debuggingæ–¹æ³•æŒ‡å¯¼LLMsè¿›è¡Œä»£ç ç”Ÿæˆå’Œè°ƒè¯•çš„æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨Leetcodeé—®é¢˜é›†ä¸ŠéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç®€å•å’Œä¸­ç­‰éš¾åº¦çš„é—®é¢˜ä¸Šã€‚å°½ç®¡åœ¨é«˜éš¾åº¦é—®é¢˜ä¸Šæ•ˆæœæœ‰é™ï¼Œä½†è¿™é¡¹å·¥ä½œä»ç„¶æ˜¯LLMsåœ¨ä»£ç è°ƒè¯•æ–¹é¢çš„ä¸€ä¸ªé‡è¦è¿›æ­¥ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05319v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.05319.md)  |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs**<br><sub>æœºæ„: Zhejiang University, Ant Group<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºARALLMçš„æ–°æ–¹æ³•ï¼Œè¯¥æ–¹æ³•ç»“åˆäº†ç±»æ¯”æ¨ç†å’Œå¤šä»»åŠ¡æ¨¡å‹æç‚¼ï¼Œæœ‰æ•ˆä¿ƒè¿›äº†å¤§å‹è¯­è¨€æ¨¡å‹ä»è‡ªç„¶è¯­è¨€ä¸­ç†è§£å¹¶è½¬æ¢ä¸ºç»“æ„åŒ–çš„é€»è¾‘è¯­è¨€çš„èƒ½åŠ›ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œéä¸“ä¸šè¥é”€äººå‘˜èƒ½å¤Ÿåˆ©ç”¨è‡ªç„¶è¯­è¨€æ¥é€‰æ‹©ç›®æ ‡ç”¨æˆ·ï¼Œæœ‰æœ›æ”¹å˜ç”¨æˆ·å®šä½å®è·µã€‚è¿™ç§èƒ½åŠ›çš„æå‡ï¼Œä¸ä»…åœ¨è¥é”€åœºæ™¯ä¸­æœ‰å®é™…çš„åº”ç”¨ä»·å€¼ï¼ŒåŒæ—¶ä¹Ÿä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„åŠŸèƒ½æ€§å’Œå®ç”¨æ€§åšå‡ºäº†æœ‰ç›Šçš„æ¢ç´¢ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04319v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.04319.md)  |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **The Critique of Critique**<br><sub>æœºæ„: The Hong Kong Polytechnic University, Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory<br>METACRITIQUEæ˜¯é¦–ä¸ªé’ˆå¯¹è‡ªç„¶è¯­è¨€æ‰¹åˆ¤è¿›è¡Œè¯„ä»·çš„æ¡†æ¶ï¼Œå…¶é€šè¿‡ç²¾ç¡®åº¦å’Œå¬å›ç‡çš„åŸåˆ™è¯„ä¼°æ‰¹åˆ¤çš„è´¨é‡ï¼Œå¹¶å®ç°äº†é«˜åº¦çš„å¯è§£é‡Šæ€§å’Œé€æ˜æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04518v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.04518.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/GAIR-NLP/MetaCritique)</div> |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search**<br><sub>æœºæ„: Nanyang Technological University Singapore<br>ReCoåˆ©ç”¨LLMsé‡å†™ä»£ç åº“ä¸­çš„ä»£ç ï¼Œé€šè¿‡é£æ ¼è§„èŒƒåŒ–æ˜¾è‘—æé«˜äº†ä»£ç æœç´¢çš„å‡†ç¡®æ€§ï¼Œå¹¶é€šè¿‡æ–°çš„è¯„ä»·æŒ‡æ ‡CSSimé‡åŒ–äº†é£æ ¼çš„å·®å¼‚ï¼Œæ¨åŠ¨äº†ä»£ç æ ·å¼æ ‡å‡†åŒ–çš„ç ”ç©¶ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04514v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.04514.md)  |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding**<br><sub>æœºæ„: University of California San Diego, Google Cloud AI Research, Google Research<br>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„CHAIN-OF-TABLEæ¡†æ¶ï¼Œé€šè¿‡å°†è¡¨æ ¼æ•°æ®æ˜¾å¼åœ°ç”¨äºæ¨ç†é“¾ï¼ŒåŠ¨æ€åœ°è§„åˆ’å¹¶æ›´æ–°æ“ä½œè¿‡ç¨‹ï¼Œä»è€Œæé«˜äº†LLMsåœ¨åŸºäºè¡¨æ ¼çš„æ¨ç†ä»»åŠ¡ä¸­çš„å‡†ç¡®æ€§å’Œå¯é æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04398v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.04398.md)  |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Large Language Models for Robotics: Opportunities, Challenges, and Perspectives**<br><sub>æœºæ„: Northwestern Polytechnical University, University of Georgia, Shaanxi Normal University<br>è®ºæ–‡æå‡ºçš„å¤šæ¨¡æ€GPT-4Væ¡†æ¶ï¼Œç»“åˆè‡ªç„¶è¯­è¨€å¤„ç†å’Œè§†è§‰æ„ŸçŸ¥ï¼Œæœ‰æœ›è§£å†³LLMsåœ¨æœºå™¨äººä»»åŠ¡è§„åˆ’ä¸­é¢å¯¹çš„æŒ‘æˆ˜ã€‚è¿™å¯¹äºç†è§£å’Œå®ç°æ›´é«˜çº§åˆ«çš„äººæœºäº¤äº’å’Œäººå·¥æ™ºèƒ½çš„æœªæ¥å…·æœ‰é‡è¦æ„ä¹‰ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04334v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.04334.md)  |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Agent Alignment in Evolving Social Norms**<br><sub>æœºæ„: Fudan University<br>æ­¤è®ºæ–‡æå‡ºäº†ä¸€ä¸ªEvoluationaryAgentæ¡†æ¶ï¼Œç”¨äºè¯„ä¼°å’Œå¢å¼ºå¤§å‹æ™ºèƒ½ä»£ç†åœ¨åŠ¨æ€æŒç»­å˜åŒ–çš„ç¤¾ä¼šè§„èŒƒä¸­çš„è‡ªé€‚åº”æ€§å’Œä¸€è‡´æ€§ã€‚ç ”ç©¶å¼ºè°ƒäº†ä»£ç†åœ¨è¿›åŒ–ä¸­ä¸ç¤¾ä¼šè§„èŒƒå¯¹é½çš„é‡è¦æ€§ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†æ¨¡å‹çš„å¯è¡Œæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04620v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.0462.md)  |
| <span style='display: inline-block; width: 42px;'>01-08</span> | **MARG: Multi-Agent Review Generation for Scientific Papers**<br><sub>æœºæ„: Northwestern University, The Hebrew University of Jerusalem, Allen Institute for AI<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„å¤šä»£ç†è¯„è®ºç”Ÿæˆæ–¹æ³•ï¼ˆMARGï¼‰ï¼Œå¯ä»¥è·¨è¶ŠåŸºç¡€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å¤§å°é™åˆ¶ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ç§‘å­¦è®ºæ–‡åŒè¡Œè¯„å®¡åé¦ˆã€‚é€šè¿‡ç”¨æˆ·ç ”ç©¶å’Œè‡ªåŠ¨åŒ–åº¦é‡ï¼ŒMARGçš„åé¦ˆè´¨é‡å¯¹æ¯”åŸºçº¿æœ‰æ˜¾è‘—æé«˜ï¼Œç”Ÿæˆçš„æœ‰ç”¨è¯„è®ºæ•°é‡æé«˜äº†2.2å€ï¼ŒåŒæ—¶ç”Ÿæˆäº†æ›´åŠ å…·ä½“çš„è¯„è®ºã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04259v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.04259.md)  |
| <span style='display: inline-block; width: 42px;'>01-08</span> | **TTMs: Fast Multi-level Tiny Time Mixers for Improved Zero-shot and Few-shot Forecasting of Multivariate Time Series**<br><sub>æœºæ„: IBM Research<br>TTMå±•ç¤ºäº†ä¸“é—¨é’ˆå¯¹å¤šæ ·åŒ–æ—¶é—´åºåˆ—æ•°æ®è®­ç»ƒçš„å°å‹é¢„è®­ç»ƒæ¨¡å‹åœ¨å¤šå˜é‡æ—¶é—´åºåˆ—é›¶/å°‘æ ·æœ¬é¢„æµ‹ä¸­çš„é«˜æ•ˆæ€§å’Œè½¬ç§»å­¦ä¹ èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03955v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.03955.md)  |
| <span style='display: inline-block; width: 42px;'>01-08</span> | **SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems**<br><sub>æœºæ„: Fudan University<br>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹çš„å¤šä»£ç†ç³»ç»Ÿâ€”â€”SpeechAgentsï¼Œå…¶èƒ½æ¨¡æ‹ŸåŒ…å«å¤šè¾¾ 25 åä»£ç†äººçš„äººç±»äº¤æµåœºæ™¯ï¼Œå¹¶å±•ç°å‡ºå“è¶Šçš„å¯æ‰©å±•æ€§ã€‚é€šè¿‡ä½¿ç”¨å¤šæ¨¡æ€ä¿¡å·ä½œä¸ºä»£ç†é—´äº¤æµçš„åª’ä»‹ï¼Œç³»ç»Ÿä¸ä»…å¯ä»¥æ¨¡æ‹Ÿå…·æœ‰æ­£ç¡®å†…å®¹ã€çœŸå®èŠ‚å¥å’Œä¸°å¯Œæƒ…æ„Ÿçš„å¯¹è¯ï¼Œè€Œä¸”è¿˜èƒ½åº”ç”¨äºå¦‚æˆå‰§åˆ›ä½œå’Œæœ‰å£°å°è¯´ç”Ÿæˆç­‰ä»»åŠ¡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03945v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.03945.md)  |
| <span style='display: inline-block; width: 42px;'>01-07</span> | **Grimoire is All You Need for Enhancing Large Language Models**<br><sub>æœºæ„: Beihang University, Renmin University of China<br>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºSLEICLçš„æ–¹æ³•ï¼Œé€šè¿‡å¼ºè¯­è¨€æ¨¡å‹å­¦ä¹ ç¤ºä¾‹æŠ€èƒ½å¹¶å°†å…¶è½¬ç§»ç»™å¼±è¯­è¨€æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†å¼±æ¨¡å‹çš„ICLèƒ½åŠ›ã€‚é€šè¿‡å®éªŒéªŒè¯äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå±•ç°äº†è¯¥æŠ€æœ¯åœ¨å¢å¼ºå¼±è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03385v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.03385.md)  |
| <span style='display: inline-block; width: 42px;'>01-07</span> | **Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects**<br><sub>æœºæ„: The Chinese University of Hong Kong, DeepWisdom, Peking University<br>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºæŒ‡å¯¼æœªæ¥ç ”ç©¶ä¸å¼€å‘çš„åŸºäºLLMçš„æ™ºèƒ½ä»£ç†ç³»ç»Ÿçš„æ¡†æ¶ï¼Œå¹¶æ¢è®¨äº†æé«˜å®ƒä»¬çš„è®¡åˆ’èƒ½åŠ›å’Œå¤šæ¨¡æ€ä¿¡æ¯å¤„ç†èƒ½åŠ›çš„ä¸åŒæ–¹æ³•ï¼Œä»¥åŠå¦‚ä½•è§£å†³LLMä»£ç†æ‰€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„ç ”ç©¶æ–¹å‘æä¾›äº†æ¸…æ™°çš„æŒ‡å—ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03428v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.03428.md)  |
| <span style='display: inline-block; width: 42px;'>01-07</span> | **Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon**<br><sub>æœºæ„: Beijing Academy of Artificial Intelligence, Renmin University of China, Nankai University<br>è¿™ç¯‡æ–‡ç« ä»‹ç»äº†æ¿€æ´»ä¿¡æ ‡è¿™ä¸€èƒ½å¤Ÿæ‰©å±•å¤§å‹è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡é•¿åº¦çš„æ–°æŠ€æœ¯ï¼Œä½¿å¾—æ¨¡å‹èƒ½åœ¨æœ‰é™ä¸Šä¸‹æ–‡çª—å£å†…æ„ŸçŸ¥æ›´å¹¿çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒæ—¶ä¿ç•™å¯¹çŸ­ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å¤„ç†èƒ½åŠ›ã€‚æ¿€æ´»ä¿¡æ ‡ä»£è¡¨äº†ä¸€ç§æœ‰æ•ˆã€é«˜æ•ˆã€å…¼å®¹ä¸”è®­ç»ƒæˆæœ¬ä½çš„æ–¹æ³•ï¼Œæ¥æ‰©å±•LLMsçš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03462v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.03462.md)  |
| <span style='display: inline-block; width: 42px;'>01-07</span> | **ChatGPT for Conversational Recommendation: Refining Recommendations by Reprompting with Feedback**<br><sub>æœºæ„: University of Louisville, Microsoft<br>è¯¥è®ºæ–‡æ¢ç´¢äº†ChatGPTä½œä¸ºå¯¹è¯æ¨èç³»ç»Ÿçš„æœ‰æ•ˆæ€§ã€‚é€šè¿‡æ„å»ºå›´ç»•ChatGPTçš„æµç¨‹ï¼Œæ¨¡æ‹Ÿç”¨æˆ·å®é™…ä½¿ç”¨æƒ…æ™¯ï¼Œå¹¶å¯¹æµè¡Œåè§è¿›è¡Œäº†ç ”ç©¶å’Œç¼“è§£ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03605v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.03605.md)  |
| <span style='display: inline-block; width: 42px;'>01-06</span> | **Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification**<br><sub>æœºæ„: Aerospace Information Research Institute Chinese Academy of Sciences, Key Laboratory of Target Cognition and Application Technology, University of Chinese Academy of Sciences<br>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªé’ˆå¯¹çŸ­æ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„Quartet Logic: A Four-Step Reasoning (QLFR)æ¡†æ¶ï¼Œä»¥åŠä¸€ä¸ªCoTé©±åŠ¨çš„å¤šä»»åŠ¡å­¦ä¹ ï¼ˆQLFR-CMLï¼‰æ–¹æ³•ï¼Œè¿™ä¸¤è€…éƒ½é€šè¿‡å¤§è¯­è¨€æ¨¡å‹çš„æ¨ç†é“¾æ¥è§£å†³STCé¢†åŸŸä¸­çš„æŒ‘æˆ˜ã€‚å®éªŒç»“æœè¯æ˜äº†è¿™äº›æ–¹æ³•çš„æœ‰æ•ˆæ€§å’Œå®ç”¨æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03158v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.03158.md)  |
| <span style='display: inline-block; width: 42px;'>01-06</span> | **The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models**<br><sub>æœºæ„: Renmin University of China, UniversitÃ© de MontrÃ©al<br>æœ¬è®ºæ–‡é€šè¿‡ç³»ç»Ÿæ€§å®è¯ç ”ç©¶ï¼Œæ·±å…¥äº†è§£å¹¶æ¢ç´¢å¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œè¯†åˆ«äº†å¹»è§‰çš„æ¥æºã€æ£€æµ‹æ–¹æ³•å’Œå‡è½»ç­–ç•¥ï¼Œå¹¶æå‡ºäº†æ–°çš„åŸºå‡†HaluEval 2.0å’Œç®€å•æœ‰æ•ˆçš„å¹»è§‰æ£€æµ‹æ¡†æ¶ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03205v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.03205.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/RUCAIBox/HaluEval-2.0)</div> |
| <span style='display: inline-block; width: 42px;'>01-06</span> | **CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models**<br><sub>æœºæ„: Harbin Institute of Technology, Kuaishou Technology<br>CogGPTé€šè¿‡å¼•å…¥è¿­ä»£è®¤çŸ¥æœºåˆ¶å’Œè®°å¿†ä¿æŒç³»ç»Ÿï¼Œæœ‰æ•ˆåœ°è§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ¨¡ä»¿äººç±»è®¤çŸ¥åŠ¨æ€æ–¹é¢çš„æŒ‘æˆ˜ï¼Œå±•ç¤ºäº†åœ¨è¿ç»­ä¿¡æ¯å¤„ç†ä¸­çš„ä¼˜ç§€è¡¨ç°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08438v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.08438.md)  |
| <span style='display: inline-block; width: 42px;'>01-05</span> | **Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache**<br><sub>æœºæ„: Alibaba Group, Shanghai Jiao Tong University<br>æ–‡ç« æå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆæ”¯æŒé•¿ä¸Šä¸‹æ–‡è¯­è¨€æ¨¡å‹äº‘æœåŠ¡çš„ç³»ç»Ÿï¼Œé€šè¿‡åˆ†å¸ƒå¼ç®—æ³•DistAttentionï¼Œä¼˜åŒ–äº†æ³¨æ„åŠ›æ¨¡å—çš„å¤„ç†å’Œå­˜å‚¨ï¼Œå¹¶é€šè¿‡DistKV-LLMæœåŠ¡ç³»ç»Ÿè¿›è¡Œç®¡ç†å’Œåè°ƒï¼Œå®ç°äº†åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­å¯¹èµ„æºçš„é«˜æ•ˆåˆ†é…å’Œç®¡ç†ï¼ŒéªŒè¯äº†å…¶åœ¨æ€§èƒ½ä¸Šçš„æ˜æ˜¾æé«˜ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02669v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.02669.md)  |
| <span style='display: inline-block; width: 42px;'>01-05</span> | **From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models**<br><sub>æœºæ„: Beike Inc.<br>æœ¬è®ºæ–‡ä»‹ç»äº†RAISEæ¡†æ¶ï¼Œé€šè¿‡å¢å¼ºè®°å¿†ç³»ç»Ÿå’Œç»“æ„åŒ–çš„ä»£ç†æ„å»ºè¿‡ç¨‹ï¼Œæé«˜äº†LLMsåœ¨å¤šè½®å¯¹è¯ä¸­çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨æˆ¿åœ°äº§é”€å”®æƒ…å¢ƒä¸­ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02777v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.02777.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)**<br><sub>æœºæ„: University of South Carolina, New Mexico State University, IBM Research<br>æœ¬è®ºæ–‡åœ¨ç†è§£å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ä¸è‡ªåŠ¨è§„åˆ’å’Œè°ƒåº¦ï¼ˆAPSï¼‰çš„æ•´åˆå‰æ™¯ï¼Œçªç ´äº†ä¼ ç»Ÿç³»ç»Ÿå¯¹ä¸Šä¸‹æ–‡çš„é€‚åº”æ€§å±€é™æ€§ï¼Œä¸ºå®ç°æ›´åŠ¨æ€ã€ä¸Šä¸‹æ–‡æ•æ„Ÿçš„è§„åˆ’é€”å¾„æä¾›äº†å¯èƒ½æ€§ï¼Œå¹¶ä¸ºè¿›ä¸€æ­¥çš„åº”ç”¨å’Œç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02500v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.025.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **Using LLM to select the right SQL Query from candidates**<br><sub>æœºæ„: Peking University<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§å€ŸåŠ©å¤§å‹è¯­è¨€æ¨¡å‹è‡ªåŠ¨ç”Ÿæˆtext-to-SQLæµ‹è¯•ç”¨ä¾‹çš„æ–¹æ³•ï¼Œå¹¶è®¾è®¡äº†ä¸‰æ­¥é‡æ–°æ’åºè¿‡ç¨‹ï¼Œå®éªŒæ˜¾ç¤ºè¯¥æ–¹æ³•èƒ½æ˜¾è‘—æé«˜ç°æœ‰text-to-SQLæ¨¡å‹çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02115v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.02115.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers**<br><sub>æœºæ„: Bytedance Inc.<br>æœ¬è®ºæ–‡æå‡ºäº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ç‰¹å®šé¢†åŸŸä»»åŠ¡ä¸­æ·±åº¦ä¸å‡†ç¡®æ€§æå‡çš„æ–¹æ³•â€”â€”ICE-GRTã€‚é€šè¿‡ç»“åˆäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼ŒICE-GRT åœ¨ä¸ç‰ºç‰²ä¸€èˆ¬æ€§èƒ½çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡äº†ç‰¹å®šé¢†åŸŸçš„èƒ½åŠ›ï¼Œå¹¶åœ¨å¤šé¡¹è¯„ä¼°ä»»åŠ¡ä¸­è¾¾åˆ°äº†æœ€å…ˆè¿›çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02072v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.02072.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives**<br><sub>æœºæ„: Zhejiang University, OPPO Research Institute<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºâ€œè‡ªæˆ‘å¯¹æ¯”â€çš„æ–°ç­–ç•¥ï¼Œç”¨äºæ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰åœ¨åæ€å’Œè‡ªæˆ‘ä¿®æ­£è¿‡ç¨‹ä¸­å­˜åœ¨çš„å›ºæ‰§å’Œä¸ä¸€è‡´é—®é¢˜ï¼Œé€šè¿‡åˆ›å»ºå¤šæ ·åŒ–è§£å†³æ–¹æ¡ˆè§†è§’ï¼Œå¯¹æ¯”ä¸åŒè§£å†³æ–¹æ¡ˆçš„å·®å¼‚ï¼Œå¹¶å°†å·®å¼‚æ€»ç»“ä¸ºæ£€æŸ¥æ¸…å•ï¼Œè¿›è€Œæå‡äº†LLMçš„åæ€è´¨é‡ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†è¯¥ç­–ç•¥çš„æ•ˆæœå’Œå¹¿æ³›é€‚ç”¨æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02009v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.02009.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)**<br><sub>æœºæ„: University of South Carolina, New Mexico State University, IBM Research<br>æœ¬æ–‡æ˜¯å¯¹å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è‡ªåŠ¨è§„åˆ’å’Œè°ƒåº¦é¢†åŸŸçš„åº”ç”¨è¿›è¡Œäº†ç»¼è¿°ï¼Œæå‡ºäº†å°†é¢†å…ˆçš„ LLMs å¦‚ GPT-4 å’Œ BERT ä¸ç»å…¸è§„åˆ’æ–¹æ³•ç»“åˆçš„å‰æ™¯ï¼Œä»¥åŠåœ¨å…«ä¸ªä¸åŒçš„è§„åˆ’é—®é¢˜ç±»åˆ«ä¸­åº”ç”¨ LLMs çš„æ½œåŠ›ï¼Œä»¥æœŸå‘å±•æ›´å…ˆè¿›ã€æ›´æ™ºèƒ½çš„è§„åˆ’ç³»ç»Ÿã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02500v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.025.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **LLM Augmented LLMs: Expanding Capabilities through Composition**<br><sub>æœºæ„: Google Research, Google DeepMind<br>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¨¡å‹æ‰©å±•æ¡†æ¶ â€”â€” CALMï¼Œæœ‰æ•ˆæ•´åˆäº†ä¸¤ä¸ªå¤§å‹è¯­è¨€æ¨¡å‹ä»¥å®ç°æ–°çš„ä»»åŠ¡ï¼Œä¸”åœ¨å¤šä¸ªå®éªŒä¸­è¯æ˜äº†å…¶æœ‰æ•ˆæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02412v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.02412.md)  |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval**<br><sub>æœºæ„: Columbia University<br>æœ¬è®ºæ–‡é’ˆå¯¹åŒ»é™¢å‡ºé™¢æ€»ç»“çš„é•¿ç¯‡æ–‡æ¡£ä»»åŠ¡ï¼Œæå‡ºäº†ä¸€ä¸ªåŸºäºåµŒå…¥å¼å®ä½“æ£€ç´¢çš„å¥å­çº§è§„åˆ’æ–¹æ³•SPEERï¼Œé€šè¿‡å¼•å¯¼å¤§å‹è¯­è¨€æ¨¡å‹LLMsæ›´å¥½åœ°è¦†ç›–å…³é”®å®ä½“ï¼Œç”Ÿæˆæ›´å®Œæ•´å’Œå¯ä¿¡çš„ä¸´åºŠæ€»ç»“ã€‚ç ”ç©¶è¯æ˜äº†SPEERæ–¹æ³•åœ¨å®é™…åº”ç”¨ä¸­å¯ä»¥æé«˜æ–‡æ¡£çš„è¦†ç›–åº¦å’Œå‡†ç¡®æ€§ï¼Œå‡è½»ä¸´åºŠåŒ»ç”Ÿçš„æ–‡æ¡£è´Ÿæ‹…ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02369v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.02369.md)  |
| <span style='display: inline-block; width: 42px;'>01-03</span> | **MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries**<br><sub>æœºæ„: Indian Institute of Technology Patna, Stanford University, Amazon GenAI<br>MedSummæ˜¯ä¸€ä¸ªæ–°é¢–çš„å¤šæ¨¡æ€åŒ»ç–—é—®é¢˜æ€»ç»“æ¡†æ¶ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡æ•´åˆæ–‡æœ¬å’Œè§†è§‰ä¿¡æ¯ç”ŸæˆåŒ»å­¦ç»†èŠ‚ä¸°å¯Œçš„æ€»ç»“ï¼Œæœ‰æ½œåŠ›æé«˜åŒ»ç–—å†³ç­–çš„è´¨é‡å¹¶åŠ æ·±å¯¹æ‚£è€…é—®é¢˜çš„ç†è§£ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01596v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.01596.md)  |
| <span style='display: inline-block; width: 42px;'>01-03</span> | **Social Media Ready Caption Generation for Brands**<br><sub>æœºæ„: Adobe Research India<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ¡†æ¶ï¼Œæ—¨åœ¨å¸®åŠ©å“ç‰Œåœ¨ç¤¾äº¤åª’ä½“ä¸Šåˆ›é€ ä¸å“ç‰Œå½¢è±¡å’Œä¸ªæ€§ç›¸ç¬¦çš„å¸å¼•äººçš„æ ‡é¢˜ã€‚æ¡†æ¶åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼ŒæˆåŠŸåº”å¯¹äº†ç”Ÿæˆä¸å“ç‰Œç›¸å…³æ€§å¼ºä¸”å¸å¼•çœ¼çƒçš„ç¤¾äº¤åª’ä½“æ ‡é¢˜çš„æŒ‘æˆ˜ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01637v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.01637.md)  |
| <span style='display: inline-block; width: 42px;'>01-02</span> | **A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models**<br><sub>æœºæ„: Islamic University of Technology Bangladesh, University of South Carolina, Stanford University<br>æœ¬æ–‡æ˜¯å¯¹LLMå¹»è§‰å‡è½»æŠ€æœ¯çš„å…¨é¢ç»¼è¿°ï¼Œæå‡ºäº†åˆ†ç±»æ¡†æ¶å’Œç³»ç»ŸåŒ–çš„åé¦ˆå’Œç†ç”±æ–¹æ³•ï¼Œå¹¶è¯„ä¼°äº†è¿™äº›æŠ€æœ¯çš„æœ‰æ•ˆæ€§å’Œå½±å“ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01313v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.01313.md)  |
| <span style='display: inline-block; width: 42px;'>01-02</span> | **LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning**<br><sub>è¿™ç¯‡è®ºæ–‡æˆåŠŸå±•ç¤ºäº†ä¸€ç§æ— éœ€fine-tuningå³å¯æ‰©å±•LLMsä¸Šä¸‹æ–‡çª—å£çš„æ–¹æ³•ï¼Œè¿™å¯¹äºåœ¨è®¡ç®—èµ„æºå—é™æƒ…å†µä¸‹æå‡å¤§å‹è¯­è¨€æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›å…·æœ‰é‡è¦æ„ä¹‰ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01325v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.01325.md)  |
| <span style='display: inline-block; width: 42px;'>01-01</span> | **The Earth is Flat? Unveiling Factual Errors in Large Language Models**<br><sub>æœºæ„: The Chinese University of Hong Kong, Tencent AI Lab<br>æœ¬æ–‡ä»‹ç»çš„FactCheckeræä¾›äº†é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹çš„äº‹å®é”™è¯¯è‡ªåŠ¨æµ‹è¯•æ–°æ¡†æ¶ï¼Œé€šè¿‡æ„å»ºçŸ¥è¯†å›¾è°±å¹¶ç”Ÿæˆæµ‹è¯•é—®é¢˜ï¼Œæ­ç¤ºå¹¶å‡å°‘äº†æ¨¡å‹çš„äº‹å®é”™è¯¯ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00761v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.00761.md)  |
| <span style='display: inline-block; width: 42px;'>01-01</span> | **From Prompt Engineering to Prompt Science With Human in the Loop**<br><sub>æœºæ„: University of Washington<br>æ–‡ç« å±•ç¤ºäº†å¦‚ä½•å°†LLMsçš„æç¤ºå·¥ç¨‹è½¬åŒ–ä¸ºæ›´ä¸ºç§‘å­¦å’Œç³»ç»Ÿçš„æç¤ºç§‘å­¦ã€‚é€šè¿‡å¼•å…¥äººåœ¨ç¯ä¸­çš„è´¨æ€§ç¼–ç æ–¹æ³•ï¼Œç¡®ä¿äº†LLMç”Ÿæˆçš„å“åº”çš„è´¨é‡å’Œä¸€è‡´æ€§ï¼ŒåŒæ—¶æ¶ˆé™¤äº†ä¸ªä½“ä¸»è§‚æ€§å’Œéšæ„æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04122v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.04122.md)  |
| <span style='display: inline-block; width: 42px;'>01-01</span> | **A & B == B & A: Triggering Logical Reasoning Failures in Large Language Models**<br><sub>æœºæ„: The Chinese University of Hong Kong, Tencent AI Lab<br>æœ¬è®ºæ–‡é’ˆå¯¹LLMsçš„é€»è¾‘æ¨ç†èƒ½åŠ›çš„è¯„ä¼°å’Œæ”¹è¿›é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªåä¸ºLogicAskerçš„æ–¹æ³•ï¼Œèƒ½å¤Ÿå…¨é¢è¯„ä¼°LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡é—®é¢˜ç”Ÿæˆå’Œä¸Šä¸‹æ–‡å­¦ä¹ æœ‰æ•ˆæå‡è¿™äº›èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00757v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2024-01/2401.00757.md)  |

---

### 12æœˆ

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>12-31</span> | **BatchEval: Towards Human-like Text Evaluation**<br><sub>æœºæ„: Beijing Institute of Technology, Xiaohongshu Inc  <br>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„LLMè¯„ä¼°èŒƒå¼â€”â€”BATCHEVALï¼Œè§£å†³äº†è‡ªåŠ¨æ–‡æœ¬è¯„ä¼°åœ¨é²æ£’æ€§å’Œä¸äººç±»åˆ¤æ–­ä¸€è‡´æ€§æ–¹é¢çš„é—®é¢˜ã€‚é€šè¿‡æ‰¹é‡è¯„ä¼°å’Œè¿­ä»£å¤„ç†ï¼ŒBATCHEVALåœ¨å‡†ç¡®æ€§å’Œæˆæœ¬æ•ˆç‡æ–¹é¢æ˜¾è‘—è¶…è¶Šäº†ç°æœ‰æ–¹æ³•ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00437v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2401.00437.md)  |
| <span style='display: inline-block; width: 42px;'>12-31</span> | **Improving Text Embeddings with Large Language Models**<br><sub>æœºæ„: Microsoft Corporation<br>æœ¬æ–‡é‡‡ç”¨æœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹å’Œåˆæˆæ•°æ®ï¼Œæå‡ºä¸€ç§æ–°é¢–çš„æ–‡æœ¬åµŒå…¥æ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨æ— éœ€äººå·¥æ ‡æ³¨æ•°æ®ä¸”è®­ç»ƒæ­¥éª¤å°‘äº1åƒçš„æƒ…å†µä¸‹ï¼Œè¾¾åˆ°ä¸ç«äº‰æ€§åŸºå‡†ç›¸åŒ¹é…çš„æ€§èƒ½ï¼Œä¸ºè¿›ä¸€æ­¥æå‡æ–‡æœ¬åµŒå…¥æŠ€æœ¯æä¾›äº†æœ‰åŠ›è¯æ®ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00368v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2401.00368.md)  |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception**<br><sub>æœºæ„: **Institution:** Shanghai Key Laboratory of Data Science School of Computer Science Fudan University, School of Data Science Fudan University, DataGrand Co. LTD<br>æœ¬æ–‡çš„ç ”ç©¶é€šè¿‡å»ºç«‹ç»´åº¦å•ä½çŸ¥è¯†åº“å’Œå®šåˆ¶åŒ–åŸºå‡†æµ‹è¯•ï¼Œæ˜¾è‘—æå‡äº†LLMsçš„å®šé‡æ¨ç†èƒ½åŠ›ã€‚è¿™ä¸ºç†è§£æ–‡æœ¬ä¸­é‡è¦çš„é‡å€¼ä¿¡æ¯å¹¶è¿›è¡Œé«˜å‡†ç¡®åº¦çš„æ¨ç†ä»»åŠ¡æä¾›äº†æ–°çš„é€”å¾„ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17532v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17532.md)  |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **Building Efficient Universal Classifiers with Natural Language Inference**<br><sub>æœºæ„: Vrije Universiteit Amsterdam, University of London Royal Holloway, Hugging Face<br>è¿™ç¯‡è®ºæ–‡æä¾›äº†ä¸€ç§åˆ©ç”¨è‡ªç„¶è¯­è¨€æ¨æ–­è¿›è¡Œé€šç”¨æ–‡æœ¬åˆ†ç±»çš„æ–°æ–¹æ³•ï¼Œå¹¶ä¸”æä¾›äº†å®ç°è¯¥æ–¹æ³•çš„è¯¦ç»†æ­¥éª¤å’Œå·¥å…·ï¼Œèƒ½å¤Ÿåœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹æ˜¾è‘—æé«˜æ¨¡å‹çš„æ•ˆç‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17543v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17543.md)  |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **DB-GPT: Empowering Database Interactions with Private Large Language Models**<br><sub>æœºæ„: Alibaba Group<br>æœ¬æ–‡æå‡ºäº†åä¸ºDB-GPTçš„åˆ›æ–°é¡¹ç›®ï¼Œè¯¥é¡¹ç›®é›†æˆäº†LLMsåŠæ•°æ®åº“ç³»ç»Ÿï¼Œä»¥æå‡ç”¨æˆ·ä½“éªŒå’Œæ— éšœç¢æ€§ã€‚DB-GPTå±•ç°äº†å±‚æ¬¡åŒ–è®¾è®¡ï¼Œæœ‰æ•ˆå¤„ç†äº†éšç§å’Œå®‰å…¨ä¿æŠ¤ç­‰é—®é¢˜ï¼ŒåŒæ—¶é€šè¿‡å¤šæºRAGå’Œè‡ªé€‚åº”ICLæå‡äº†ç³»ç»Ÿçš„æ•´ä½“æ€§èƒ½å’Œæ•ˆç‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17449v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17449.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/eosphoros-ai/DB-GPT)</div> |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17484v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17484.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/jongjyh/trfr)</div> |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **The Right Prompts for the Job: Repair Code-Review Defects with Large Language Model**<br><sub>æœºæ„: Ant Group, Nanjing University<br>ç ”ç©¶æ¢è®¨äº†LLMsåœ¨ä»£ç å®¡æŸ¥ç¼ºé™·ä¿®å¤ä¸­çš„åº”ç”¨ï¼Œæå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„åŠè‡ªåŠ¨APRèŒƒä¾‹ï¼Œåˆ†æäº†9ç§æµè¡Œæ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶è®¾è®¡äº†æœ‰æ•ˆçš„æç¤ºä»¥æŒ‡å¯¼ä»£ç ä¿®å¤è¿‡ç¨‹ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17485v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17485.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs**<br><sub>æœºæ„: Chinese University of Hong Kong, Tencent AI Lab<br>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæŒ‘æˆ˜LLMsè¿›è¡Œå…ƒæ¨ç†çš„æ–°è¯„ä¼°èŒƒå¼ï¼Œå¹¶å¼€å‘äº†é…å¥—çš„å…¬å¼€åŸºå‡†DiagGSM8Kï¼Œè¿™ä¸ºè¯„ä¼°LLMsçš„è®¤çŸ¥èƒ½åŠ›å¢åŠ äº†ä¸€ä¸ªæ–°ç»´åº¦ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17080v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1708.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Experiential Co-Learning of Software-Developing Agents**<br><sub>æœºæ„: Tsinghua University,Dalian University of Technology,Beijing University of Posts and Telecommunications<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œç§°ä¸ºç»éªŒå…±åŒå­¦ä¹ ï¼ˆExperiential Co-Learningï¼‰ï¼Œé€šè¿‡å…±åŒè¿½è¸ªã€å…±åŒè®°å¿†å’Œå…±åŒæ¨ç†æ¨¡å—çš„é¡ºåºå®ç°ï¼Œä½¿å¾—LLMé©±åŠ¨çš„æ™ºèƒ½ä»£ç†èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°ä»å†å²è½¨è¿¹ä¸­å­¦ä¹ ï¼Œå¹¶åˆ©ç”¨å†å²ç»éªŒæ¥ç›¸äº’æ¨ç†è§£å†³æ–°ä»»åŠ¡ã€‚å±•ç¤ºäº†æ˜æ˜¾ä¼˜äºç°æœ‰æŠ€æœ¯çš„ç»©æ•ˆæ”¹è¿›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17025v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17025.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos**<br><sub>æœºæ„: Tsinghua University<br>æœ¬è®ºæ–‡æå‡ºäº†Grounding-Prompteræ–¹æ³•ï¼Œé’ˆå¯¹é•¿è§†é¢‘ä¸­çš„TSGé—®é¢˜ï¼Œå°†LLMä¸æ—¶åºæ¨ç†å’Œå¤šæ¨¡æ€ä¿¡æ¯ç»“åˆèµ·æ¥ï¼Œè¯æ˜äº†é€šè¿‡å¤šæ¨¡æ€æç¤ºLLMçš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶åœ¨é•¿è§†é¢‘TSGä»»åŠ¡ä¸­çš„ä¼˜è¶Šæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17117v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17117.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs**<br><sub>æœºæ„: Chinese University of Hong Kong, Tencent AI Lab<br>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„è¯„ä¼°æ¨¡å‹ï¼Œè¦æ±‚LLMsä¸ä»…è¦è§£å†³é—®é¢˜ï¼Œè¿˜è¦è¿›è¡Œå…ƒæ¨ç†â€”â€”å³è¯„ä¼°æ¨ç†è¿‡ç¨‹æœ¬èº«ã€‚è¿™ç§æ–¹æ³•æœ‰æœ›æ­ç¤ºç”±äºä»¥å¾€ä»¥ç»“æœä¸ºå¯¼å‘çš„è¯„ä¼°æ–¹æ³•è€Œå¿½ç•¥çš„æ¨¡å‹è®¤çŸ¥ç¼ºé™·ï¼Œä¸ºæœªæ¥LLMsçš„è¯„ä¼°å’Œè®­ç»ƒæä¾›äº†æ–°çš„æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17080v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1708.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Structured Packing in LLM Training Improves Long Context Utilization**<br><sub>æœºæ„: University of Warsaw, Google DeepMind, Polish Academy of Sciences<br>è¿™ç¯‡è®ºæ–‡é€šè¿‡æå‡ºSPLICEæ–¹æ³•æ¥æ”¹è¿›é•¿è·ç¦»ä¸Šä¸‹æ–‡çš„åˆ©ç”¨ï¼ŒéªŒè¯äº†å…¶åœ¨æé«˜å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ä¸Šä¸‹æ–‡åˆ©ç”¨ç‡å’Œæ”¹è¿›é•¿ä¸Šä¸‹æ–‡ä»»åŠ¡æ€§èƒ½æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚SPLICEç‰¹åˆ«é€‚ç”¨äºåœ¨ç¼ºä¹é¢å¤–ç»“æ„åŒ–ä¿¡æ¯çš„è®­ç»ƒæ•°æ®ä¸Šæ„é€ è®­ç»ƒç¤ºä¾‹ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17296v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17296.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **GitAgent: Facilitating Autonomous Agent with GitHub by Tool Extension**<br><sub>æœºæ„: Tsinghua University, Renmin University of China<br>æ–‡ç« ä¸»è¦ä»‹ç»äº†ä¸€ä¸ªåä¸ºGITAGENTçš„è‡ªä¸»ä»£ç†ï¼Œå®ƒå¯ä»¥è‡ªä¸»ä»GitHubæ‰©å±•å·¥å…·ï¼Œä»¥æ»¡è¶³ç”¨æˆ·æŸ¥è¯¢çš„å¤šç§éœ€æ±‚ã€‚GITAGENTé€šè¿‡è§£å†³éæ ‡å‡†åŒ–æŒ‘æˆ˜ï¼Œèƒ½å¤Ÿè‡ªä¸»å­¦ä¹ åŸºäºGitHub Issues/PRsçš„äººç±»ç»éªŒï¼Œä»¥è§£å†³å·¥å…·æ‰©å±•è¿‡ç¨‹ä¸­çš„é—®é¢˜ï¼Œå¹¶ä¸”å±•ç¤ºäº†åœ¨è‡ªä¸»é›†æˆå·¥å…·ä»¥å®Œæˆè·¨ä¸“ä¸šé¢†åŸŸä»»åŠ¡æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17294v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17294.md)  |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Improving In-context Learning via Bidirectional Alignment**<br><sub>æœºæ„: Nanyang Technological University, Princeton University, Salesforce Research USA<br>æœ¬æ–‡é€šè¿‡å¼•å…¥æ–°é¢–çš„æ’åæŸå¤±ä»¥åŠå¯¹è¾“å‡ºåˆ†å¸ƒçš„å¯¹é½ï¼Œæå‡ºäº†åŒå‘å¯¹é½(BiAlign)ï¼Œæœ‰æ•ˆæé«˜äº†å°å‹æ¨¡å‹çš„ ICL èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17055v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17055.md)  |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges**<br><sub>æœºæ„: Shanghai Jiao Tong University (SJTU)<br>æœ¬è®ºæ–‡æ˜¯ä¸€ä¸ªå…³äºå¦‚ä½•é€‚é…å¤§å‹è¯­è¨€æ¨¡å‹äºæ•™è‚²ç³»ç»Ÿçš„ç»¼è¿°ï¼Œå®ƒæä¾›äº†å¯¹LLMsåœ¨æ•™è‚²ç›¸å…³èƒ½åŠ›æ–¹é¢çš„å‘å±•æƒ…å†µçš„æ¦‚è¿°ï¼Œå¹¶æ¢è®¨äº†æ„å»ºè¿™æ ·ç³»ç»Ÿçš„æ½œåŠ›ä¸æŒ‘æˆ˜ï¼Œä¸ºæœªæ¥çš„ç›¸å…³ç ”ç©¶æä¾›äº†æ´è§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08664v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2401.08664.md)  |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **How Robust are LLMs to In-Context Majority Label Bias?**<br><sub>æœºæ„: Amazon<br>æœ¬æ–‡å¯¹LLMsåœ¨é¢å¯¹ICLä¸­å¤šæ•°ç±»æ ‡ç­¾åå·®æ—¶çš„é²æ£’æ€§è¿›è¡Œäº†å…¨é¢ç ”ç©¶ï¼Œé€šè¿‡å®éªŒå‘ç°æŸäº›æ¨¡å‹åœ¨å¤„ç†è¿™ç§åå·®æ—¶æ˜¾ç¤ºå‡ºæ˜¾è‘—çš„ç¨³å®šæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16549v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.16549.md)  |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **Rethinking Tabular Data Understanding with Large Language Models**<br><sub>æœºæ„: UC San Diego, USC, UC Davis  <br>è¿™ç¯‡è®ºæ–‡æ·±å…¥æ¢è®¨äº†LLMså¯¹è¡¨æ ¼æ•°æ®çš„ç†è§£å’Œæ¨ç†èƒ½åŠ›ï¼Œå¯¹è¡¨æ ¼ç»“æ„çš„é²æ£’æ€§ã€æ–‡æœ¬ä¸ç¬¦å·æ¨ç†çš„æ¯”è¾ƒï¼Œä»¥åŠå¤šæ¨ç†è·¯å¾„èšåˆå¯¹æ¨¡å‹æ€§èƒ½æå‡çš„å½±å“åšå‡ºäº†è´¡çŒ®ã€‚æ‰€æå‡ºçš„è¡¨æ ¼ç»“æ„æ ‡å‡†åŒ–æ–¹æ³•å’Œæ··åˆè‡ªä¸€è‡´æ€§æœºåˆ¶å¯¹æé«˜LLMsåœ¨è¡¨æ ¼æ•°æ®æ¨ç†ä¸Šçš„æ€§èƒ½å…·æœ‰é‡è¦æ„ä¹‰ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16702v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.16702.md)  |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **Conversational Question Answering with Reformulations over Knowledge Graph**<br><sub>æœºæ„: University of Illinois at Urbana-Champaign, Amazon<br>CoRnNet æ˜¯ä¸€ç§æ–°å‹RLæ¨¡å‹ï¼Œç”¨äºåœ¨çŸ¥è¯†å›¾è°±ä¸Šè¿›è¡Œä¼šè¯å¼é—®é¢˜å›ç­”å¹¶ç»“åˆLLMç”Ÿæˆçš„æ”¹å†™ï¼Œå±•ç°äº†æ¯”å…¶ä»–å…ˆè¿›æ¨¡å‹æ›´å‡ºè‰²çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17269v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17269.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Think and Retrieval: A Hypothesis Knowledge Graph Enhanced Medical Large Language Models**<br><sub>æœºæ„: Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education; School of Computer Science Peking University, Beijing China<br>HyKGEæ¡†æ¶æœ‰æ•ˆè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹åŒ»ç–—é¢†åŸŸå¤æ‚é—®é¢˜æ—¶çš„å‡†ç¡®æ€§å’Œè§£é‡Šæ€§æŒ‘æˆ˜ï¼Œå…·æœ‰åœ¨åŒ»ç–—é¢†åŸŸä¸­çš„æ½œåœ¨åº”ç”¨å¹¶ä¸”åœ¨å®é™…åœºæ™¯ä¸­å±•ç¤ºå‡ºäº†å¾ˆå¤§çš„ä¼˜è¶Šæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15883v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.15883.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Supervised Knowledge Makes Large Language Models Better In-context Learners**<br><sub>æœºæ„: School of Engineering Westlake University, Westlake Institute for Advanced Study, Peking University<br>è®ºæ–‡æå‡ºçš„SuperContextæ¡†æ¶é€šè¿‡åˆ©ç”¨ç‰¹å®šä»»åŠ¡å¾®è°ƒçš„SLMsçš„ç›‘ç£çŸ¥è¯†ï¼Œæ˜¾è‘—æé«˜äº†LLMsåœ¨è‡ªç„¶è¯­è¨€ç†è§£å’Œé—®ç­”ä»»åŠ¡ä¸­çš„æ³›åŒ–èƒ½åŠ›å’Œäº‹å®æ€§ã€‚å®ƒä»£è¡¨äº†å°†å°å‹æ¨¡å‹çš„å¼ºå¤§åŠŸèƒ½èå…¥LLMsï¼Œä»¥å¤„ç†åˆ†å¸ƒå¤–æ•°æ®å’Œæœ€å°åŒ–å¹»è§‰ç°è±¡çš„ä¸€ç§åˆ›æ–°åšæ³•ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15918v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.15918.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Align on the Fly: Adapting Chatbot Behavior to Established Norms**<br><sub>æœºæ„: Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, The Hong Kong Polytechnic University<br>è¯¥å·¥ä½œæå‡ºäº†ä¸€ä¸ªåŠ¨æ€çš„OPOæ–¹æ³•ï¼Œé€šè¿‡æ”¶é›†æ³•å¾‹å’Œé“å¾·è§„åˆ™ä½œä¸ºå¤–éƒ¨å­˜å‚¨å™¨æ¥é™åˆ¶LLMsçš„è¡Œä¸ºï¼Œæ— éœ€è¿›ä¸€æ­¥è®­ç»ƒï¼Œå¹¶é€šè¿‡ä¸€ä¸ªå¯æ‰©å±•çš„è¯„ä¼°æ¨¡å—æ¥åº”å¯¹æ½œåœ¨çš„åŸºå‡†æµ‹è¯•æ³„æ¼é—®é¢˜åŠæ‰©å¤§æµ‹è¯•è§„åˆ™çš„èŒƒå›´ã€‚å°½ç®¡è¯¥æ–¹æ³•åœ¨æ¨ç†æ•ˆç‡æ–¹é¢å­˜åœ¨å±€é™æ€§å¹¶ä¸”æ£€ç´¢æ¨¡å‹ä»å¯è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œä½†åœ¨å¤šä¸ªè¯„ä¼°æ•°æ®é›†ä¸Šçš„å¹¿æ³›å®éªŒè¡¨æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15907v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.15907.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/GAIR-NLP/OPO)</div> |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Scaling Down, LiTting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq Encoder-Decoder Models**<br><sub>æœºæ„: University of Waterloo<br>è¿™ç¯‡è®ºæ–‡æå‡ºäº†LiT5-Distillå’ŒLiT5-Scoreä¸¤ç§åºåˆ—åˆ°åºåˆ—çš„ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œç”¨äºæœ‰æ•ˆçš„é›¶æ ·æœ¬åˆ—è¡¨çº§é‡æ–°æ’åºã€‚è¿™äº›æ–¹æ³•ä¸ä»…åœ¨æ¨¡å‹æ•ˆæœä¸Šç«äº‰åŠ›å¼ºï¼Œå¹¶ä¸”è§£å†³äº†ä¼ ç»Ÿä¾èµ–äºå¤§å‹LLMå’Œå¤–éƒ¨ç›¸å…³æ€§æ ‡ç­¾çš„é—®é¢˜ï¼Œå±•ç¤ºäº†åœ¨è¿™ä¸€é¢†åŸŸçš„ä¼˜åŒ–å’Œè¿›æ­¥ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16098v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.16098.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/castorini/LiT5)</div> |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph**<br><sub>æœºæ„: Northeastern University, Neusoft AI Magic Technology Research, Neusoft Institute of Intelligent Medical Research<br>è¿™ç¯‡è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°å‹æ¡†æ¶KnowledgeNavigatorï¼Œå®ƒé€šè¿‡æ”¹å–„çŸ¥è¯†å›¾è°±ä¸Šçš„æ¨ç†è¿‡ç¨‹ï¼Œè§£å†³äº†LLMåœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸Šçš„æ€§èƒ½å±€é™é—®é¢˜ã€‚å®éªŒç»“æœè¯å®äº†å…¶æœ‰æ•ˆæ€§ï¼Œå¹¶æœ‰æœ›åœ¨é«˜é£é™©å’Œé«˜æ•æ„Ÿé¢†åŸŸæ¨å¹¿LLMçš„åº”ç”¨ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15880v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1588.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **A Prompt Learning Framework for Source Code Summarization**<br><sub>æœºæ„: Nanyang Technological University, Tencent Inc., Nanjing University<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„PromptCSæ¡†æ¶ï¼Œç”¨äºæºä»£ç æ‘˜è¦ï¼Œèƒ½å¤Ÿç”Ÿæˆé«˜è´¨é‡çš„æ‘˜è¦ï¼Œå‡å°‘äº†è®­ç»ƒæˆæœ¬ï¼Œå¹¶æä¾›äº†ä»£ç ä»¥ä¾›ä»–äººç ”ç©¶ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16066v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.16066.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation**<br><sub>æœºæ„: City University of Hong Kong, The Chinese University of Hong Kong, Hangdian University<br>è¯¥è®ºæ–‡æå‡ºäº†RecRankerè¿™ä¸€æ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡æŒ‡ä»¤è°ƒæ•´çš„æ–¹å¼ä¼˜åŒ–äº†LLMsåœ¨top-kæ¨èä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶æœ‰æ•ˆåœ°èåˆäº†ä¼ ç»Ÿæ¨èç³»ç»Ÿçš„ä¿¡å·ï¼Œæ”¹å–„äº†æ¨¡å‹åœ¨æ¨èåœºæ™¯ä¸­çš„åº”ç”¨è¡¨ç°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16018v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.16018.md)  |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Aligning Large Language Models with Human Preferences through Representation Engineering**<br><sub>æœºæ„: Fudan University  <br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ–¹æ³•RAHFï¼Œé€šè¿‡è¡¨ç¤ºå·¥ç¨‹æŠ€æœ¯æ“çºµå†…éƒ¨æ¨¡å‹è¡¨ç¤ºæ¥å¯¹é½LLMsä¸äººç±»åå¥½ï¼Œè¿™ç§æ–¹æ³•åœ¨è®¡ç®—ä¸Šé«˜æ•ˆä¸”å®¹æ˜“å®ç°ï¼Œå¹¶å±•ç¤ºäº†å¤„ç†å¤šç§äººç±»åå¥½æˆ–ä»·å€¼çš„æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15997v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.15997.md)  |
| <span style='display: inline-block; width: 42px;'>12-25</span> | **Alleviating Hallucinations of Large Language Models through Induced Hallucinations**<br><sub>æœºæ„: Soochow University, Tencent AI Lab<br>è®ºæ–‡æå‡ºä¸€ä¸ªæ–°é¢–çš„å‡å°‘LLMså¹»è§‰çš„æ–¹æ³•ï¼Œé€šè¿‡æ„å»ºä¸€ä¸ªäº‹å®ä¸Šè¾ƒå¼±çš„LLMå¹¶åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‡å»å…¶çŸ¥è¯†ï¼Œæ”¹è¿›äº†æ¨¡å‹åœ¨ç”Ÿæˆäº‹å®æ€§å†…å®¹æ–¹é¢çš„è¡¨ç°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15710v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1571.md)  |
| <span style='display: inline-block; width: 42px;'>12-25</span> | **ESGReveal: An LLM-based approach for extracting structured data from ESG reports**<br><sub>æœºæ„: Alibaba Cloud, Tsinghua University, Sun Yat-Sen University<br>ESGRevealä»£è¡¨äº†åœ¨å¤„ç†ESGæ•°æ®ä¸­çš„ä¸€å¤§æ­¥è¿›ï¼Œæ—¨åœ¨é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹å’Œç›¸å…³æŠ€æœ¯æ¥æé«˜ä»å…¬å¸æŠ¥å‘Šä¸­æå–ç»“æ„åŒ–æ•°æ®çš„ä¸€è‡´æ€§å’Œå‡†ç¡®æ€§ï¼Œå¹¶æ¨åŠ¨äº†ESGå®è·µå’Œé€æ˜åº¦çš„æ”¹è¿›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17264v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.17264.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Large Language Model (LLM) Bias Index -- LLMBI**<br><sub>æœºæ„: University of Oxford, University Canada West, Amazon Web Services (AWS)<br>å¼•å…¥LLMBIæ˜¯åœ¨åˆ›å»ºå…¬å¹³å¯é çš„LLMsæ–¹é¢è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚å®ƒä¸ºç³»ç»Ÿå·¥ç¨‹å¸ˆå’Œç ”ç©¶äººå‘˜æä¾›äº†ä¸€ç§å®šé‡è¡¡é‡åè§çš„å·¥å…·ï¼Œå¼•å¯¼ä»–ä»¬æŒç»­æ”¹è¿›è¿™äº›å¼ºå¤§çš„æ¨¡å‹ï¼Œç¡®ä¿å®ƒä»¬åæ˜ ç¤¾ä¼šçš„å¤šæ ·æ€§å’Œä¸æ–­è¿›åŒ–çš„ç»“æ„ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14769v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.14769.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes**<br><sub>æœºæ„: University of Michigan, Rutgers University<br>æœ¬è®ºæ–‡é€šè¿‡NPHardEvalåŸºå‡†æµ‹è¯•æä¾›äº†ä¸€ç§æ–°çš„è¯„ä¼°LLMsæ¨ç†èƒ½åŠ›çš„æ–¹æ³•ã€‚è¯¥åŸºå‡†æµ‹è¯•å¹¿æ³›æ¶µç›–äº†ä»å¤šé¡¹å¼æ—¶é—´åˆ°NP-Hardå¤æ‚æ€§çº§åˆ«çš„é—®é¢˜ï¼Œå¹¶è®¾è®¡äº†åŠ¨æ€æ•°æ®æ›´æ–°æœºåˆ¶ä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼Œä»è€Œç¡®ä¿äº†è¯„ä¼°ç»“æœçš„å¯é æ€§å’ŒçœŸå®æ€§ã€‚è¿™äº›å‘ç°æå¤§åœ°ä¿ƒè¿›äº†å¯¹LLMså½“å‰èƒ½åŠ›çš„ç†è§£ï¼Œå¹¶ä¸ºæé«˜è¿™äº›æ¨¡å‹çš„æ¨ç†èƒ½åŠ›é“ºå¹³äº†é“è·¯ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14890v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1489.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/casmlab/NPHardEval)</div> |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation**<br><sub>æœºæ„: University of Waterloo, IN.AI Research<br>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºVIEScoreçš„è¯„ä¼°æ¡†æ¶ï¼Œæ—¨åœ¨æä¾›å¯¹æ¡ä»¶å›¾åƒç”Ÿæˆä»»åŠ¡çš„å¯è§£é‡Šæ€§è¯„ä»·ã€‚VIEScoreå…‹æœäº†ç°æœ‰è‡ªåŠ¨åŒ–åº¦é‡æ— æ³•è§£é‡Šè¯„åˆ†ç†ç”±çš„æŒ‘æˆ˜ï¼Œå¹¶èƒ½å¤Ÿé€‚åº”å„ç§ä»»åŠ¡éœ€æ±‚ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14867v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.14867.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **A Survey of Reinforcement Learning from Human Feedback**<br><sub>æœºæ„: LMU Munich, Duke Kunshan University<br>è¿™ç¯‡æ–‡ç« æ˜¯å¯¹RLHFçš„ç»¼è¿°ï¼Œåˆ†æäº†å®ƒåœ¨äººå·¥æ™ºèƒ½å’Œäººæœºäº¤äº’äº¤å‰ç‚¹ä¸­çš„åº”ç”¨ï¼Œå¹¶æ¢è®¨äº†ä¸LLMsç›¸å…³çš„æœ€æ–°ç ”ç©¶è¶‹åŠ¿ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14925v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.14925.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Generative AI Beyond LLMs: System Implications of Multi-Modal Generation**<br><sub>è¯¥è®ºæ–‡æ˜¯é’ˆå¯¹è·¨æ–‡æœ¬ã€å›¾åƒå’Œè§†é¢‘ç”Ÿæˆæ¨¡å‹çš„ç³»ç»Ÿæ€§èƒ½ç‰¹å¾åŒ–çš„é¦–æ¬¡å·¥ä½œï¼Œå®ƒæ­ç¤ºäº†ä¸åŒäºä¼ ç»ŸLLMsçš„ç‹¬ç‰¹ç³»ç»Ÿå±æ€§ï¼Œå¹¶æå‡ºäº†å¯¹äºTTI/TTVæ¨¡å‹è€Œè¨€ï¼Œä¼ ç»Ÿçš„ä¼˜åŒ–æŠ€æœ¯éœ€è¦é‡æ–°è€ƒè™‘çš„æŒ‘æˆ˜å’Œæœºä¼šã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14385v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.14385.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Reasons to Reject? Aligning Language Models with Judgments**<br><sub>æœºæ„: Tencent AI Lab, The Chinese University of Hong Kong<br>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„é€šè¿‡ç›´æ¥åˆ©ç”¨è¯­è¨€åé¦ˆæ¥å¯¹é½LLMsçš„æ¡†æ¶Contrastive Unlikelihood Trainingï¼ˆCUTï¼‰ï¼Œå¹¶ä¸”è¯æ˜äº†å…¶åœ¨å¤šç§åœºæ™¯ä¸‹çš„æœ‰æ•ˆæ€§ï¼ŒåŒ…æ‹¬ç¦»çº¿å¯¹é½å’Œåœ¨çº¿å¯¹é½ï¼Œä»¥åŠä»æœªå¯¹é½çš„æ¨¡å‹ï¼ˆå†·å¯åŠ¨ï¼‰å’Œå·²å¯¹é½çš„æ¨¡å‹ï¼ˆçƒ­å¯åŠ¨ï¼‰è¿›è¡Œè¿›ä¸€æ­¥ä¼˜åŒ–ã€‚ç ”ç©¶æ˜¾ç¤ºï¼Œä¸å¥–åŠ±ç›¸æ¯”ï¼Œè¯„åˆ¤æ€§åé¦ˆåœ¨å¯¹é½LLMsæ–¹é¢å…·æœ‰æ›´å¤§çš„æ½œåŠ›ï¼Œå€¼å¾—è¿›è¡Œè¿›ä¸€æ­¥ç ”ç©¶ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14591v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.14591.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Plan, Posture and Go: Towards Open-World Text-to-Motion Generation**<br><sub>æœºæ„: Tsinghua University, Microsoft Research Asia<br>ç ”ç©¶è€…ä»¬æå‡ºäº†ä¸€ä¸ªåä¸ºPRO-Motionçš„æ–°æ¡†æ¶ï¼Œä»¥å…‹æœä¼ ç»Ÿæ–‡æœ¬åˆ°åŠ¨ä½œç”Ÿæˆæ–¹æ³•çš„å±€é™æ€§ï¼Œå¹¶æˆåŠŸåœ¨å¼€æ”¾ä¸–ç•Œåœºæ™¯ä¸­ç”Ÿæˆæ›´å¤šæ ·å’ŒçœŸå®çš„åŠ¨ä½œã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14828v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.14828.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **YAYI 2: Multilingual Open-Source Large Language Models**<br><sub>æœºæ„: Beijing Wenge Technology Co. Ltd., Institute of Automation Chinese Academy of Sciences<br>è¯¥è®ºæ–‡æå‡ºäº†YAYI 2ï¼Œä¸€ä¸ªé’ˆå¯¹å¤šè¯­è¨€åœºæ™¯ä¼˜åŒ–çš„å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œé€šè¿‡åœ¨å¤§è§„æ¨¡è¯­æ–™åº“ä¸Šè¿›è¡Œé¢„è®­ç»ƒï¼Œå¹¶é€šè¿‡å¤šç§æ–¹æ³•ä¸äººç±»ä»·å€¼è§‚å¯¹é½ï¼Œæ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤šç§ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œç‰¹åˆ«æ˜¯åœ¨ä¸­æ–‡ç›¸å…³ä»»åŠ¡ä¸Šã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14862v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.14862.md)  |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning**<br><sub>æœºæ„: Huawei Noah's Ark Lab, University College London, University of Oxford<br>æœ¬è®ºæ–‡æå‡ºäº†Pangu-Agentæ¡†æ¶ï¼Œç›®æ ‡æ˜¯è§£å†³æ ‡å‡†RLæ–¹æ³•åœ¨å¤šä»»åŠ¡ç¯å¢ƒä¸­æ‰€é¢ä¸´çš„æŒ‘æˆ˜ã€‚Pangu-Agenté€šè¿‡å†…åœ¨å‡½æ•°å¼•å…¥ç»“æ„æ€§æ¨ç†ï¼Œå¹¶é€šè¿‡ç›‘ç£å­¦ä¹ å’ŒRLå®ç°æ™ºèƒ½ä½“çš„å¾®è°ƒï¼Œæé«˜äº†æ™ºèƒ½ä½“é€‚åº”å¤šç¯å¢ƒäº¤äº’çš„èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14878v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.14878.md)  |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **AppAgent: Multimodal Agents as Smartphone Users**<br><sub>æœºæ„: Tencent  <br>è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„å¤šæ¨¡æ€ä»£ç†æ¡†æ¶ï¼Œå®ƒå…è®¸ä»£ç†åƒäººç±»ç”¨æˆ·ä¸€æ ·æ“ä½œä»»ä½•æ™ºèƒ½æ‰‹æœºåº”ç”¨ï¼Œå¹¶é€šè¿‡è‡ªåŠ¨æ¢ç´¢å’Œè§‚å¯Ÿäººç±»æ¼”ç¤ºæ¥å­¦ä¹ æ–°åº”ç”¨çš„ä½¿ç”¨æ–¹æ³•ã€‚ç ”ç©¶ç»“æœè¯å®äº†è¯¥æ¡†æ¶åœ¨æ‰§è¡Œå¤šæ ·åŒ–é«˜çº§ä»»åŠ¡æ—¶çš„æ•ˆç‡å’Œé€‚åº”æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13771v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.13771.md)  |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning**<br><sub>æœºæ„: Language Technology Lab University of Cambridge<br>æœ¬æ–‡æä¾›äº†åœ¨èµ„æºæœ‰é™çš„æƒ…å†µä¸‹ä¸åŒå­¦ä¹ æ–¹æ³•çš„æ€§èƒ½å’Œæ ¡å‡†çš„å…¨é¢åˆ†æã€‚è¿™è¡¨æ˜è™½ç„¶æé«˜æ€§èƒ½å’Œæ ¡å‡†åŒæ—¶è¾¾æˆæ˜¯å›°éš¾çš„ï¼Œä½†é€šè¿‡è‡ªç»„è£…æŠ€æœ¯èƒ½å¤Ÿåœ¨ä¸å½±å“æ€§èƒ½çš„å‰æä¸‹å¢å¼ºæ¨¡å‹çš„æ ¡å‡†ï¼Œå¯¹äºæœªæ¥LLMsçš„åº”ç”¨æä¾›äº†é‡è¦çš„å®è·µæŒ‡å¯¼ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13772v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.13772.md)  |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **De novo Drug Design using Reinforcement Learning with Multiple GPT Agents**<br><sub>æœºæ„: Tsinghua University, Microsoft Research AI<br>è¿™ç¯‡è®ºæ–‡æ¨å‡ºäº†ä¸€ä¸ªç»“åˆå¤šä¸ªGPTä»£ç†çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ç”¨äºè¯ç‰©åˆ†å­çš„ç”Ÿæˆï¼Œå¹¶åœ¨GuacaMolåŸºå‡†æµ‹è¯•å’ŒSARS-CoV-2è›‹ç™½é¶æ ‡æŠ‘åˆ¶å‰‚è®¾è®¡ä¸­æ˜¾ç¤ºå‡ºè‰¯å¥½çš„æ€§èƒ½å’Œå®ç”¨æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06155v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2401.06155.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/HXYfighter/MolRL-MGPT)</div> |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction**<br><sub>æœºæ„: MIT, Microsoft Research NYC<br>è¯¥è®ºæ–‡æå‡ºäº†LASERï¼Œä¸€ç§åœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåå¯¹Transformeræ¨¡å‹çš„ç‰¹å®šå±‚è¿›è¡Œè£å‰ªä»¥æå‡æ€§èƒ½çš„æ–¹æ³•ã€‚ä½œè€…è¡¨æ˜ï¼Œè¿™ç§ç­–ç•¥ä¸ä»…æœ‰æ•ˆï¼Œè€Œä¸”æ˜¯é¦–æ¬¡å‘ç°å¯ä»¥é€šè¿‡ç²¾å¿ƒé€‰æ‹©çš„å‰ªææ¥å¢å¼ºTransformeræ¨¡å‹çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13558v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.13558.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation**<br><sub>æœºæ„: The University of Hong Kong, Shanghai Jiao Tong University, Kingâ€™s College London<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„åŸºäºå¤šæ™ºèƒ½ä½“çš„ä»£ç ç”Ÿæˆè§£å†³æ–¹æ¡ˆAgentCoderï¼Œé€šè¿‡ç‰¹å®šçš„æ™ºèƒ½ä½“èšç„¦äºä»£ç ç”Ÿæˆã€æµ‹è¯•è®¾è®¡å’Œæµ‹è¯•æ‰§è¡Œï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ä»£ç ç”Ÿæˆä¸æµ‹è¯•ä¹‹é—´çš„å¹³è¡¡é—®é¢˜ï¼Œå¹¶å®ç°äº†ä¼˜äºç°æœ‰SOTAæ–¹æ³•çš„ä»£ç ç”Ÿæˆè´¨é‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13010v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1301.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Mini-GPTs: Efficient Large Language Models through Contextual Pruning**<br><sub>æœºæ„: Massachusetts Institute of Technology<br>è¿™ç¯‡è®ºæ–‡å±•ç¤ºäº†é€šè¿‡ä¸Šä¸‹æ–‡å‰ªæå¼€å‘å°å‹ä½†é«˜æ•ˆçš„GPTæ¨¡å‹ï¼Œå³Mini-GPTsçš„è¿‡ç¨‹å’Œç»“æœã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œç ”ç©¶äººå‘˜åœ¨ä¸åŒé¢†åŸŸç‰¹å®šçš„æ•°æ®é›†ä¸ŠæˆåŠŸå‡å°‘äº†LLMsçš„å°ºå¯¸å¹¶ä¸”ä¿æŒäº†æ€§èƒ½ï¼Œå±•ç°äº†å‰ªææŠ€æœ¯ä¸ä»…ç†è®ºä¸Šå¯è¡Œï¼Œè€Œä¸”åœ¨å¼€å‘èµ„æºé«˜æ•ˆçš„é¢†åŸŸç‰¹å®šLLMsä¸­å®è·µä¸Šå…·æœ‰å®ç”¨ä»·å€¼ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12682v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.12682.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Lampr: Boosting the Effectiveness of Language-Generic Program Reduction via Large Language Models**<br><sub>æœºæ„: University of Waterloo, The Hong Kong University of Science and Technology, Concordia University<br>Lampræ˜¯ç¬¬ä¸€ä¸ªæ•´åˆLLMsäºç¨‹åºç¼©å‡è¿‡ç¨‹çš„ç®—æ³•ã€‚å®ƒé€šè¿‡å¤šå±‚æ¬¡æç¤ºæ–¹æ³•å’ŒLLMsçš„è¾…åŠ©ï¼Œå–å¾—äº†è·¨è¯­è¨€é€šç”¨æ€§å’Œç‰¹å®šè¯­è¨€è¯­ä¹‰æ„è¯†ä¹‹é—´çš„å¹³è¡¡ï¼Œå¹¶ä¸”åœ¨å®éªŒä¸­è¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13064v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.13064.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy**<br><sub>æœºæ„: Ant Group<br>æœ¬è®ºæ–‡æå‡ºäº†åä¸ºLookaheadçš„æ¨ç†åŠ é€Ÿæ¡†æ¶ï¼Œå®ƒé€šè¿‡ä½¿ç”¨åŸºäºTrieæ ‘çš„å¤šåˆ†æ”¯æ¨ç†ç­–ç•¥ï¼Œåœ¨æé«˜LLMsæ¨ç†é€Ÿåº¦çš„åŒæ—¶ï¼Œä¿æŒäº†ç”Ÿæˆå‡†ç¡®æ€§ã€‚æ¡†æ¶é€šè¿‡å¹¿æ³›çš„å®éªŒéªŒè¯äº†å…¶æ€§èƒ½ï¼Œå¹¶åœ¨æ”¯ä»˜å®çš„å®é™…ä½¿ç”¨åœºæ™¯ä¸­å¾—åˆ°äº†éƒ¨ç½²ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12728v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.12728.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation**<br><sub>æœºæ„: The University of Hong Kong, Shanghai Jiao Tong University<br>AgentCoderæ˜¯ä¸€ç§æ–°å‹çš„å¤šä»£ç†æ¡†æ¶ï¼Œå®ƒåœ¨è‡ªåŠ¨ä»£ç ç”Ÿæˆä¸­é€šè¿‡è¿›è¡Œè¿­ä»£æµ‹è¯•å’Œä¼˜åŒ–ï¼Œæ˜æ˜¾æé«˜äº†ä»£ç ç”Ÿæˆçš„è´¨é‡å’Œå‡†ç¡®æ€§ï¼Œå°¤å…¶æ˜¯åœ¨é¢å¯¹æŒ‘æˆ˜æ€§æ›´å¤§çš„å¢å¼ºå‹æ•°æ®é›†æ—¶è¡¨ç°å‡ºå…¶ä¼˜åŠ¿ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13010v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1301.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Time is Encoded in the Weights of Finetuned Language Models**<br><sub>è¿™é¡¹ç ”ç©¶é€šè¿‡æ—¶é—´å‘é‡çš„æ¦‚å¿µè¡¨æ˜äº†æ—¶é—´å˜åŒ–å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šé€šè¿‡è¯­è¨€æ¨¡å‹çš„æƒé‡ç©ºé—´æ¥ç¼–ç ï¼Œå¹¶ä¸”æƒé‡æ’å€¼å¯ä»¥å¸®åŠ©å®šåˆ¶æ¨¡å‹ä»¥é€‚åº”æ–°çš„æ—¶é—´æ®µã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13401v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.13401.md)  |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Generative Multimodal Models are In-Context Learners**<br><sub>æœºæ„: Beijing Academy of Artificial Intelligence, Tsinghua University, Peking University<br>æœ¬è®ºæ–‡é€šè¿‡æ‰©å¤§æ¨¡å‹è§„æ¨¡ï¼ŒæˆåŠŸæå‡äº†å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ Emu2 åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä¸Šçš„è¡¨ç°ï¼Œå¹¶åœ¨ä¸€ç³»åˆ—å¤šæ¨¡æ€ç†è§£ä»»åŠ¡ä¸­å–å¾—äº†çªç ´æ€§çš„æ•ˆæœï¼Œå°¤å…¶åœ¨åŸºäºæŒ‡ä»¤å¾®è°ƒåçš„è§†è§‰é—®ç­”å’Œå¯æ§è§†è§‰ç”Ÿæˆæ–¹é¢ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13286v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.13286.md)  |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT**<br><sub>æœ¬æ–‡æå‡ºäº†é¦–ä¸ªç»“åˆäººç±»æ ¸å®ä¸ChatGPTè¾…åŠ©çš„å‡æ–°é—»æ£€æµ‹å…¬å…±åŸºå‡†æ•°æ®é›†ChatGPT-FCï¼Œå¹¶é€šè¿‡å®šé‡åˆ†æå¯¹æ¯”äº†äººç±»è®°è€…ä¸LLMè¿›è¡Œäº‹å®æ ¸æŸ¥çš„å·®å¼‚ã€‚ç ”ç©¶å‘ç°ChatGPTå¯ä»¥å¢å¼ºæ–°é—»äº‹å®æ ¸æŸ¥è¿‡ç¨‹çš„å®¢è§‚æ€§å’Œå¯é æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11870v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1187.md)  |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **Active Preference Inference using Language Models and Probabilistic Reasoning**<br><sub>æœºæ„: Cornell University, Cornell Tech<br>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ä¸ªå®æ—¶ç®—æ³•ï¼Œé€šè¿‡ç”Ÿæˆä¿¡æ¯ä¸°å¯Œçš„é—®é¢˜æ¥åŠ å¿«LLMså¯¹ç”¨æˆ·åå¥½çš„æ¨æ–­ï¼Œå¹¶åœ¨ç½‘è´­åœºæ™¯ä¸­éªŒè¯äº†å…¶å‡å°‘ç”¨æˆ·äº¤äº’å¹¶æé«˜ä»»åŠ¡æ€§èƒ½çš„èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12009v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.12009.md)  |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **Text-Conditioned Resampler For Long Form Video Understanding**<br><sub>æœºæ„: University of Oxford, Google, Google DeepMind<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºTCRçš„æ–°å‹æ¶æ„åŠé¢„è®­ç»ƒæ–¹æ³•ï¼Œèƒ½å¤Ÿå¤„ç†ä¸æ–‡æœ¬æ¡ä»¶ç›¸ç»“åˆçš„é•¿è§†é¢‘ã€‚å®ƒæœ‰æ•ˆåœ°æ¡¥æ¥äº†é¢„è®­ç»ƒçš„è§†è§‰ç¼–ç å™¨å’ŒLLMï¼Œå®ç°äº†é•¿æœŸè§†é¢‘ç†è§£çš„é—®é¢˜ï¼Œå¹¶åœ¨å¤šä¸ªè¯„ä¼°ä»»åŠ¡ä¸Šå–å¾—äº†æœ€ä½³æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11897v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.11897.md)  |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes**<br><sub>æœºæ„: University of Cambridge<br>æœ¬æ–‡ä»‹ç»äº†CLLMï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†å¤§å‹è¯­è¨€æ¨¡å‹çš„å…ˆéªŒçŸ¥è¯†å’Œå¼ºå¤§çš„æ•°æ®ä¸­å¿ƒæ–¹æ³•æ¥è¿›è¡Œæ•°æ®å¢å¼ºçš„æ–°æ–¹æ³•ï¼Œæ—¨åœ¨ä¸ºèµ„æ–™åŒ®ä¹çš„é¢†åŸŸå’Œåœ°åŒºçš„æœºå™¨å­¦ä¹ æä¾›äº†æ–°çš„é€”å¾„ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12112v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.12112.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Agent-based Learning of Materials Datasets from Scientific Literature**<br><sub>æœºæ„: University of Toronto  <br>æœ¬è®ºæ–‡å±•ç¤ºäº†ä¸€ä¸ªä»¥å¤§å‹è¯­è¨€æ¨¡å‹ä¸ºåŸºç¡€çš„æ™ºèƒ½ä»£ç†åœ¨è‡ªåŠ¨å­¦ä¹ å’Œæå–ç§‘å­¦æ–‡çŒ®ä¸­ææ–™ç›¸å…³æ•°æ®é›†æ–¹é¢çš„èƒ½åŠ›ã€‚Eunomiaå±•ç¤ºäº†åœ¨æ²¡æœ‰ä»»ä½•å¾®è°ƒçš„æƒ…å†µä¸‹åœ¨æå–å®ä½“å’Œå…³ç³»æ–¹é¢çš„æœ‰æ•ˆæ€§ï¼Œä¸”å¯ä»¥å¢å¼ºå…¶åœ¨å¤„ç†å¤æ‚ä»»åŠ¡æ—¶é¿å…é”™è¯¯çš„èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11690v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1169.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/AI4ChemS/Eunomia)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows**<br><sub>æœºæ„: University of Washington, Stanford University, Allen Institute for AI<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªè®¾è®¡ç©ºé—´æ¦‚å¿µæ¡†æ¶ä»¥åŠé€šè¿‡è½¬æ¢ä¼—åŒ…å·¥ä½œæµåˆ°LLMé“¾çš„ä¸‰ä¸ªæ¡ˆä¾‹ç ”ç©¶ï¼Œä¸ºæœªæ¥LLMé“¾çš„è®¾è®¡å’Œå¼€å‘æä¾›äº†å®è·µæŒ‡å¯¼å’Œç†è®ºè§è§£ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11681v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.11681.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Towards Better Serialization of Tabular Data for Few-shot Classification with Large Language Models**<br><sub>æœºæ„: Carnegie Mellon University<br>è®ºæ–‡æˆåŠŸåœ°å±•ç¤ºäº†åœ¨è¡¨æ ¼æ•°æ®åˆ†ç±»ä¸­åº”ç”¨LLMsçš„åˆ›æ–°å®è·µï¼Œå¹¶ä»¥LaTeXåºåˆ—åŒ–æ¡†æ¶ä¸ºç‰¹ç‚¹ï¼Œæå‡ºäº†æœ‰æ•ˆå¤„ç†é¢†åŸŸç‰¹å®šæ•°æ®é›†çš„æ–°å‹åºåˆ—åŒ–æ–¹æ³•ã€‚ç ”ç©¶è¿˜å¯¹LLMsåœ¨è§£è¯»å¤æ‚æ•°æ®å…³ç³»æ–¹é¢çš„èƒ½åŠ›è¿›è¡Œäº†æ·±å…¥çš„æ¢ç´¢ã€‚è®ºæ–‡çš„LaTeXåºåˆ—åŒ–æ–¹æ³•ä¸ä»…æå‡äº†LLMsåœ¨åˆ†ç±»ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œè¿˜æ˜¾è‘—æé«˜äº†å†…å­˜çš„ä½¿ç”¨æ•ˆç‡å’Œè®¡ç®—æ•ˆç‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12464v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.12464.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Generalized Category Discovery with Large Language Models in the Loop**<br><sub>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç«¯åˆ°ç«¯çš„ä¸»åŠ¨å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶é€šè¿‡å¼•å…¥å¤§å‹è¯­è¨€æ¨¡å‹è¿›å…¥è®­ç»ƒå¾ªç¯ï¼Œæœ‰æ•ˆåœ°æå‡äº†æ¨¡å‹åœ¨æ³›åŒ–ç±»åˆ«å‘ç°ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶èƒ½è‡ªåŠ¨ç”Ÿæˆç±»åˆ«åç§°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10897v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10897.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Retrieval-Augmented Generation for Large Language Models: A Survey**<br><sub>æœºæ„: Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University, Fudan University<br>è¿™ç¯‡è®ºæ–‡ä¸ºRAGé¢†åŸŸæä¾›äº†ä¸€ä¸ªå…¨é¢å’Œç³»ç»Ÿçš„æŠ€æœ¯æ¦‚è§ˆï¼Œå¼ºè°ƒäº†æå‡LLMsæ£€ç´¢å’Œç”Ÿæˆèƒ½åŠ›çš„é‡è¦æ€§ï¼ŒæŒ‡å‡ºäº†ç°æœ‰æŒ‘æˆ˜ï¼Œå±•æœ›äº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10997v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10997.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **"Paraphrasing The Original Text" Makes High Accuracy Long-Context QA**<br><sub>æœºæ„: Tsinghua University  <br>è®ºæ–‡ä¸»è¦é€šè¿‡ç†è®ºè¯æ˜å’Œå®éªŒéªŒè¯ï¼Œæå‡ºäº†ä¸€ç§ä½æˆæœ¬ä¸”é«˜æ•ˆçš„æ–¹æ³•ï¼Œé€šè¿‡åŸæ–‡é‡Šä¹‰ä»»åŠ¡å’Œæœ‰æ•ˆçš„æŒ‡ä»¤å¾®è°ƒæ•°æ®æ‰©å±•ç°æœ‰è¯­è¨€æ¨¡å‹å¤„ç†é•¿æ–‡æœ¬çš„èƒ½åŠ›ï¼Œæ˜¾è‘—æé«˜äº†é•¿æ–‡æœ¬é—®ç­”çš„å‡†ç¡®æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11193v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.11193.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **MAC-SQL: Multi-Agent Collaboration for Text-to-SQL**<br><sub>æœºæ„: Beihang University, Tencent Cloud AI<br>æ€»ä½“è€Œè¨€ï¼ŒMAC-SQL æ¡†æ¶é€šè¿‡è”åˆæ™ºèƒ½ä»£ç†ï¼Œè§£å†³äº† Text-to-SQL ä»»åŠ¡ä¸­çš„ä¸€äº›å…³é”®æŒ‘æˆ˜ï¼Œå¦‚å¤„ç†å¤§å‹æ•°æ®åº“ã€å¤æ‚æŸ¥è¯¢ä»¥åŠSQLéªŒè¯å’Œä¿®æ­£é—®é¢˜ã€‚è¿˜å‘å¸ƒäº†ä¸€ä¸ªå¼€æºæ¨¡å‹SQL-Llamaï¼Œè¯¥æ¨¡å‹å±•ç¤ºäº†é¼“åŠ±æ€§çš„ç»“æœï¼Œå¹¶å…·å¤‡ä¸æ”¶è´¹æ¨¡å‹å¦‚GPT-4ç›¸åª²ç¾çš„æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11242v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.11242.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/wbbeyourself/MAC-SQL)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation**<br><sub>æœºæ„: University of Waterloo, Huawei Noahâ€™s Ark Lab, FEEC-Unicamp Brazil<br>è¿™é¡¹å·¥ä½œé€šè¿‡å¼•å…¥NoMIRACLæ•°æ®é›†ï¼Œä¸ºè¯„ä¼°LLMåœ¨æ£€ç´¢å¼å¢å¼ºç”Ÿæˆä¸­çš„ç¨³å¥æ€§æä¾›äº†ä¸€ä¸ªå¤šè¯­è¨€çš„è¯„ä¼°å·¥å…·ï¼Œå¹¶é€šè¿‡å»ºç«‹GPT-4åŸºçº¿æ¨¡å‹å±•ç¤ºäº†LLMåœ¨è¯†åˆ«ç›¸å…³ä¸éç›¸å…³æ£€ç´¢ç»“æœä¸­å­˜åœ¨çš„æŒ‘æˆ˜ï¼Œçªå‡ºäº†æœªæ¥ç ”ç©¶æé«˜LLMç¨³å¥æ€§çš„å¿…è¦æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11361v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.11361.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model**<br><sub>æœºæ„: Huawei Noah's Ark Lab, The University of Hong Kong, The Hong Kong University of Science and Technology<br>è¿™ç¯‡è®ºæ–‡é€šè¿‡æ„å»º Geo170K æ•°æ®é›†å’Œå¼€å‘åŸºäºå®ƒçš„ G-LLaVA æ¨¡å‹ï¼Œå…‹æœäº†å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³å‡ ä½•é—®é¢˜ä¸Šçš„é™åˆ¶ï¼Œå¹¶å®ç°äº†æ¯”ç°æœ‰æœ€å…ˆç«¯æ¨¡å‹æ›´å¥½çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11370v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1137.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Social Learning: Towards Collaborative Learning with Large Language Models**<br><sub>æœºæ„: Google, EPFL<br>æœ¬æ–‡æå‡ºäº†åœ¨LLMsä¸­å®ç°çŸ¥è¯†ä¼ é€’çš„æ–°æ¡†æ¶â€”ç¤¾äº¤å­¦ä¹ ï¼Œå¹¶æä¾›äº†ä¿æŠ¤éšç§çš„è§£å†³æ–¹æ¡ˆã€‚è¯¥æ¡†æ¶é€šè¿‡è‡ªç„¶è¯­è¨€åœ¨æ¨¡å‹é—´äº¤æ¢çŸ¥è¯†ï¼ŒåŒæ—¶é¿å…æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§å’Œéšç§ä¿æŠ¤èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11441v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.11441.md)  |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **From Google Gemini to OpenAI Q-Star: A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape**<br><sub>æœºæ„: Cyberstronomy Pty Ltd, Academies Australasia Polytechnic, Massey University<br>è¿™é¡¹ç»¼è¿°è¯¦å°½åœ°åˆ†æäº†ç”Ÿæˆå‹AIé¢†åŸŸçš„å‘å±•åŠå…¶å¯¹ç ”ç©¶æ™¯è§‚çš„é‡å¡‘æ•ˆåº”ï¼Œå°¤å…¶å…³æ³¨äº†MoEå¤šæ¨¡æ€å­¦ä¹ å’ŒAGIçš„å‰æ™¯ã€‚ç ”ç©¶æ¶µç›–äº†ä»AIæ¨¡å‹ç»“æ„å’ŒåŸ¹è®­æŠ€æœ¯åˆ°åº”ç”¨é¢†åŸŸå’Œä¼¦ç†è€ƒè™‘çš„å…¨é¢åˆ†ç±»ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10868v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10868.md)  |
| <span style='display: inline-block; width: 42px;'>12-17</span> | **Mixed Distillation Helps Smaller Language Model Better Reasoning**<br><sub>æœºæ„: Zhejiang University, Dalian Medical University<br>Mixed Distillationæ¡†æ¶é€šè¿‡æ•´åˆLLMsä¸­çš„PoTå’ŒCoTèƒ½åŠ›åˆ°æ›´å°çš„æ¨¡å‹ä¸­ï¼Œæ˜¾è‘—æ”¹å–„äº†å®ƒä»¬çš„é«˜çº§æ¨ç†èƒ½åŠ›ï¼Œç‰¹åˆ«æ˜¯åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10730v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1073.md)  |
| <span style='display: inline-block; width: 42px;'>12-17</span> | **Distinguishing Translations by Human, NMT, and ChatGPT: A Linguistic and Statistical Approach**<br><sub>æœºæ„: Shanghai Jiao Tong University<br>æœ¬ç ”ç©¶ä¸ºChatGPTä½œä¸ºNMTä¹‹å¤–çš„å¦ä¸€ç§ç¿»è¯‘å·¥å…·çš„å¯èƒ½æ€§æä¾›äº†åˆæ­¥ç­”æ¡ˆï¼Œå¹¶å±•ç¤ºäº†ChatGPTä¸NMTå’ŒHTç›¸æ¯”çš„ç‹¬ç‰¹ç‰¹æ€§ã€‚è¿™äº›æ–°è®¤è¯†æœ‰åŠ©äºæœªæ¥æ›´äººæ€§åŒ–ã€æ›´ç¬¦åˆè¯­å¢ƒçš„ç¿»è¯‘ç³»ç»Ÿçš„å¼€å‘ï¼Œå¹¶ä¸ºå¦‚ä½•æœ‰æ•ˆä½¿ç”¨AIç”Ÿæˆçš„ç¿»è¯‘æä¾›æ´è§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10750v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.1075.md)  |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **CoAScore: Chain-of-Aspects Prompting for NLG Evaluation**<br><sub>æœºæ„: GSAI Renmin University of China<br>CoAScore æ˜¯ä¸€ä¸ªæ–°é¢–çš„è¯„ä¼°æŒ‡æ ‡ï¼Œå®ƒé€šè¿‡â€œæ–¹é¢é“¾â€çš„æ–¹æ³•æå‡äº†å¯¹äº NLG ä»»åŠ¡çš„è¯„ä¼°ç²¾åº¦ï¼Œå¹¶ä¸”è¯¥æ•ˆæœè·å¾—äº†å®éªŒçš„è¯å®ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10355v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10355.md)  |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large Language Models**<br><sub>æœºæ„: Science Foundation Ireland (SFI), JSPS KAKENHI<br>è¿™ç¯‡è®ºæ–‡æå‡ºäº†RecPromptæ¨¡å‹ï¼Œåˆ©ç”¨LLMå¯¹æ–°é—»æ¨èè¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡æ‰‹åŠ¨å’ŒLLMè‡ªåŠ¨ç”Ÿæˆçš„æç¤ºæ¨¡æ¿çš„è¿­ä»£ä¼˜åŒ–è¿‡ç¨‹ï¼Œæ˜¾è‘—æé«˜äº†æ–°é—»æ¨èæ€§èƒ½ï¼Œå°¤å…¶æ˜¯åœ¨ä½¿ç”¨GPT-4è¿›è¡Œè‡ªåŠ¨ç”Ÿæˆçš„æç¤ºæ¨¡æ¿ä¸‹ã€‚ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•å¹¶éæ€»æ˜¯èƒ½è¶…è¶Šä¼ ç»Ÿçš„æ¨èæ–¹æ³•ï¼Œä¸”æ¨èæ•ˆæœå—åˆ°LLMé€‰æ‹©çš„æ˜¾è‘—å½±å“ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10463v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10463.md)  |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **ProTIP: Progressive Tool Retrieval Improves Planning**<br><sub>æœºæ„: Apple  <br>è¿™ç¯‡è®ºæ–‡æå‡ºäº† ProTIPï¼Œä¸ºå¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚è§„åˆ’ä»»åŠ¡ä¸­çš„å·¥å…·æ£€ç´¢å’Œä½¿ç”¨æä¾›äº†ä¸€ç§è¿›æ­¥çš„ç­–ç•¥ã€‚ProTIP çš„æ ¸å¿ƒåœ¨äºæ¸è¿›å¼æ£€ç´¢ã€æœ‰æ•ˆåˆ©ç”¨æ‰§è¡Œå†å²å’Œå®ç°å­ä»»åŠ¡ä¸å·¥å…·åŠŸèƒ½çš„å¯¹é½ã€‚å®éªŒç»“æœå±•ç¤ºå‡º ProTIP æ˜æ˜¾è¶…è¿‡ä¼ ç»Ÿæ–¹æ³•ï¼Œé™ä½äº†å·¥å…·è™šæ„ï¼Œå¹¶æé«˜äº†è§„åˆ’æ•ˆç‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10332v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10332.md)  |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **A Survey on Robotic Manipulation of Deformable Objects: Recent Advances, Open Challenges and New Frontiers**<br><sub>æœºæ„: Tongji University, National Natural Science Foundation of China, Shanghai Municipal Science and Technology Major Project<br>æœ¬ç»¼è¿°å½’çº³äº†æœºå™¨äººæ“ä½œå¯å˜å½¢å¯¹è±¡ï¼ˆDOMï¼‰é¢†åŸŸçš„è¿‘æœŸè¿›å±•ã€å­˜åœ¨çš„æŒ‘æˆ˜å’Œæ–°å‰æ²¿ã€‚ç‰¹åˆ«å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æœºå™¨äººæ“çºµä¸­çš„åˆå§‹è¿›å±•ï¼Œå¹¶æŒ‡å‡ºè¿™ä¸€é¢†åŸŸå€¼å¾—è¿›ä¸€æ­¥ç ”ç©¶çš„é‡è¦æ–¹å‘ã€‚å°½ç®¡ç»¼è¿°äº†å¤§é‡çš„æ–‡çŒ®å¹¶æŒ‡å‡ºäº†æœªæ¥ç ”ç©¶æ–¹å‘ï¼Œä½†å®é™…çš„éƒ¨ç½²ç¤ºä¾‹å’Œå®šé‡è¯„ä¼°æ˜¯æœ‰é™çš„ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10419v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10419.md)  |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **RIGHT: Retrieval-augmented Generation for Mainstream Hashtag Recommendation**<br><sub>æœºæ„: CAS Key Lab of Network Data Science and Technology ICT CAS, University of Chinese Academy of Sciences Beijing China<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ£€ç´¢å¢å¼ºå‹ç”Ÿæˆä¸»æµæ ‡ç­¾æ¨èç³»ç»Ÿï¼ˆRIGHTï¼‰ï¼Œé€šè¿‡ç»“åˆæ£€ç´¢å™¨ã€é€‰æ‹©å™¨å’Œç”Ÿæˆå™¨çš„ä¼˜åŠ¿ï¼Œå…‹æœäº†ç°æœ‰æ–¹æ³•åœ¨ç†è§£æ–°ä¿¡æ¯å’Œè¯†åˆ«ä¸»æµæ ‡ç­¾æ–¹é¢çš„é™åˆ¶ï¼Œå¹¶åœ¨å®éªŒä¸­å–å¾—æ˜¾è‘—æˆæ•ˆã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10466v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10466.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **Generative Context-aware Fine-tuning of Self-supervised Speech Models**<br><sub>æœºæ„: ASAPP, Carnegie Mellon University, Toyota Technological Institute at Chicago<br>è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„è‡ªç›‘ç£è¯­éŸ³æ¨¡å‹å¾®è°ƒæ–¹æ³•ï¼Œå®ƒä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬ä¿¡æ¯ä½œä¸ºä¸Šä¸‹æ–‡ï¼Œä»¥æé«˜ä»»åŠ¡æ‰§è¡Œçš„è¡¨ç°åŠ›ï¼ŒåŒæ—¶åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹å‡å°‘å¯¹é¢å¤–å¤§å‹è¯­è¨€æ¨¡å‹çš„ä¾èµ–å’Œå‡å°‘æ¨ç†æ—¶çš„èµ„æºæ¶ˆè€—ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09895v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.09895.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **GSVA: Generalized Segmentation via Multimodal Large Language Models**<br><sub>æœºæ„: Tsinghua University<br>è®ºæ–‡æå‡ºçš„GSVAæ–¹æ³•é€šè¿‡å­¦ä¹ é¢„æµ‹å¤šä¸ª[SEG]æ ‡è®°å’Œåˆ›æ–°æ€§åœ°ç”Ÿæˆ[REJ]æ ‡è®°ä»¥è§£å†³GRESä»»åŠ¡ä¸­å­˜åœ¨çš„å¤šç›®æ ‡å’Œç©ºç›®æ ‡æŒ‘æˆ˜ï¼Œç›¸è¾ƒäºç°æœ‰æŠ€æœ¯ï¼Œå±•ç°äº†æ˜¾è‘—ä¼˜åŠ¿ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10103v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10103.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **Challenges with unsupervised LLM knowledge discovery**<br><sub>æœºæ„: Google DeepMind, Google Research<br>æœ¬æ–‡é€šè¿‡ç†è®ºè¯æ˜å’Œå®éªŒéªŒè¯ï¼ŒæŒ‘æˆ˜äº†ç°æœ‰æ— ç›‘ç£æ–¹æ³•åœ¨æ¢ç´¢LLMsä¸­éšæ€§çŸ¥è¯†çš„èƒ½åŠ›ï¼Œå¹¶æå‡ºäº†æœªæ¥è¯„ä¼°çŸ¥è¯†å¯å‘æ–¹æ³•æ—¶åº”è€ƒè™‘çš„ç†æ™ºæ£€æŸ¥ã€‚æ€»ä½“ä¸Šï¼Œä½œè€…è®¤ä¸ºæœªæ¥çš„æ— ç›‘ç£æ–¹æ³•å¾ˆå¯èƒ½ä¼šé‡åˆ°ç±»ä¼¼çš„é—®é¢˜ï¼Œå³éš¾ä»¥å‡†ç¡®åŒºåˆ†æ¨¡å‹çŸ¥è¯†å’Œå…¶ä»–ç‰¹å¾ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10029v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10029.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **Faithful Persona-based Conversational Dataset Generation with Large Language Models**<br><sub>æœºæ„: University of Southern California, Google, Information Sciences Institute<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºLLMsçš„æ¡†æ¶ï¼Œç”¨äºç”Ÿæˆã€æ‰©å±•å’Œæ›´æ–°å¤§å‹çš„ä¸ªæ€§åŒ–å¯¹è¯æ•°æ®é›†ï¼Œå¹¶ä¸”é€šè¿‡Generator-Criticæ¶æ„å’Œä¿¡å®æ€§æ ‡å‡†æ¥æé«˜å¯¹è¯çš„è´¨é‡ï¼Œæœ‰æ•ˆåœ°å»ºç«‹äº†Synthetic-Persona-Chatæ•°æ®é›†ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10007v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10007.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **The Art of Balancing: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment**<br><sub>æœºæ„: NLP Group Fudan University, Hikvision Inc  <br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºLoRAMoEçš„æ¨¡å‹ï¼Œç”¨äºè§£å†³å¤§è§„æ¨¡å¾®è°ƒæ•°æ®å¯¼è‡´çš„è¯­è¨€æ¨¡å‹ä¸­çš„ä¸–ç•ŒçŸ¥è¯†é—å¿˜é—®é¢˜ï¼Œå¹¶åœ¨å¤šä»»åŠ¡å­¦ä¹ ä¸­è¡¨ç°å‡ºæ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09979v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.09979.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent**<br><sub>æœºæ„: Google<br>æœ¬è®ºæ–‡é€šè¿‡å®šä¹‰ä¸€ä¸ªèƒ½å¤Ÿè¿›è¡Œæ¨ç†å’Œå¤–éƒ¨çŸ¥è¯†äº’åŠ¨çš„LLMä»£ç†ï¼Œå¹¶é‡‡ç”¨è‡ªæˆ‘æ”¹è¿›ç®—æ³•ï¼Œå®ç°äº†åœ¨åˆæˆé—®ç­”åŸºå‡†æµ‹è¯•ä¸­å°å‹æ¨¡å‹ä¸å¤§å‹æ¨¡å‹ç›¸åª²ç¾çš„è¡¨ç°ã€‚æå‡ºçš„æ–¹æ³•ä¸ä»…æé«˜äº†æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œä¹Ÿå¤§å¤§å‡å°äº†æ¨¡å‹æ‰€éœ€çš„å‚æ•°æ•°é‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10003v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.10003.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement with Large Language Models (LLMs)**<br><sub>æœºæ„: LuleÃ¥ University of Technology Sweden<br>æœ¬æ–‡é€šè¿‡å¼•å…¥ProCoTæ–¹æ³•ï¼Œå±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨LLMä¿ƒè¿›å­¦ç”Ÿæ‰¹åˆ¤æ€§æ€ç»´ä¸å†™ä½œï¼ŒåŒæ—¶é˜²æ­¢ä½œå¼Šã€‚è¿™ç§æ–¹æ³•æœ‰åŠ©äºæ•™è‚²è€…æ›´å¥½åœ°åˆ©ç”¨è¿™äº›æŠ€æœ¯å·¥å…·ï¼Œå¹¶åŸ¹å…»å­¦ç”Ÿæˆä¸ºæ›´å¥½çš„æ‰¹åˆ¤æ€§æ€ç»´è€…ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09801v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.09801.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models**<br><sub>æœºæ„: Fudan University<br>æœ¬è®ºæ–‡é¦–æ¬¡ç³»ç»Ÿåœ°ç ”ç©¶äº†ä»æ•ˆç‡è§’åº¦å‡ºå‘ï¼ŒåŸºäº"é”™è¿‡"çš„è¯­è¨€æ¨¡å‹çš„è„†å¼±æ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆå’Œé€šç”¨çš„æ•ˆç‡é²æ£’æ€§è¯„ä¼°æ¡†æ¶No-Skimï¼Œä»¥ç”Ÿæˆå¢åŠ è®¡ç®—å¤æ‚åº¦çš„å¯¹æŠ—æ€§è¾“å…¥ã€‚åŒæ—¶ï¼Œè¯¥æ¡†æ¶è¿˜é€šè¿‡ä¸åŒçš„æ’ä»¶æ¨¡å—è¿›è¡Œäº†æ¨¡å—åŒ–è®¾è®¡ï¼Œè¿™äº›æ¨¡å—åœ¨ä¸åŒçš„å®é™…æƒ…æ™¯ä¸‹å·¥ä½œï¼Œè¯„ä¼°å¯ä»¥åœ¨ä¸‰ç§ä¸åŒçš„çŸ¥è¯†æ°´å¹³ä¸‹è¿›è¡Œã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09494v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.09494.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know**<br><sub>æœºæ„: Apple<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åä¸ºKGLensçš„æ¡†æ¶ï¼Œç”¨äºè¯„ä¼°LLMä¸­çš„äº‹å®çŸ¥è¯†ã€‚KGLensåˆ©ç”¨KGç»“æ„ç”Ÿæˆè‡ªç„¶è¯­è¨€é—®é¢˜å¹¶è¿›è¡Œè¯„ä¼°ï¼ŒODè¾…ä»¥å‚æ•°åŒ–çš„KGå’Œå›¾æŒ‡å¯¼çš„QGç­–ç•¥ä»¥æé«˜è‡ªç„¶é—®é¢˜çš„ç”Ÿæˆè´¨é‡å’Œè¯„ä¼°è¿‡ç¨‹çš„æ•ˆç‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11539v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.11539.md)  |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION**<br><sub>æœºæ„: OpenAI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)</div><div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/openai/weak-to-strong)</div><div style='min-width:85px;'>[![Blog](https://img.shields.io/badge/Blog-Posts-yellow?logo=rss)](https://mp.weixin.qq.com/s/f6YW-CxnLhnfMWTLg4M4Cw)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Forbidden Facts: An Investigation of Competing Objectives in Llama-2**<br><sub>æœºæ„: MIT<br>è¿™ç¯‡è®ºæ–‡é€šè¿‡ç ”ç©¶æ¨¡å‹åœ¨ç¦æ­¢äº‹å®ä»»åŠ¡ä¸‹çš„è¡Œä¸ºï¼Œè§£æäº†Llama-2-chatæ¨¡å‹å¦‚ä½•å¤„ç†ç›¸äº’ç«äº‰çš„ç›®æ ‡ï¼Œå¹¶å¯¹å®ƒçš„åˆ†ææå‡ºäº†æ–°çš„æ‰‹æ³•ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08793v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08793.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning**<br><sub>æœºæ„: National University of Singapore, University of Illinois Urbana-Champaign, Microsoft  <br>æœ¬æ–‡ä¸­æå‡ºçš„TAP4LLMæ¡†æ¶é€šè¿‡é‡‡æ ·ã€å¢å¼ºå’Œæ‰“åŒ…åŠç»“æ„åŒ–æ•°æ®ï¼Œæ˜¾è‘—æå‡äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è¡¨æ ¼æ¨ç†ä»»åŠ¡ä¸­çš„æ€§èƒ½ï¼Œå¹¶ä¸”å¯ä»¥ä½œä¸ºæ’ä»¶æä¾›ç»™ä¸åŒç»„ä»¶ï¼Œç”¨äºå¢å¼ºLLMså¯¹äºç»“æ„åŒ–æ•°æ®çš„ç†è§£ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09039v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.09039.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Entity-Augmented Code Generation**<br><sub>æœºæ„: JetBrains<br>è®ºæ–‡ä¸ºè§£å†³åˆ©ç”¨å¤–éƒ¨å®ä½“è¿›è¡Œä»£ç ç”Ÿæˆçš„ä»»åŠ¡æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„æ¶æ„ã€‚è¯¥æ¶æ„èƒ½åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„å‰æä¸‹æ‰©å±•ï¼Œé€šè¿‡å°†å®ä½“æ£€ç´¢å™¨æ³¨å…¥åˆ°è§£ç å™¨è€Œéç¼–ç å™¨ä¸­ï¼Œæ¨¡å‹å¯ä»¥ä¸€æ¬¡æ€§æŸ¥çœ‹æ‰€æœ‰å®ä½“å¹¶ç›´æ¥ä½¿ç”¨å®ƒä»¬ã€‚æ–°æ¶æ„ä¸ä»…è§£å†³äº†ç°æœ‰æ¨¡å‹çš„é™åˆ¶ï¼Œè¿˜åœ¨å¤šä¸ªå®éªŒåœºæ™¯ä¸­å±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08976v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08976.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Math-Shepherd: A Label-Free Step-by-Step Verifier for LLMs in Mathematical Reasoning**<br><sub>æœºæ„: Peking University, DeepSeek-AI, The University of Hong Kong<br>MATH-SHEPHERDé€šè¿‡è‡ªåŠ¨ç”Ÿæˆç›‘ç£æ•°æ®è®­ç»ƒLLMsï¼Œæ¥è§£å†³é«˜æˆæœ¬äººåŠ›æ ‡æ³¨çš„é—®é¢˜ï¼Œå¹¶æé«˜äº†LLMsåœ¨å¤æ‚æ•°å­¦é—®é¢˜ä¸Šçš„å‡†ç¡®æ€§ã€‚è¿™ä¸€æˆæœä¸ºLLMsçš„è¿›æ­¥å’Œå®é™…åº”ç”¨å¼€è¾Ÿäº†æ–°çš„å¯èƒ½æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08935v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08935.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent**<br><sub>æœºæ„: Shanghai Jiao Tong University<br>è®ºæ–‡å»ºè®®é€šè¿‡MathAgentæ¡†æ¶ï¼Œå³Planner-Reasoner-Executor-Reflector (PRER)ï¼Œæå‡LLMsè§£å†³å¤æ‚æ•°å­¦é—®é¢˜çš„èƒ½åŠ›ã€‚é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºå¤šä¸ªé˜¶æ®µå¹¶æ¨¡æ‹Ÿäººç±»è§£é¢˜è¿‡ç¨‹ï¼ŒMathAgentèƒ½æ˜¾è‘—æé«˜å¯¹æŒ‘æˆ˜æ€§æ•°å­¦æ•°æ®é›†çš„è§£å†³èƒ½åŠ›ï¼Œå°¤å…¶æ˜¯åœ¨ä¼°ç®—å’Œç»¼åˆèƒ½åŠ›è¦æ±‚è¾ƒé«˜çš„é¢†åŸŸã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08926v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08926.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning**<br><sub>æœºæ„: Hong Kong University of Science and Technology, Microsoft Research<br>è¿™ç¯‡è®ºæ–‡æå‡ºäº†CoT-Maxï¼Œä¸€ä¸ªé€šè¿‡ç²—åˆ°ç»†çš„å‰ªææŠ€æœ¯æ¥å¢å¼ºLLMsæ•°å­¦æ¨ç†èƒ½åŠ›çš„æ–¹æ³•ï¼Œæœ‰æ•ˆåœ°æé«˜äº†å°‘æ ·æœ¬å­¦ä¹ åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸­çš„æ•ˆæœã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08901v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08901.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Weight subcloning: direct initialization of transformers using larger pretrained ones**<br><sub>æœºæ„: Apple<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„æƒé‡å­å…‹éš†ï¼ˆweight subcloningï¼‰æŠ€æœ¯ï¼Œç”¨ä»¥ä»è¾ƒå¤§çš„é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–è¾ƒå°çš„å˜æ¢å™¨æ¨¡å‹ï¼Œæ˜¾è‘—æé«˜äº†è®­ç»ƒé€Ÿåº¦ï¼Œå¹¶ä½¿å¾—å…¨æ–°çš„æ¨¡å‹å³ä½¿åœ¨ä½è®¡ç®—èµ„æºæ¡ä»¶ä¸‹ä¹Ÿèƒ½å¾—åˆ°é«˜æ•ˆè®­ç»ƒã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09299v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.09299.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention**<br><sub>æœºæ„: Tencent AI Lab Seattle<br>æœ¬æ–‡æå‡ºçš„Zebraæ¨¡å‹é€šè¿‡ä½¿ç”¨åˆ†ç»„çš„å±€éƒ¨-å…¨å±€æ³¨æ„åŠ›å±‚ï¼Œæœ‰æ•ˆåœ°é™ä½äº†è®¡ç®—å’Œå†…å­˜éœ€æ±‚ï¼Œå¹¶åœ¨é•¿çŸ­åºåˆ—å¤„ç†ä¸Šå±•ç¤ºäº†å“è¶Šçš„æ€§èƒ½ã€‚ç ”ç©¶å›¢é˜Ÿé€šè¿‡ä¸€ç³»åˆ—å®éªŒéªŒè¯äº†æ¨¡å‹çš„æ•ˆæœï¼Œè¯æ˜äº†Zebraæ¶æ„çš„ä¼˜åŠ¿ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08618v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08618.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **StemGen: A music generation model that listens**<br><sub>æœºæ„: SAMI, ByteDance Inc.<br>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„éè‡ªå›å½’çš„è¯­è¨€æ¨¡å‹æ–¹æ³•ç”¨äºéŸ³ä¹ç”Ÿæˆï¼Œä¼˜åŒ–äº†å¤šå£°é“çš„å¤„ç†å’ŒéŸ³ä¹ä¸ä¸Šä¸‹æ–‡ä¿¡æ¯çš„ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡å®¢è§‚å’Œä¸»è§‚è¯„ä¼°è¯æ˜äº†æ¨¡å‹ç”Ÿæˆçš„éŸ³ä¹è´¨é‡å’Œä¸ä¸Šä¸‹æ–‡ä¿¡æ¯çš„å¥‘åˆç¨‹åº¦ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08723v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08723.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **CogAgent: A Visual Language Model for GUI Agents**<br><sub>æœºæ„: Tsinghua University, Zhipu AI<br>CogAgent æ‰“ç ´äº†çº¯æ–‡æœ¬è¾“å…¥æ–¹å¼çš„å±€é™æ€§ï¼Œé€šè¿‡ç»“åˆé«˜ä½åˆ†è¾¨ç‡çš„å½±åƒç¼–ç å™¨å’Œè§†è§‰è¯­è¨€æ¨¡å‹ï¼Œé«˜æ•ˆåœ°è§£å†³äº†åœ¨å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆGUIï¼‰ä¸­ç†è§£å’Œå¯¼èˆªçš„æŒ‘æˆ˜ï¼ŒåŒæ—¶åœ¨ä¹ä¸ªè§†è§‰é—®ç­”åŸºå‡†æµ‹è¯•ä¸­å–å¾—å›½é™…é¢†å…ˆæ°´å¹³ï¼Œæ¨åŠ¨äº†VLMåœ¨AIä»£ç†ç ”ç©¶å’Œåº”ç”¨æ–¹é¢çš„æœªæ¥å‘å±•ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08914v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08914.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/THUDM/CogVLM)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **TinyGSM: achieving >80% on GSM8k with small language models**<br><sub>æœºæ„: Carnegie Mellon University, Microsoft Research  <br>è¿™ç¯‡è®ºæ–‡é€šè¿‡åˆ›å»ºä¸€ä¸ªåˆæˆçš„æ•°å­¦é—®é¢˜æ•°æ®é›†TinyGSMåŠå…¶å¯¹åº”çš„Pythonè§£å†³æ–¹æ¡ˆï¼ŒæˆåŠŸä½¿å°å‹è¯­è¨€æ¨¡å‹åœ¨GSM8Kæ•°å­¦é—®é¢˜æ¨ç†åŸºå‡†æµ‹è¯•ä¸Šçš„å‡†ç¡®ç‡è¶…è¿‡äº†80%ï¼Œå±•ç¤ºäº†é€šè¿‡é«˜è´¨é‡æ•°æ®é›†å’ŒéªŒè¯å™¨ç­–ç•¥æ˜¾è‘—æé«˜äº†å°å‹æ¨¡å‹æ€§èƒ½çš„å¯è¡Œæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09241v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.09241.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Self-Evaluation Improves Selective Generation in Large Language Models**<br><sub>æœºæ„: Google DeepMind, Google Research<br>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œé€šè¿‡æŒ‡å¯¼LLMè¿›è¡Œè‡ªæˆ‘è¯„ä¼°ï¼Œä»¥æé«˜å…¶åœ¨é€‰æ‹©æ€§ç”Ÿæˆåœºæ™¯ä¸­è¾“å‡ºå†…å®¹è´¨é‡çš„æ ¡å‡†ã€‚å®éªŒè¯æ˜ï¼Œè¯¥æ–¹æ³•å¯ä»¥æé«˜LLMç”Ÿæˆå†…å®¹çš„å‡†ç¡®æ€§å’Œæ•´ä½“è´¨é‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09300v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.093.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft**<br><sub>æœºæ„: CUHK-SenseTime Joint Laboratory, Shanghai AI Laboratory, Tsinghua University<br>Auto MC-Rewardæ˜¯ä¸€ç§å…ˆè¿›çš„å­¦ä¹ ç³»ç»Ÿï¼Œåˆ©ç”¨LLMsä»¥è‡ªåŠ¨æ–¹å¼è®¾è®¡é’ˆå¯¹Minecraftä»»åŠ¡çš„å¯†é›†å‹å¥–åŠ±ï¼Œé€šè¿‡LLMsçš„ç†è§£å’Œç»éªŒæ€»ç»“èƒ½åŠ›ï¼Œæœ‰æ•ˆåœ°æé«˜äº†ä»£ç†åœ¨å¤æ‚ç¯å¢ƒä¸­å­¦ä¹ æ–°è¡Œä¸ºå’Œå®Œæˆé•¿æœŸä»»åŠ¡çš„èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09238v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.09238.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **The Earth is Flat because...: Investigating LLMs' Belief towards Misinformation via Persuasive Conversation**<br><sub>æœºæ„: Tsinghua University, Stanford University, Nanyang Technological University<br>æœ¬è®ºæ–‡ä¸ºé¦–æ¬¡å…¨é¢ç ”ç©¶LLMsé¢å¯¹äº‹å®é”™è¯¯ä¿¡æ¯åœ¨åŠè¯´æ€§å¯¹è¯è®¾ç½®ä¸­çš„é²æ£’æ€§ï¼Œå¹¶æ­ç¤ºäº†LLMså¯¹åŠè¯´æ€§é”™è¯¯ä¿¡æ¯çš„æ˜“æ„Ÿæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09085v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.09085.md)  |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Towards Verifiable Text Generation with Evolving Memory and Self-Reflection**<br><sub>æœºæ„: Peking University, Chinese Academy of Sciences, Baidu Inc<br>VTGé€šè¿‡æ¼”åŒ–çš„é•¿çŸ­æœŸè®°å¿†å’Œè‡ªæˆ‘åæ€çš„æ–¹æ³•æ¥æå‡LLMsç”Ÿæˆæ–‡æœ¬æ—¶çš„å¯é æ€§å’ŒéªŒè¯æ€§ï¼Œå¯¹å¤æ‚çš„æ³¨æ„åŠ›è½¬ç§»é—®é¢˜å’Œæ–‡æ¡£æ£€ç´¢çš„æŒ‘æˆ˜æœ‰ç€æœ‰æ•ˆçš„åº”å¯¹ç­–ç•¥ï¼Œå¹¶ä¸”é€šè¿‡å®éªŒè·å¾—äº†éªŒè¯ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09075v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.09075.md)  |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **E&V: Prompting Large Language Models to Perform Static Analysis by Pseudo-code Execution and Verification**<br><sub>æœºæ„: UC Riverside, Microsoft Research<br>æœ¬è®ºæ–‡é€šè¿‡æå‡ºE&Væ–¹æ³•ï¼Œå±•ç¤ºäº†LLMsåœ¨æ‰§è¡Œä¼ªä»£ç é™æ€åˆ†æå’Œè‡ªæˆ‘éªŒè¯ä¸­çš„æ½œåŠ›ã€‚è¯¥æ–¹æ³•ä¸ä»…æé«˜äº†é™æ€åˆ†æçš„çµæ´»æ€§å’Œç²¾å‡†åº¦ï¼Œè¿˜å‡å°‘äº†ç¼–å†™é™æ€åˆ†æå·¥å…·éœ€è¦çš„äººåŠ›å’Œä¸“ä¸šçŸ¥è¯†ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08477v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08477.md)  |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision**<br><sub>æœºæ„: Peking University<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»“åˆLLMså¢å¼ºæç¤ºå’Œå¤šæºç›‘ç£çš„çŸ¥è¯†æ„ŸçŸ¥å¤ä»£æ–‡ç‰©å›¾åƒåˆæˆæ–¹æ³•ï¼Œè§£å†³äº†ç°æœ‰æ–‡æœ¬åˆ°å›¾åƒåˆæˆæ–¹æ³•åœ¨è€ƒå¤é¢†åŸŸåº”ç”¨æ—¶ç¼ºä¹é¢†åŸŸçŸ¥è¯†çš„é—®é¢˜ï¼Œå¹¶åœ¨è´¨é‡å’Œå†å²çŸ¥è¯†å¯¹é½æ–¹é¢å–å¾—äº†æ˜¾è‘—è¿›æ­¥ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08056v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08056.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/danielwusg/artifact_diffusion)</div> |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models**<br><sub>æœºæ„: University of Southern California, Amazon.com Inc.<br>æ–‡ç« é’ˆå¯¹ç°æœ‰çš„ç½‘ç»œæœ‰å®³å†…å®¹è‡ªåŠ¨æ¢æµ‹é¢ä¸´çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªç§°ä¸ºBD-LLMçš„æ–°æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¸€ä¸ªæ–°çš„æ–¹æ³•DToTæ¥æå‡LLMsåœ¨æœ‰å®³å†…å®¹æ£€æµ‹ä»»åŠ¡ä¸­çš„æ•ˆèƒ½å’Œè½¬ç§»æ€§ï¼Œå¹¶å°†ä¼˜åŒ–æ¨¡å‹å‹ç¼©ä»¥ä¾¿æ›´æœ‰æ•ˆåœ°éƒ¨ç½²ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08303v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08303.md)  |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention**<br><sub>æœºæ„: The Swiss AI Lab IDSIA USI & SUPSI, AI Initiative KAUST, Center for Brain Science Harvard University<br>SwitchHeadæ˜¯ä¸€ç§æ–°é¢–çš„æ–¹æ³•ï¼Œå®ƒé€šè¿‡ä¼˜åŒ–å¤šå¤´è‡ªæ³¨æ„åŠ›ç»“æ„ä¸­çš„èµ„æºä½¿ç”¨ï¼Œå®ç°äº†èµ„æºæ¶ˆè€—çš„é™ä½åŒæ—¶ä¿æŒäº†æ¨¡å‹æ€§èƒ½ã€‚è¯¥æ–¹æ³•å…·æœ‰å®é™…åº”ç”¨æ½œåŠ›ï¼Œå°¤å…¶å¯¹äºèµ„æºæœ‰é™çš„ç ”ç©¶äººå‘˜å’Œæœºæ„è€Œè¨€ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07987v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.07987.md)  |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **LDM$^2$: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement**<br><sub>æœºæ„: University of Chinese Academy of Sciences<br>è¯¥è®ºæ–‡æå‡ºäº†LDM2æ¨¡å‹ï¼Œå®ƒä½¿ç”¨åŠ¨æ€å†…å­˜æœºåˆ¶å’Œæ ‘æ¢ç´¢ç­–ç•¥æ¥å¢å¼ºLLMsçš„å†³ç­–èƒ½åŠ›ï¼Œä½¿å…¶èƒ½å¤Ÿé€‚åº”æ›´å¤æ‚å’ŒæœªçŸ¥çš„ç¯å¢ƒï¼Œå¹¶å®ç°åŠ¨æ€å­¦ä¹ èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08402v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.08402.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **Alignment for Honesty**<br><sub>æœºæ„: Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory, Fudan University<br>è®ºæ–‡æå‡ºäº†ä¸äººç±»çš„è¯šå®æ€§å¯¹é½çš„æ¦‚å¿µï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†æŒ‘æˆ˜å’Œè§£å†³æ–¹æ³•ã€‚é€šè¿‡æ­£å¼å®šä¹‰é—®é¢˜ã€æå‡ºæ–°æ–¹æ³•å’Œå»ºç«‹è¯„ä¼°æ¡†æ¶ï¼Œè®ºæ–‡ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹ä¸­çš„è¯šå®æ€§å¯¹é½æä¾›äº†å…¨é¢çš„è§£å†³æ–¹æ¡ˆã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07000v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.07.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/GAIR-NLP/alignment-for-honesty)</div> |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **VILA: On Pre-training for Visual Language Models**<br><sub>æœºæ„: NVIDIA, MIT  <br>VILAåˆ©ç”¨æ”¹è¿›çš„é¢„è®­ç»ƒç­–ç•¥ï¼Œåœ¨å¤šç§è§†è§‰è¯­è¨€ä»»åŠ¡ä¸­æ˜¾ç¤ºå‡ºå“è¶Šçš„æ€§èƒ½ï¼Œä¸ºæœªæ¥è§†è§‰è¯­è¨€æ¨¡å‹çš„è®¾è®¡æä¾›äº†å®ç”¨æŒ‡å—ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07533v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.07533.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection**<br><sub>æœºæ„: Shanghai Jiao Tong University<br>æœ¬æ–‡ä»ç¤ºä¾‹é—´å…³ç³»çš„è§’åº¦ç ”ç©¶ICLï¼Œæå‡ºé€šè¿‡æœ€å°åŒ–ç¼–è¾‘æ–‡æœ¬ä»¥æ„é€ Comparable Demonstrationsï¼ˆCDsï¼‰æ¥å‡è½»æ½œåœ¨çš„ç¤ºä¾‹åå€šï¼Œå®éªŒè¯æ˜äº†å…¶åœ¨OODæƒ…å½¢ä¸‹çš„æ€§èƒ½å¢ç›Šï¼Œè¡¨æ˜äº†CDsåœ¨ç®€åŒ–ä»»åŠ¡ä¸­å°¤å…¶å¿…è¦ï¼Œå¹¶å±•ç¤ºäº†å…¶ç›¸å¯¹äºç¤ºä¾‹æ•°çš„ç¨³å¥æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07476v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.07476.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **LLMEval: A Preliminary Study on How to Evaluate Large Language Models**<br><sub>æœºæ„: Fudan University, Shanghai Jiaotong University  <br>è®ºæ–‡é’ˆå¯¹å¦‚ä½•è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œå¯¹å¤šç§è¯„ä¼°æ ‡å‡†ã€ä¸åŒç±»å‹çš„è¯„ä¼°è€…ã€è¯„åˆ†æ–¹æ³•å’Œæ’åç³»ç»Ÿè¿›è¡Œäº†æ¯”è¾ƒå’Œåˆ†æï¼Œæå‡ºäº†æ–°çš„è¯„ä¼°æ•°æ®é›†LLMEvalï¼Œå¯¹20ä¸ªLLMsè¿›è¡Œäº†è¯„ä¼°ï¼Œç”Ÿæˆäº†å¤§é‡çš„æ‰‹åŠ¨å’Œè‡ªåŠ¨è¯„ä¼°ç»“æœã€‚è¯¥ç ”ç©¶ä¸ºæœªæ¥çš„LLMè¯„ä¼°æä¾›äº†æœ‰ç›Šçš„æ´è§å’Œç»“è®ºã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07398v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.07398.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **Efficient Few-Shot Clinical Task Adaptation with Large Language Models**<br><sub>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåœ¨å°‘æ ·æœ¬çš„åŒ»å­¦å›¾åƒåˆ†ç±»ä¸­é€šè¿‡å†·å†»ä¸€éƒ¨åˆ†ç½‘ç»œå±‚è¿›è¡Œé«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¹¶ä¸”å¼•å…¥äº†å¤§å‹è¯­è¨€æ¨¡å‹æ¥ä¸Šä¸‹æ–‡åŒ–æ ‡ç­¾ï¼Œä»¥æä¾›æœ‰æ•ˆçš„è¯­ä¹‰æŒ‡å¯¼ã€‚æ–¹æ³•åœ¨æŒ‘æˆ˜èµ›ä¸­å–å¾—äº†ä¼˜å¼‚çš„æˆç»©ï¼Œè¡¨æ˜åœ¨å¤„ç†å°‘æ ·æœ¬åœºæ™¯ä¸‹è‡ªç„¶å›¾åƒæ¨¡å‹åˆ°åŒ»å­¦å›¾åƒä»»åŠ¡çš„é€‚é…é—®é¢˜æ—¶å…·æœ‰å¾ˆé«˜çš„æœ‰æ•ˆæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07125v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.07125.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **Tell, don't show: Declarative facts influence how LLMs generalize**<br><sub>æœºæ„: Apollo Research, University of Oxford<br>æœ¬æ–‡ç ”ç©¶äº†åŸ¹è®­æ•°æ®ä¸­å£°æ˜æ€§é™ˆè¿°ä¸ç»Ÿè®¡æ¨¡å¼æˆ–â€œç¨‹åºâ€ç¤ºä¾‹ç›¸å†²çªæ—¶æ¨¡å‹çš„æ³›åŒ–æƒ…å†µã€‚æ‰€å¾—ç»“æœå¯¹äºAIé£é™©ï¼ˆå…³äºâ€œèƒŒå›è½¬æŠ˜â€ï¼‰å’Œå…¬å¹³æ€§æœ‰é‡è¦å½±å“ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07779v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.07779.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **diff History for Long-Context Language Agents**<br><sub>æœºæ„: New York University<br>è®ºæ–‡æå‡ºå¹¶éªŒè¯äº†ä½¿ç”¨diffå†å²æ¥æé«˜å¯¹é•¿äº¤äº’å†å²çš„æ¨¡å‹å¤„ç†èƒ½åŠ›ã€‚è¿™ä¸€æ–¹æ³•æ˜¾è‘—æå‡äº†æ¨¡å‹åœ¨å¤æ‚å†³ç­–ä»»åŠ¡ä¸­çš„è¡¨ç°ï¼Œå¹¶èƒ½æœ‰æ•ˆæ‰©å¤§æ¨¡å‹å¯å¤„ç†çš„å†å²é•¿åº¦ï¼Œä¸ºé•¿æ—¶é—´åºåˆ—å†³ç­–ä»£ç†çš„è®¾è®¡æä¾›äº†æ–°æ€è·¯ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07540v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.0754.md)  |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **LLM in a flash: Efficient Large Language Model Inference with Limited Memory**<br><sub>æœºæ„: Apple<br>è¿™ä»½ç ”ç©¶æä¾›äº†ä¸€ä¸ªåˆ›æ–°ä¸”å®ç”¨çš„è§£å†³æ–¹æ¡ˆï¼Œä¸ä»…èƒ½æœ‰æ•ˆé™ä½åœ¨å†…å­˜å—é™è®¾å¤‡ä¸Šè¿è¡Œå¤§å‹è¯­è¨€æ¨¡å‹æ—¶çš„æ•°æ®è´Ÿè½½ï¼Œè¿˜èƒ½æ˜¾è‘—æå‡æ¨ç†é€Ÿåº¦ï¼Œåœ¨å®é™…åº”ç”¨ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11514v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.11514.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Oracle-based Protocol Testing with Eywa**<br><sub>æœºæ„: Microsoft Research<br>æœ¬æ–‡ä»‹ç»äº†åŸºäºç¥è°•çš„æµ‹è¯•æ–¹æ³•ï¼Œå……åˆ†åˆ©ç”¨LLMså»ºç«‹äº†ä¸°å¯Œçš„åè®®è¡Œä¸ºæ¨¡å‹ï¼Œå¹¶é€šè¿‡ç¬¦å·æ‰§è¡Œå’Œä¼ ç»Ÿæµ‹è¯•ç”Ÿæˆæ–¹æ³•ç›¸ç»“åˆï¼Œæå‡äº†ç½‘ç»œåè®®æµ‹è¯•ç”¨ä¾‹çš„è‡ªåŠ¨ç”Ÿæˆå’Œè¦†ç›–é¢ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06875v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.06875.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Extracting Self-Consistent Causal Insights from Users Feedback with LLMs and In-context Learning**<br><sub>æœºæ„: Microsoft, Microsoft Research<br>è¯¥ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°æ¡†æ¶ï¼Œä½¿ç”¨LLMså’ŒICLä»ç”¨æˆ·åé¦ˆä¸­æå–è‡ªæ´½çš„å› æœè§è§£ï¼Œä»¥æ”¯æŒå¾®è½¯Feedback Hubçš„åˆ†æã€‚è¯¥æ¡†æ¶é‡‡ç”¨åˆ›æ–°çš„è‡ªæ´½æ€§å’Œæç¤ºé›†åˆæŠ€æœ¯ä»¥æŠ‘åˆ¶LLMsçš„å¹»è§‰å’Œé”™è¯¯æ¨ç†ï¼Œå¹¶æå‡ºäº†ä¸¤ç§å¯å‘å¼æ–¹æ³•æ¥è¯„ä¼°åé¦ˆçš„ä¿¡æ¯ä¸°å¯Œåº¦ã€‚å®éªŒæ˜¾ç¤ºï¼Œè¯¥æ–¹æ³•èƒ½æœ‰æ•ˆåœ°æå–å› æœè§è§£å’Œæ–°çš„bugï¼Œå¹¶æœ‰åŠ©äºå¾®è½¯å·¥ç¨‹å¸ˆä¼˜å…ˆå¤„ç†ä¿¡æ¯é‡ä¸°å¯Œçš„åé¦ˆã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06820v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.0682.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **On Meta-Prompting**<br><sub>æœºæ„: Microsoft  <br>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºèŒƒç•´è®ºçš„ç†è®ºæ¡†æ¶æ¥æ¦‚æ‹¬å’Œæç»˜è‡ªåŠ¨åŒ–æç¤ºæ–¹æ³•ï¼Œé€šè¿‡åœ¨æ„æƒ³åŠ›å’Œåˆ›é€ åŠ›è¿™ä¸¤ä¸ªé¢†åŸŸçš„å®éªŒï¼Œå±•ç¤ºäº†meta-promptingæ¯”ä¼ ç»Ÿå›ºå®šæç¤ºæ–¹æ³•æ›´èƒ½ç”Ÿæˆç”¨æˆ·åå¥½çš„è¾“å‡ºã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06562v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.06562.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Honeybee: Locality-enhanced Projector for Multimodal LLM**<br><sub>æœºæ„: Kakao Brain<br>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°å‹çš„å±€éƒ¨æ€§å¢å¼ºæŠ•å½±å™¨è®¾è®¡ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨å¤„ç†è§†è§‰ç‰¹å¾å±€éƒ¨æ€§ä¸Šçš„ä¸è¶³ï¼Œå¹¶æœ‰æ•ˆåˆ©ç”¨äº†å¤šé¢å‘æŒ‡ä»¤æ•°æ®é›†ï¼Œæœ€ç»ˆä½¿å¾—Honeybeeæ¨¡å‹åœ¨å¤šä¸ªMLLMåŸºå‡†æµ‹è¯•ä¸­å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06742v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.06742.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/kakaobrain/honeybee)</div> |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples**<br><sub>æœºæ„: Xiamen University, Tencent YouTu Lab<br>è¿™é¡¹å·¥ä½œé€šè¿‡æå‡ºMMICTï¼Œå±•ç¤ºäº†åœ¨å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹ä¸Šè¿ç”¨ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ä»¥å¢å¼ºå¾®è°ƒæ€§èƒ½çš„æ–°èŒƒå¼ã€‚é€šè¿‡è®¾è®¡M-Hubè¿™ä¸€å¤šåŠŸèƒ½æ¨¡å—å¹¶é€šè¿‡å„ç§ä¸Šä¸‹æ–‡ç¤ºèŒƒå®éªŒï¼Œç ”ç©¶æ­ç¤ºäº†ä¸Šä¸‹æ–‡å­¦ä¹ åœ¨æ”¹å–„å¤šæ¨¡æ€ä»»åŠ¡æ€§èƒ½ä¸­çš„æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06363v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.06363.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Dense X Retrieval: What Retrieval Granularity Should We Use?**<br><sub>æœºæ„: University of Washington, Tencent AI Lab<br>æœ¬æ–‡æå‡ºå‘½é¢˜ä½œä¸ºä¸€ç§æ–°å‹ç¨ å¯†æ£€ç´¢å•å…ƒï¼Œå…¶åœ¨å‡å°‘æ‰€æ£€ç´¢æ–‡æœ¬ä¸­æ— å…³ä¿¡æ¯çš„åŒæ—¶ï¼Œæé«˜äº†ä¸‹æ¸¸é—®ç­”ä»»åŠ¡çš„æ€§èƒ½å’Œè·¨ä»»åŠ¡æ³›åŒ–èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06648v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.06648.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes**<br><sub>æœºæ„: Zhejiang University, Alibaba Group<br>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„è”é‚¦å…¨å‚æ•°å¾®è°ƒæ–¹æ³•â€”â€”FedKSeedï¼Œé€šè¿‡ZOOä¸æœ‰é™ç»„ç§å­ç»“åˆï¼Œæ˜¾è‘—é™ä½äº†æ•°åäº¿å¤§å°LLMså…¨å‚æ•°å¾®è°ƒæ‰€éœ€çš„é€šä¿¡å¼€é”€ï¼ŒåŒæ—¶å®ç°äº†è¾ƒé«˜çš„æ¨¡å‹ç²¾ç¡®åº¦å’Œè®¡ç®—æ•ˆç‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06353v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.06353.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Unlocking Anticipatory Text Generation: A Constrained Approach for Faithful Decoding with Large Language Models**<br><sub>æœºæ„: Salesforce AI Research<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§é€šè¿‡è€ƒè™‘æœªæ¥çº¦æŸæ»¡è¶³æ¥æ”¹å–„å¤§å‹è¯­è¨€æ¨¡å‹è§£ç æ–¹æ³•çš„æ–°é€”å¾„ã€‚æå‡ºçš„æ­£å¼æ–¹æ³•å’Œè¯„åˆ†æœºåˆ¶é€šè¿‡ä¸LLMsçš„åŸºå‡†æµ‹è¯•ï¼Œå¯ä»¥æ˜¾è‘—æé«˜æ–‡æœ¬ç”Ÿæˆçš„è´¨é‡å’Œæ§åˆ¶ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06149v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.06149.md)  |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **"What's important here?": Opportunities and Challenges of Using LLMs in Retrieving Information from Web Interfaces**<br><sub>æœºæ„: Carnegie Mellon University<br>æœ¬æ–‡ç ”ç©¶äº†LLMsåœ¨ä»Webç•Œé¢æ£€ç´¢ä¿¡æ¯ä¸­çš„åº”ç”¨æ½œåŠ›å’Œé¢ä¸´çš„æŒ‘æˆ˜ã€‚é€šè¿‡ä¸€ç³»åˆ—å®éªŒï¼Œæ­ç¤ºäº†æ¨¡å‹æ€§èƒ½çš„å…³é”®å› ç´ åŠå…¶é™åˆ¶ï¼Œå¹¶ä¸ºæœªæ¥å·¥ä½œæŒ‡æ˜äº†æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06147v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.06147.md)  |
| <span style='display: inline-block; width: 42px;'>12-10</span> | **Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs**<br><sub>æœºæ„: Microsoft Israel<br>è¿™é¡¹ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºå®ƒå¯¹æ¯”äº†ç»†åŒ–è®­ç»ƒå’ŒRAGä¸¤ç§æ–¹æ³•å¯¹äºLLMsçŸ¥è¯†æ³¨å…¥èƒ½åŠ›çš„å½±å“ï¼Œå¹¶å‘ç°RAGåœ¨æ³¨å…¥æ–°çš„å’Œå·²æœ‰çš„çŸ¥è¯†æ–¹é¢è¡¨ç°æ›´ä½³ã€‚ç ”ç©¶ä½¿ç”¨äº†åˆ›æ–°çš„æ•°æ®é›†å’Œè¯„ä¼°æ–¹æ³•ï¼Œç¡®ä¿äº†ç†è®ºå‘ç°çš„å®ç”¨æ€§å’Œå¯è¡Œæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05934v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.05934.md)  |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Context Tuning for Retrieval Augmented Generation**<br><sub>æœºæ„: Apple  <br>æœ¬è®ºæ–‡é€šè¿‡å¼•å…¥ä¸Šä¸‹æ–‡è°ƒä¼˜è¿™ä¸€æ–°é¢–ç»„ä»¶ï¼Œæé«˜äº†åŸºäºæ£€ç´¢çš„å¢å¼ºè®¡åˆ’ï¼ˆRAG-based planningï¼‰çš„æ•ˆæœï¼Œä½¿å…¶èƒ½å¤„ç†ä¸å®Œæ•´æˆ–ä¸æ˜ç¡®çš„æŸ¥è¯¢ï¼ŒåŒæ—¶è¿˜é™ä½äº†å¹»è§‰æ€§é”™è¯¯çš„äº§ç”Ÿã€‚ç ”ç©¶å¯¹æ¯”äº†ä¸åŒçš„æ£€ç´¢æ–¹æ³•åœ¨è½»é‡æ¨¡å‹å’ŒLLMsä¸­çš„åº”ç”¨ï¼Œå¹¶å±•ç¤ºäº†æ–°æ–¹æ³•åœ¨æé«˜ä¸Šä¸‹æ–‡ç†è§£ä¸Šçš„æœ‰æ•ˆæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05708v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.05708.md)  |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **NLLG Quarterly arXiv Report 09/23: What are the most influential current AI Papers?**<br><sub>æœºæ„: University of Mannheim, University of Bielefeld<br>è¯¥è®ºæ–‡é€šè¿‡åˆ†æåœ¨ç‰¹å®šæ—¶é—´å†…arXivä¸Šå¼•ç”¨æœ€å¤šçš„è®ºæ–‡ï¼Œæä¾›äº†AIç ”ç©¶é¢†åŸŸçš„æœ€æ–°è¶‹åŠ¿å’Œå½±å“åŠ›åˆ†æï¼Œç‰¹åˆ«å¼ºè°ƒäº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å…¶ä¸­çš„é‡è¦æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05688v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.05688.md)  |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge**<br><sub>æœºæ„: Northeastern University, Oracle<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºAgile-Quantçš„æ¿€æ´»å¼•å¯¼é‡åŒ–æ¡†æ¶ï¼Œä»¥åŠ é€Ÿå¤§å‹è¯­è¨€æ¨¡å‹çš„è¾¹ç¼˜è®¾å¤‡æ¨ç†ã€‚Agile-Quantå…‹æœäº†æ¿€æ´»å€¼å¼‚å¸¸çš„æŒ‘æˆ˜å’Œè¾¹ç¼˜è®¾å¤‡ä¸Šçš„ç¡¬ä»¶å®æ–½é—®é¢˜ï¼Œå¹¶å®ç°äº†ä¸ä»…æƒé‡é‡åŒ–æ–¹æ³•ç›¸å½“çš„ä»»åŠ¡æ€§èƒ½ï¼ŒåŒæ—¶åœ¨å®é™…è®¾å¤‡ä¸Šè·å¾—äº†æ˜¾è‘—çš„æ¨ç†é€Ÿåº¦æå‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05693v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.05693.md)  |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis**<br><sub>æœºæ„: Shanghai Jiao Tong University<br>æœ¬ç ”ç©¶ç³»ç»Ÿåœ°æ¢ç´¢äº†LLMsåœ¨æ¸¸æˆç†è®ºèƒŒæ™¯ä¸‹çš„èƒ½åŠ›è¾¹ç•Œï¼Œå¹¶ä»ä¸‰ä¸ªè§’åº¦å‡ºå‘ï¼Œæä¾›äº†å°†LLMsåœ¨ç¤¾ä¼šç§‘å­¦ç ”ç©¶ä¸­ä½¿ç”¨çš„è¿›ä¸€æ­¥æŒ‡å¯¼ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05488v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.05488.md)  |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Sim-GPT: Text Similarity via GPT Annotated Data**<br><sub>æœºæ„: Shannon.AI, Zhejiang University, Bytedance<br>Sim-GPTæ˜¯ä¸€ä¸ªåˆ©ç”¨GPT-4ç”Ÿæˆæ•°æ®æ ‡ç­¾æ¥è®­ç»ƒSTSæ¨¡å‹çš„æ¡†æ¶ã€‚å®ƒåœ¨ç”Ÿæˆæ•°æ®æ—¶ä»…äº§ç”Ÿä¸€æ¬¡æ€§æˆæœ¬ï¼Œé€Ÿåº¦è¾ƒå¿«ï¼Œæ¨¡å‹åœ¨å¤šä¸ªSTSåŸºå‡†ä¸Šæ€§èƒ½ä¼˜è¶Šã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05603v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.05603.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/ShuheWang1998/Sim-GPT)</div> |
| <span style='display: inline-block; width: 42px;'>12-08</span> | **Using Program Knowledge Graph to Uncover Software Vulnerabilities**<br><sub>è®ºæ–‡é€šè¿‡ç»“åˆç¨‹åºå›¾å’Œå®‰å…¨æ•°æ®ï¼Œæå‡ºäº†ç¨‹åºçŸ¥è¯†å›¾è°±ï¼Œå¹¶åˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹çš„æç¤ºè°ƒæ•´æ¥è‡ªåŠ¨ç”Ÿæˆæ£€æµ‹è½¯ä»¶ä»£ç ä¸­æ¼æ´çš„æŸ¥è¯¢ã€‚è¯¥æ–¹æ³•æ—¨åœ¨å…‹æœä¼ ç»Ÿæ¼æ´æ£€æµ‹æ–¹æ³•çš„å±€é™æ€§ï¼Œæé«˜æ¼æ´æ£€æµ‹çš„è‡ªåŠ¨åŒ–ç¨‹åº¦å’Œæœ‰æ•ˆæ€§ï¼Œå°¤å…¶æ˜¯åœ¨é™æ€åˆ†æä¸­çš„åº”ç”¨ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04818v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.04818.md)  |
| <span style='display: inline-block; width: 42px;'>12-08</span> | **PaperQA: Retrieval-Augmented Generative Agent for Scientific Research**<br><sub>æœºæ„: RAND Corporation, Carnegie Mellon University, LangChain<br>è¯¥è®ºæ–‡æå‡ºäº†PaperQAï¼Œä¸€ä¸ªåŸºäºæ£€ç´¢çš„ç”Ÿæˆå‹ä»£ç†ï¼Œç”¨äºç§‘å­¦ç ”ç©¶ã€‚PaperQAå¯ä»¥å‡†ç¡®å›ç­”åŸºäºæœ€æ–°ç§‘å­¦æ–‡çŒ®çš„é—®é¢˜ï¼Œå¹¶ä¸”ä¸äººç±»ä¸“å®¶çš„å›ç­”ç›¸å½“ï¼Œç”šè‡³åœ¨æŸäº›æ–¹é¢è¡¨ç°æ›´å¥½ã€‚è®ºæ–‡å±•ç¤ºäº†PaperQAçš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡ä¸äººç±»ä¸“å®¶å’Œå…¶ä»–å•†ä¸šå·¥å…·çš„å¯¹æ¯”ï¼Œè¯æ˜äº†å…¶ä¼˜è¶Šæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07559v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.07559.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Fortify the Shortest Stave in Attention: Enhancing Context Awareness of Large Language Models for Effective Tool Use**<br><sub>æœºæ„: Gaoling School of Artificial Intelligence, Renmin University of China, Alibaba Group<br>è¯¥è®ºæ–‡é’ˆå¯¹LLMsåœ¨å·¥å…·ä½¿ç”¨æ—¶å¯¹ä¸Šä¸‹æ–‡è®¤çŸ¥çš„ä¸è¶³æå‡ºäº†Attention Bucketsæ–¹æ³•ï¼Œé€šè¿‡å¤„ç†ä¸åŒçš„RoPEè§’åº¦åŸºç¡€æ¥å¼ºåŒ–å¯¹ä¸Šä¸‹æ–‡çš„å…³æ³¨ï¼Œæ˜¾è‘—æå‡äº†LLMsåœ¨å·¥å…·ä½¿ç”¨ä»»åŠ¡çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04455v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.04455.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Chain of Code: Reasoning with a Language Model-Augmented Code Emulator**<br><sub>æœºæ„: Google DeepMind, Stanford University, University of California Berkeley  <br>Chain of Code (CoC)ä¸ºè¯­è¨€æ¨¡å‹å¢åŠ äº†é€šè¿‡ç¼–å†™ä»£ç å’Œæ¨¡æ‹Ÿä»£ç æ‰§è¡Œæ¥æ”¹å–„æ¨ç†èƒ½åŠ›çš„æ–°ç»´åº¦ã€‚å®ƒåœ¨æ•°å­—å’Œè¯­ä¹‰æ¨ç†ä»»åŠ¡ä¸­å‡å®ç°äº†çªç ´æ€§çš„æ€§èƒ½ï¼Œå¯¹LLMsçš„åº”ç”¨èŒƒå›´è¿›è¡Œäº†æ‰©å±•ï¼Œå¹¶æœ‰æ½œåŠ›åº”ç”¨äºæ›´å¹¿æ³›çš„é—®é¢˜ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04474v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.04474.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **A Study on the Calibration of In-context Learning**<br><sub>æœºæ„: Harvard University<br>è¯¥è®ºæ–‡æ·±å…¥ç ”ç©¶äº†ä¸Šä¸‹æ–‡å†…å­¦ä¹ (ICL)åœ¨è¯­è¨€æ¨¡å‹(LMs)ä¸­çš„æ ¡å‡†å‡†ç¡®æ€§é—®é¢˜ï¼Œå¹¶æå‡ºäº†è¯„ä¼°å’Œåˆ†ææ–¹æ³•ã€‚å®ƒæ­ç¤ºäº†æ ¡å‡†è¯¯å·®ä¸æ¨¡å‹å¤§å°å’Œå¾®è°ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–å…³ç³»ï¼Œä»¥åŠæ ¡å‡†åœ¨æ¨ç†ä»»åŠ¡ç”Ÿæˆä¸­çš„é™ä½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04021v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.04021.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models**<br><sub>æœºæ„: MPI for Intelligent Systems, University of Washington<br>æ­¤ç ”ç©¶ä¸ºæµ‹è¯•å’Œåˆ†æå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨æ­£è§„å› æœæ¨ç†ä¸Šçš„èƒ½åŠ›æå‡ºäº†CLADDERæ•°æ®é›†å’ŒCAUSALCOTæ€ç»´è·¯å¾„æç¤ºç­–ç•¥ï¼Œé€šè¿‡å®éªŒçªæ˜¾äº†LLMsçš„å±€é™å¹¶ä¸ºæœªæ¥ç ”ç©¶æå‡ºäº†æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04350v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.0435.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/causalNLP/cladder)</div> |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Generating Illustrated Instructions**<br><sub>æœºæ„: GenAI Meta, Columbia University<br>æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§åä¸ºStackedDiffusionçš„æ–°æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆæ’å›¾è¯´æ˜ï¼Œè¿™æ˜¯ä¸€ç§å°†æ–‡æœ¬å’Œå›¾åƒç»“åˆèµ·æ¥æè¿°å¦‚ä½•å®ç°æŸä¸€ç›®æ ‡çš„ä»»åŠ¡ã€‚è¯¥æ–¹æ³•é€šè¿‡ç»“åˆå¤§å‹è¯­è¨€æ¨¡å‹å’Œæ–‡æœ¬åˆ°å›¾åƒæ‰©æ•£æ¨¡å‹ï¼Œå¹¶å¼•å…¥ä¸€äº›æ–°é¢–çš„å»ºæ¨¡æŠ€å·§ï¼Œè§£å†³äº†ç°æœ‰T2Iæ¨¡å‹æ— æ³•ç›´æ¥ä»ç”¨æˆ·æŸ¥è¯¢ä¸­ç”Ÿæˆè§†è§‰æ•ˆæœçš„é—®é¢˜ï¼Œå¹¶åœ¨äººç±»è¯„ä¼°ä¸­è¶…è¶Šäº†ç°æœ‰æŠ€æœ¯æ°´å¹³ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04552v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.04552.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration**<br><sub>æœºæ„: Renmin University of China, Beijing Institute of Technology, HKUST (GZ)<br>è¿™ç¯‡è®ºæ–‡æä¾›äº†ä¸€ä¸ªå…¨é¢çš„ç ”ç©¶ï¼Œæ—¨åœ¨æ¢ç´¢å¦‚ä½•å¼€å‘ä¸€ç§æˆæœ¬æ•ˆç›Šçš„æ‰¹é‡æç¤ºæ–¹æ³•æ¥è¿›è¡Œå®ä½“è§£æã€‚ä¸»è¦è´¡çŒ®æ˜¯ä»‹ç» BATCHER æ¡†æ¶å¹¶æå‡ºåŸºäºè¦†ç›–çš„æ¼”ç¤ºé€‰æ‹©ç­–ç•¥ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03987v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.03987.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Beyond Surface: Probing LLaMA Across Scales and Layers**<br><sub>æœºæ„: Hong Kong University of Science and Technology<br>æœ¬ç ”ç©¶çš„æ ¸å¿ƒè´¡çŒ®åœ¨äºæå‡ºäº†ä¸€ç³»åˆ—è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹é«˜é˜¶èƒ½åŠ›çš„æ¢é’ˆä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡å›´ç»•ç€è®¡ç®—èƒ½åŠ›ã€æ•°å­¦æ¨ç†ã€é€»è¾‘æ¨ç†å’ŒçœŸå®æ€§æ£€æµ‹ã€‚ç ”ç©¶æ­ç¤ºäº†LLMçš„è¡¨ç°å¦‚ä½•éšç€æ¨¡å‹è§„æ¨¡å’Œå±‚æ¬¡ç»“æ„çš„å˜åŒ–ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04333v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.04333.md)  |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **An LLM Compiler for Parallel Function Calling**<br><sub>æœºæ„: UC Berkeley, ICSI, LBNL<br>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºLLMCompilerçš„ç³»ç»Ÿï¼Œè§£å†³äº†å¤§å‹è¯­è¨€æ¨¡å‹åœ¨æ‰§è¡Œå¤šåŠŸèƒ½è°ƒç”¨æ—¶çš„é«˜å»¶è¿Ÿæˆæœ¬å’Œæ•ˆç‡ä½ä¸‹çš„é—®é¢˜ï¼Œé€šè¿‡å¹¶è¡ŒåŒ–å‡½æ•°è°ƒç”¨å’Œä¼˜åŒ–åè°ƒæ¥æé«˜é€Ÿåº¦ï¼ŒèŠ‚çœæˆæœ¬å¹¶æå‡å‡†ç¡®ç‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04511v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.04511.md)  |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment**<br><sub>æœºæ„: Zhejiang Lab<br>æ–‡ç« æˆåŠŸä»‹ç»äº†ä¸€ä¸ªèƒ½åœ¨ç½‘ç»œæ¥å£å¡å¼‚æ„ç¯å¢ƒä¸­è¿›è¡Œå¤§å‹è¯­è¨€æ¨¡å‹è®­ç»ƒçš„æ¡†æ¶â€”â€”Holmesã€‚é€šè¿‡å®è¯ç ”ç©¶å…¶æ€§èƒ½ï¼ŒHolmesè¢«è¯æ˜å¯åœ¨å¼‚æ„ç¯å¢ƒä¸­å®ç°ä¸åŒæ„RDMA NICsç›¸å½“çš„æ€§èƒ½æ°´å¹³ï¼Œä»è€Œä½¿LLMè®­ç»ƒæ›´åŠ æ™®åŠå¹¶æ‰©å¤§äº†æœ‰æ•ˆæ‰©å±•çš„å¯èƒ½æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03549v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.03549.md)  |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia**<br><sub>æœºæ„: Google DeepMind, Google Research<br>æœ¬è®ºæ–‡æå‡ºäº†åˆ©ç”¨ç”Ÿæˆå¼å¤§å‹è¯­è¨€æ¨¡å‹å¢å¼ºåŸºäºä»£ç†çš„æ¨¡å‹çš„æ–¹æ³•ï¼Œé€šè¿‡Concordiaåº“å®ç°äº†åœ¨ç¤¾ä¼šã€ç‰©ç†å’Œæ•°å­—ç©ºé—´ä¸­æ¨¡æ‹Ÿä»£ç†çš„äº¤äº’ã€‚è¯¥æ¨¡å‹æ—¨åœ¨æä¾›é€¼çœŸçš„ç¤¾ä¼šæ¨¡æ‹Ÿï¼Œå¹¶æ¢ç´¢æ¨¡å‹çš„æœ‰æ•ˆæ€§éªŒè¯ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03664v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.03664.md)  |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Efficient Large Language Models: A Survey**<br><sub>æœºæ„: The Ohio State University, Google Research, Amazon AWS AI<br>è®ºæ–‡ç»¼è¿°äº†å¤§å‹è¯­è¨€æ¨¡å‹ä¸­å¯¹äºç¨€ç–æ¿€æ´»æ–¹æ³•çš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯æ··åˆä¸“å®¶ç³»ç»Ÿï¼ˆMoEï¼‰åŠå…¶åœ¨é•¿æ–‡æœ¬å¤„ç†æ–¹é¢çš„åº”ç”¨ã€‚å®ƒæ€»ç»“äº†MoEæ¨¡å‹ä¼˜åŒ–çš„å„ç§æ–¹æ³•ï¼ŒåŒ…æ‹¬ç®—æ³•çº§åˆ«çš„æ”¹è¿›å’Œç³»ç»Ÿçº§åˆ«çš„åŠ é€Ÿæ¡†æ¶ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03863v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.03863.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/AIoT-MLSys-Lab/EfficientLLMs)</div> |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **OneLLM: One Framework to Align All Modalities with Language**<br><sub>æœºæ„: MMLab The Chinese University of Hong Kong, Shanghai Artificial Intelligence Laboratory<br>OneLLMé€šè¿‡å…¶ç»Ÿä¸€çš„å¤šæ¨¡æ€ç¼–ç æ¡†æ¶å’Œæ¸è¿›å¼å¯¹é½ç®¡é“ï¼Œåœ¨æ¨ç†å’Œåˆ©ç”¨æ–¹é¢å±•ç¤ºäº†å¼ºå¤§çš„å¤šæ¨¡æ€ç†è§£å’Œå¤„ç†èƒ½åŠ›ï¼Œå¹¶æˆåŠŸåœ°å¤„ç†äº†æ‰©å±•å¤šæ¨¡æ€LLMsçš„æŒ‘æˆ˜ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03700v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.037.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/csuhan/OneLLM)</div> |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Controllable Human-Object Interaction Synthesis**<br><sub>æœºæ„: Stanford University, FAIR Meta<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„äº¤äº’åˆæˆæ–¹æ³•CHOISï¼Œå®ƒèƒ½åœ¨å—è¯­è¨€æè¿°æŒ‡å¯¼çš„æ¡ä»¶ä¸‹ï¼Œç”Ÿæˆç¬¦åˆä¸‰ç»´åœºæ™¯å‡ ä½•çº¦æŸçš„äººä¸ç‰©ä½“çš„åŒæ­¥è¿åŠ¨ã€‚è¯¥æ–¹æ³•é€šè¿‡é›†æˆåˆ°ä¸€ä¸ªç³»ç»Ÿä¸­ï¼Œå±•ç¤ºäº†å…¶åœ¨åˆæˆè¿ç»­ã€é€¼çœŸå’Œç¯å¢ƒæ„ŸçŸ¥çš„äººç‰©äº’åŠ¨æ–¹é¢çš„æœ‰æ•ˆæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03913v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.03913.md)  |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **AnimateZero: Video Diffusion Models are Zero-Shot Image Animators**<br><sub>æœºæ„: Peking University, Tencent AI Lab, HKUST<br>AnimateZeroä¸ºT2Vç”Ÿæˆæä¾›è§£è€¦å’Œç²¾ç¡®çš„å¤–è§‚å’ŒåŠ¨ä½œæ§åˆ¶ï¼Œé€šè¿‡ç©ºé—´å¤–è§‚æ§åˆ¶å’Œæ—¶é—´ä¸€è‡´æ€§æ§åˆ¶ï¼Œå®ç°äº†ä»T2Iåˆ°I2Vçš„æ­¥éª¤å¼è§†é¢‘ç”Ÿæˆï¼ŒåŒæ—¶ç»´æŠ¤è‰¯å¥½çš„åŸŸä¸€è‡´æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03793v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.03793.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Rank-without-GPT: Building GPT-Independent Listwise Rerankers on Open-Source Large Language Models**<br><sub>æœºæ„: University of Waterloo, 2Cohere, Comcast Applied AI<br>æœ¬æ–‡çš„æ ¸å¿ƒæˆæœæ˜¯æ¼”ç¤ºäº†å¦‚ä½•æ„å»ºä¸€ç§ä¸ä¾èµ–GPTæ¨¡å‹çš„æœ‰æ•ˆåˆ—è¡¨é‡æ’åºå™¨ï¼Œèƒ½æ˜¾è‘—è¶…è¶Šç°æœ‰åŸºäºGPTçš„é‡æ’åºå™¨ï¼Œå¹¶å‘¼åç ”ç©¶ç¤¾åŒºå¼€å‘æ›´é«˜è´¨é‡çš„åˆ—è¡¨æ’åºè®­ç»ƒæ•°æ®ï¼Œä»¥æå‡æ¨¡å‹çš„è¡¨ç°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02969v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.02969.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **How should the advent of large language models affect the practice of science?**<br><sub>æœºæ„: Max Planck Institute for Biological Cybernetics, University of TÃ¼bingen, University of Washington  <br>æœ¬æ–‡è®¨è®ºäº†LLMså¯¹ç§‘å­¦å®è·µçš„å½±å“ï¼Œå¹¶å»ºè®®å¯¹å…¶ä½¿ç”¨æŒå®¡æ…æ€åº¦ï¼ŒåŒæ—¶å¼ºè°ƒäº†ä¿æŠ¤ç§‘å­¦çš„è§„èŒƒå’Œè®¤è¯†è®ºæ–¹é¢çš„é‡è¦æ€§ã€‚è™½ç„¶LLMså¯èƒ½æå‡æŸäº›ç§‘ç ”ä»»åŠ¡çš„æ•ˆç‡ï¼Œä½†ä½œä¸ºå·¥å…·ï¼Œå…¶ä½¿ç”¨åº”è¯¥è°¨æ…å¹¶ç¡®ä¿ç¬¦åˆç§‘å­¦è§„èŒƒã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03759v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.03759.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation**<br><sub>æœºæ„: Sea AI Lab, Sun Yat-sen University, Harvard University  <br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ—¨åœ¨æå‡å¤§å‹è¯­è¨€æ¨¡å‹åˆ›é€ æ€§æ€ç»´èƒ½åŠ›çš„Creative Leap-of-Thought (CLoT)èŒƒå¼ï¼Œå¹¶éªŒè¯äº†å…¶åœ¨å¤šç§ä»»åŠ¡ä¸­çš„æœ‰æ•ˆæ€§å’Œæ¦‚æ‹¬èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02439v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.02439.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/sail-sg/CLoT)</div> |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **A Hardware Evaluation Framework for Large Language Model Inference**<br><sub>æœºæ„: Princeton University<br>LLMCompass ä½œä¸ºä¸€ç§ç¡¬ä»¶è¯„ä¼°æ¡†æ¶ï¼ŒæˆåŠŸåœ°åº”å¯¹äº†è®¾è®¡LLMæ¨ç†ç¡¬ä»¶æ—¶é¢ä¸´çš„æŒ‘æˆ˜ã€‚å®ƒä¸ä»…å¿«é€Ÿç²¾å‡†ï¼Œè€Œä¸”å…·æœ‰æ¶æ„æè¿°æ€§å’Œæˆæœ¬æ„è¯†ï¼Œå·²ç»åœ¨å•†ä¸šç¡¬ä»¶ä¸Šè¿›è¡Œäº†éªŒè¯ä¸”æ˜¾ç¤ºå‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03134v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.03134.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction**<br><sub>æœºæ„: Zhejiang Lab, Ant Group<br>é€šè¿‡åœ¨KGCä¸­å¼•å…¥å¤šæ™ºèƒ½ä½“åˆä½œçš„æ–¹æ³•ï¼ŒcooperKGCæ¡†æ¶æå‡äº†æ™ºèƒ½ä½“è§£å†³å®ä½“ã€å…³ç³»å’Œäº‹ä»¶æå–ä»»åŠ¡ä¸­çš„ç²¾ç¡®åº¦ï¼Œå¹¶æœ‰æœ›ä¸ºAIçš„åä½œæ„è¯†åŒ–æœªæ¥å¥ å®šäº†åŸºç¡€ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03022v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.03022.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Inherent limitations of LLMs regarding spatial information**<br><sub>æœºæ„: ProtagoLabs, International Monetary Fund, NetMind.ai  <br>è®ºæ–‡ä¸ºGPT-4ç­‰å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç©ºé—´ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›æä¾›äº†æ–°çš„è¯„ä¼°æ¡†æ¶å’Œä¸“é—¨è®¾è®¡çš„æ•°æ®é›†ï¼Œå¹¶åˆ†æäº†GPT-4åœ¨å¤„ç†ç©ºé—´ä¿¡æ¯æ–¹é¢çš„èƒ½åŠ›å’Œå±€é™æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03042v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.03042.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education**<br><sub>æœºæ„: Carnegie Mellon University  <br>æœ¬æ–‡çš„ä¸»è¦è´¡çŒ®æ˜¯å¼€å‘äº†ä¸€ä¸ªåŸºäºGPT-4çš„è‡ªåŠ¨åŒ–MCQç”Ÿæˆç³»ç»Ÿï¼Œé€šè¿‡ä¸“é—¨çš„å¼¹æ€§æ„æ¶å’Œç²¾ç¡®çš„LOå¯¹é½æœºåˆ¶ï¼ŒæˆåŠŸç”Ÿæˆä¸é«˜ç­‰æ•™è‚²Pythonè¯¾ç¨‹LOä¸€è‡´çš„MCQsã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œè‡ªåŠ¨ç”Ÿæˆçš„MCQåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ä¸LOä¿æŒè‰¯å¥½çš„ä¸€è‡´æ€§ï¼Œè´¨é‡æ¥è¿‘äººå·¥è®¾è®¡çš„MCQï¼Œä½†åœ¨æ‹¥æœ‰å•ä¸€æ­£ç¡®ç­”æ¡ˆå’Œé«˜è´¨é‡å¹²æ‰°é¡¹æ–¹é¢ç•¥æ˜¾æ¬ ç¼ºï¼Œæœªæ¥å·¥ä½œåº”è¯¥é›†ä¸­åœ¨å‡è½»è¿™äº›é—®é¢˜ä¸Šã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03173v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.03173.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Prompt Optimization via Adversarial In-Context Learning**<br><sub>æœºæ„: National University of Singapore, Hong Kong University of Science and Technology, Institute for Infocomm Research (I2R) A*STAR<br>è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªæ–°é¢–çš„Adversarial In-Context Learningï¼ˆadv-ICLï¼‰æ–¹æ³•ï¼Œç”¨äºä¼˜åŒ–å¤§å‹æ¨¡å‹ä¸­promptçš„é€‰æ‹©ï¼Œä»¥æ­¤æé«˜æ¨¡å‹æ€§èƒ½ã€‚å®ƒå¯ä»¥å®ç°å¯¹æŠ—è®­ç»ƒç›®æ ‡ï¼Œå…‹æœæ•°æ®å’Œè®¡ç®—èµ„æºé™åˆ¶ï¼Œé€šè¿‡ä¼˜åŒ–promptè€Œä¸æ˜¯æ¨¡å‹å‚æ•°æ¥æå‡æ€§èƒ½ï¼Œä¸”å®éªŒç»“æœåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02614v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.02614.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Large Knowledge Model: Perspectives and Challenges**<br><sub>æœºæ„: Zhejiang University<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§å¤§å‹çŸ¥è¯†æ¨¡å‹ï¼ˆLKMï¼‰çš„æ¦‚å¿µï¼Œæ—¨åœ¨æ›´æœ‰æ•ˆåœ°ç®¡ç†å’Œè§£è¯»çŸ¥è¯†è¡¨ç¤ºçš„å¤šæ ·æ€§ã€‚ç ”ç©¶æŒ‡å‡ºäº†ä»ç°æœ‰çš„å¤§å‹è¯­è¨€æ¨¡å‹åˆ°LKMè½¬å˜çš„æŒ‘æˆ˜ï¼Œå¼ºè°ƒäº†ç»“æ„åŒ–çŸ¥è¯†åœ¨é¢„è®­ç»ƒä¸­çš„é‡è¦æ€§ï¼Œå¹¶æå‡ºäº†ä¸€å¥—LKMçš„è®¾è®¡åŸåˆ™ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02706v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.02706.md)  |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!**<br><sub>æœºæ„: University of Waterloo<br>RankZephyræ˜¯ä¸€æ¬¾æ–°å‹å¼€æºLLMï¼Œç‰¹åˆ«ä¼˜åŒ–äº†é›¶æ ·æœ¬åˆ—è¡¨é‡æ–°æ’åºä»»åŠ¡ã€‚å®ƒæä¾›äº†ä¸å¤§å‹ä¸“æœ‰æ¨¡å‹ç›¸å½“æˆ–æ›´ä¼˜çš„é‡æ–°æ’åºæ•ˆæœï¼ŒåŒæ—¶å¼ºè°ƒäº†æ•°æ®å¢å¼ºå¯¹äºæå‡æ¨¡å‹é²æ£’æ€§çš„é‡è¦æ€§ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†å…¶æœ‰æ•ˆæ€§å’Œåœ¨ç°å®åœºæ™¯ä¸­çš„åº”ç”¨æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02724v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.02724.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/castorini/rank_llm)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models**<br><sub>æœºæ„: Xiamen University, MBZUAI, Tencent AI Lab<br>æ–‡ç« é€šè¿‡å¼•å…¥åŠ¨æ€è‡ªåŠ¨æ£€ç´¢æœºåˆ¶å’Œåˆ†å±‚æŠ½æ ·æ–¹æ³•ï¼ŒæˆåŠŸæå‡äº†å¤šæ¨¡æ€ä»»åŠ¡ä¸­LLMsçš„CoTæ¨ç†èƒ½åŠ›ã€‚æå‡ºçš„æ–¹æ³•ä¸ä»…æé«˜äº†æ¨¡å‹æ€§èƒ½ï¼Œè€Œä¸”é€šè¿‡å¤šæ ·åŒ–ç¤ºä¾‹é€‰æ‹©è¿›ä¸€æ­¥ç»†åŒ–äº†æ¨ç†è¿‡ç¨‹ï¼Œä¸ºå¤šæ¨¡æ€æ¨ç†é¢†åŸŸæ ‘ç«‹äº†æ–°çš„æ€§èƒ½æ ‡æ†ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01714v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.01714.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **LLMs Accelerate Annotation for Medical Information Extraction**<br><sub>æœºæ„: Google Research<br>æœ¬è®ºæ–‡å±•ç¤ºäº†ä¸€ä¸ªåˆ©ç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯Googleçš„PaLM 2ï¼Œæ¥æå‡åŒ»å­¦ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸­æ³¨é‡Šé€Ÿåº¦çš„æ–¹æ³•ã€‚è¿™ä¸ªåŸºäºLLMçš„æ³¨é‡Šæµç¨‹æé«˜äº†æ•ˆç‡ä¸”ä¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œå¤æ‚çš„è°ƒå‚ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæœ‰æ½œåŠ›çš„å·¥å…·æ¥åŠ é€ŸåŒ»ç–—é¢†åŸŸçš„æ•°æ®æ³¨é‡Šå·¥ä½œã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02296v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.02296.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication**<br><sub>æœºæ„: Fudan University, National University of Singapore, Shanghai AI Laboratory  <br>æœ¬æ–‡æå‡ºçš„Exchange-of-Thoughtï¼ˆEoTï¼‰æ¡†æ¶é€šè¿‡æ¨¡å‹é—´äº¤æµæå‡LLMsçš„æ¨ç†èƒ½åŠ›ï¼Œå‡­å€Ÿå››ç§é€šä¿¡èŒƒä¾‹å’Œä¿¡å¿ƒè¯„ä¼°æœºåˆ¶ï¼Œåœ¨å¤šä¸ªæ¨ç†ä»»åŠ¡ä¸Šå–å¾—äº†æ˜¾è‘—æˆæ•ˆï¼Œå¹¶è¯æ˜äº†å¤–éƒ¨æ€ç»´åœ¨å¢å¼ºæ¨¡å‹æ€§èƒ½ä¸­çš„ä½œç”¨ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01823v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.01823.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Competition-Level Problems are Effective LLM Evaluators**<br><sub>æœºæ„: Microsoft Research Asia, Xiamen University, Microsoft Azure AI<br>æœ¬ç ”ç©¶é€šè¿‡è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤„ç†ç«èµ›çº§ç¼–ç¨‹é—®é¢˜ä¸Šçš„è¡¨ç°ï¼Œæ­ç¤ºäº†GPT-4ç­‰æ¨¡å‹åœ¨çœŸå®æ¨ç†èƒ½åŠ›ä¸Šçš„ä¸è¶³ï¼Œå¹¶æå‡ºäº†ä¸€äº›æå‡è¡¨ç°çš„æ–¹æ³•ã€‚è¿™äº›å‘ç°çªæ˜¾äº†è¿™ç±»é—®é¢˜ä½œä¸ºè¯„ä¼°LLMsçš„æœ‰æ•ˆå·¥å…·çš„é‡è¦æ€§ï¼Œå¹¶ä¿ƒè¿›äº†å¯¹äºæé«˜LLMså¤æ‚æ¨ç†èƒ½åŠ›çš„è¿›ä¸€æ­¥ç ”ç©¶ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02143v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.02143.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Data Management For Large Language Models: A Survey**<br><sub>æœºæ„: Peking University, Huawei Noahâ€™s Ark Lab<br>è¿™ç¯‡ç»¼è¿°ç ”ç©¶äº†åœ¨LLMsçš„é¢„è®­ç»ƒå’Œç›‘ç£å¼å¾®è°ƒé˜¶æ®µï¼Œæ•°æ®ç®¡ç†çš„ç ”ç©¶ç°çŠ¶ä»¥åŠæ•°æ®ç®¡ç†ç­–ç•¥çš„è®¾è®¡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01700v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.017.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/ZigeW/data_management_LLM)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning**<br><sub>æœºæ„: Allen Institute for Artificial Intelligence, University of Washington<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé€šè¿‡ä¸Šä¸‹æ–‡å­¦ä¹ å®ç°LLMså¯¹é½çš„ç®€å•æ— é¡»è°ƒæ•´æ–¹æ³•ï¼ˆURIALï¼‰ï¼Œè¡¨ç°å‡ºä¸ä¼ ç»Ÿè°ƒæ•´å¯¹é½æ–¹æ³•ç›¸åŒ¹é…ç”šè‡³æ›´å¥½çš„æ•ˆæœã€‚è¿™ä¸€å‘ç°å¯¹æœªæ¥LLMsç ”ç©¶å…·æœ‰é‡è¦çš„å¯ç¤ºï¼Œè¯´æ˜äº†åœ¨LLMså¯¹é½ä¸Šæ›´æ·±å…¥çš„åˆ†æå’Œç†è®ºç†è§£çš„é‡è¦æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01552v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.01552.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **On the Effectiveness of Large Language Models in Domain-Specific Code Generation**<br><sub>æœºæ„: Shanghai Jiao Tong University, Chongqing University, East China Normal University<br>è¿™é¡¹ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æœ‰æ•ˆåœ°æ•´åˆé¢†åŸŸçŸ¥è¯†åˆ°ä»£ç ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥å¢å¼ºLLMsåœ¨ç‰¹å®šé¢†åŸŸå†…çš„ä»£ç ç”Ÿæˆèƒ½åŠ›ã€‚DomCoderä½œä¸ºä¸€ä¸ªæ–°çš„ä»£ç ç”Ÿæˆæ–¹æ³•ï¼Œåˆ©ç”¨äº†ä¸åŒç­–ç•¥ä»¥æ•´åˆé¢†åŸŸçŸ¥è¯†ï¼Œå¹¶åœ¨ç‰¹å®šè®¾ç½®ä¸‹æå‡äº†ä»£ç ç”Ÿæˆçš„å®é™…æ•ˆæœã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01639v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.01639.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions**<br><sub>æœºæ„: Nanyang Technological University, National University of Singapore<br>è¯¥ç ”ç©¶æå‡ºäº†ç¬¬ä¸€ä¸ªç³»ç»Ÿçš„è¯„ä¼°ChatGPTåœ¨ç”Ÿæˆå‰å¤§å­¦æ•°å­¦é—®é¢˜æ½œåŠ›çš„ç ”ç©¶ã€‚é€šè¿‡ä¸¤ç§ä¸»è¦åœºæ™¯ï¼šç»™å®šä¸Šä¸‹æ–‡å’Œæœªç»™å®šä¸Šä¸‹æ–‡çš„ç”Ÿæˆé—®é¢˜ï¼Œå¹¶ä¸ºæ•™è‚²å·¥ä½œè€…æä¾›å®ç”¨çš„æ´å¯Ÿã€‚ç ”ç©¶çš„ç»“æœæœ‰å¯èƒ½ä¿ƒè¿›ç°ä»£AIæŠ€æœ¯åœ¨æ•™è‚²é¢†åŸŸçš„åº”ç”¨ï¼Œå¹¶æé«˜è‡ªåŠ¨åŒ–æ•°å­¦é—®é¢˜ç”Ÿæˆçš„å®ç”¨æ€§å’Œæ•ˆç‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01661v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.01661.md)  |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly**<br><sub>æœºæ„: Elsevier<br>è¿™ç¯‡è®ºæ–‡æ€»ç»“äº†å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰åœ¨å®‰å…¨æ€§å’Œéšç§ä¿æŠ¤ä¸­çš„åº”ç”¨åŠç›¸å…³æŒ‘æˆ˜ï¼ŒæŒ‡å‡ºLLMsåœ¨è¿™äº›é¢†åŸŸçš„å¥½å¤„ã€åå¤„å’Œä¸‘é™‹ä¹‹å¤„ï¼ŒåŒæ—¶å¼ºè°ƒäº†å…¶åœ¨æ•°æ®ä¿æŠ¤æ–¹é¢çš„æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02003v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.02003.md)  |
| <span style='display: inline-block; width: 42px;'>12-03</span> | **TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents**<br><sub>æœºæ„: University of Southern California, Google Cloud AI<br>TextGenSHAPæ˜¯ä¸€ä¸ªä¸ºå¤§å‹è¯­è¨€æ¨¡å‹è®¾è®¡çš„é«˜æ•ˆåéªŒè§£é‡Šæ€§æ–¹æ³•ï¼Œé€šè¿‡æ”¹è¿›è§£é‡Šç”Ÿæˆçš„é€Ÿåº¦ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨è¿™äº›è§£é‡Šæ”¹è¿›é•¿æ–‡æ¡£é—®ç­”å’Œæ–‡æ¡£æ£€ç´¢ç³»ç»Ÿã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01279v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.01279.md)  |
| <span style='display: inline-block; width: 42px;'>12-03</span> | **D-Bot: Database Diagnosis System using Large Language Models**<br><sub>æœºæ„: Tsinghua University, Pigsty, ModelBest<br>D-Botæ˜¯ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹çš„æ•°æ®åº“è¯Šæ–­ç³»ç»Ÿï¼Œå®ƒé€šè¿‡æ–‡æ¡£ä¸­çš„çŸ¥è¯†æå–å’Œç”Ÿæˆæœ‰æ•ˆçš„è¯Šæ–­æŠ¥å‘Šæ¥æé«˜æ•°æ®åº“è¯Šæ–­çš„æ•ˆç‡å’Œå‡†ç¡®æ€§ï¼Œè§£å†³äº†åŒºåŸŸä¸“å®¶åœ¨æ•°æ®åº“è¯Šæ–­ä¸­é‡åˆ°çš„æŒ‘æˆ˜ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01454v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.01454.md)  |
| <span style='display: inline-block; width: 42px;'>12-03</span> | **Running cognitive evaluations on large language models: The do's and the don'ts**<br><sub>æœºæ„: Massachusetts Institute of Technology<br>è¿™ç¯‡è®ºæ–‡ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„è®¤çŸ¥è¯„ä¼°ç ”ç©¶æ–¹æ³•æä¾›äº†æŒ‡å¯¼æ€§çš„å»ºè®®ï¼Œæ¢è®¨äº†åœ¨æ–¹æ³•è®ºä¸Šå¦‚ä½•é¿å…åœ¨è¿è¡Œè®¤çŸ¥è¯„ä¼°æ—¶å¯èƒ½å‡ºç°çš„é—®é¢˜ã€‚è®ºæ–‡çš„ç›®æ ‡æ˜¯è´¡çŒ®äºAIå¿ƒç†å­¦é¢†åŸŸæœ€ä½³å®è·µçš„æ›´å¹¿æ³›è®¨è®ºã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01276v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.01276.md)  |
| <span style='display: inline-block; width: 42px;'>12-02</span> | **Exploring and Improving the Spatial Reasoning Abilities of Large Language Models**<br><sub>æœºæ„: Stanford University  <br>è®ºæ–‡æé«˜äº†å¯¹LLMsåœ¨ç©ºé—´æ¨ç†å’Œåºåˆ—æ ‡æ³¨æ–¹é¢èƒ½åŠ›çš„ç†è§£ï¼Œæå‡ºäº†ä¸€ç§æ”¹è¿›LLMså¤„ç†3Dè½¨è¿¹è¯†åˆ«ä»»åŠ¡çš„æ–¹æ³•ï¼Œå…·æœ‰æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01054v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.01054.md)  |
| <span style='display: inline-block; width: 42px;'>12-02</span> | **Axiomatic Preference Modeling for Longform Question Answering**<br><sub>æœ¬æ–‡æå‡ºçš„åŸºäºå…¬ç†çš„æ¡†æ¶ä¸ºé•¿ç¯‡é—®ç­”åå¥½æ¨¡å‹æä¾›äº†ä¸€ç§æ–°æ–¹æ³•ï¼Œé€šè¿‡ç»†è‡´å®¡è§†äººç±»åå¥½ï¼Œå¹¶ä¼˜åŒ–äº†åå¥½æ‰“åˆ†çš„å‡†ç¡®æ€§ä¸æ•ˆç‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02206v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.02206.md)  |
| <span style='display: inline-block; width: 42px;'>12-02</span> | **Large Language Models Are Zero-Shot Text Classifiers**<br><sub>æœºæ„: Florida Atlantic University<br>è®ºæ–‡å±•ç¤ºäº†LLMså¯ä»¥æœ‰æ•ˆä½œä¸ºé›¶æ ·æœ¬æ–‡æœ¬åˆ†ç±»å™¨çš„èƒ½åŠ›ï¼Œè¿™å¯¹äºéœ€è¦å¿«é€Ÿéƒ¨ç½²æ–‡æœ¬åˆ†ç±»å™¨çš„å°å›¢é˜Ÿæˆ–å°ä¼ä¸šæ¥è¯´ç‰¹åˆ«æœ‰ç›Šã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œåœ¨æ‰€æœ‰å››ä¸ªæ•°æ®é›†ä¸­ï¼ŒGPT-4ä¸€è‡´è¶…è¿‡äº†ä¼ ç»ŸMLç®—æ³•ã€‚æ–‡ç« è¿˜å»ºè®®æœªæ¥çš„ç ”ç©¶æ–¹å‘åŒ…æ‹¬ä¼˜åŒ–æç¤ºä»¥è·å¾—æ›´é«˜çš„ç²¾åº¦æˆ–å¼•å…¥è¯„è®ºä»£ç†ä»¥è¯„ä¼°å’Œæå‡LLMçš„ç»“æœã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01044v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.01044.md)  |
| <span style='display: inline-block; width: 42px;'>12-02</span> | **Just-in-Time Security Patch Detection -- LLM At the Rescue for Data Augmentation**<br><sub>æœºæ„: University of Luxembourg, Windows Copilot Microsoft, Singapore Management University<br>è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„å®‰å…¨è¡¥ä¸æ£€æµ‹æ¡†æ¶ LLMDAï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œè¡¥ä¸åˆ†æå’Œæ•°æ®å¢å¼ºï¼Œå¹¶å¯¹å¤šæ¨¡æ€è¾“å…¥è¿›è¡Œå¯¹é½ã€‚è¿™ä½¿ç³»ç»Ÿèƒ½å¤Ÿä»è¡¥ä¸å’Œä»£ç çš„è”åˆä¸Šä¸‹æ–‡ä¸­æå–æ›´ä¸°å¯Œçš„ä¿¡æ¯ï¼Œæå‡æ£€æµ‹å‡†ç¡®æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01241v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.01241.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models**<br><sub>æœºæ„: University of Wisconsin - Madison<br>æœ¬è®ºæ–‡é€šè¿‡å¯¹å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰è¿›è¡Œå‹ç¼©æŠ€æœ¯ï¼ˆå‰ªæå’Œé‡åŒ–ï¼‰çš„å…¨é¢ç ”ç©¶ï¼Œæ­ç¤ºäº†è¿™äº›æŠ€æœ¯å¯¹æ¨¡å‹å‚æ•°çŸ¥è¯†ä¿ç•™çš„å½±å“ï¼Œä¸ºå®è·µè€…æä¾›äº†å…³äºæ¨¡å‹å‹ç¼©çš„æœ‰ä»·å€¼è§è§£ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00960v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.0096.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models**<br><sub>æœºæ„: University of Wisconsin - Madison<br>è¿™é¡¹ç ”ç©¶é¦–æ¬¡å¤§è§„æ¨¡è€ƒå¯Ÿäº†LLMsçš„å‹ç¼©æŠ€æœ¯å¯¹æ¨¡å‹å‚æ•°çŸ¥è¯†çš„å½±å“ï¼Œå¹¶ä¸ºå®é™…åº”ç”¨æä¾›äº†é‡è¦è§è§£ï¼Œç‰¹åˆ«æ˜¯åœ¨å…³äºä¿®å‰ªå’Œé‡åŒ–æŠ€æœ¯ç›¸å…³çš„å†³ç­–æ–¹é¢ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00960v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.0096.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Nash Learning from Human Feedback**<br><sub>æœºæ„: Google DeepMind<br>è¿™ç¯‡æ–‡ç« æå‡ºäº†ä¸€ç§å…¨æ–°çš„è°ƒèŠ‚å¤§å‹è¯­è¨€æ¨¡å‹ä»¥é€šè¿‡çº³ä»€å‡è¡¡ä¸äººç±»åå¥½å¯¹é½çš„æ–¹æ³•ï¼Œå±•ç¤ºäº†å…¶åœ¨å¤æ‚ä»»åŠ¡ä¸­çš„æ½œèƒ½ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†å…¶æ•ˆæœã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00886v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.00886.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games**<br><sub>æœºæ„: Quebec AI Institute<br>è¿™ç¯‡è®ºæ–‡è´¡çŒ®äº†é€‚åº”JuBenshaæ¸¸æˆå¤æ‚æ€§å’Œæ–°æŒ‘æˆ˜çš„è¯„ä¼°æ–¹æ³•ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªèƒ½å¤Ÿè¯„ä¼°äº¤äº’å¼ç¯å¢ƒä¸­LLMæ™ºèƒ½ä½“èƒ½åŠ›çš„æ–°æ¡†æ¶ThinkThriceï¼Œæ¨åŠ¨äº†AIåœ¨å¤šç©å®¶è§’è‰²æ‰®æ¼”æ¸¸æˆä¸­çš„åº”ç”¨ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00746v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.00746.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Instruction-tuning Aligns LLMs to the Human Brain**<br><sub>æœºæ„: EPFL<br>æœ¬ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡æŒ‡ä»¤è°ƒæ•´è®­ç»ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¸–ç•ŒçŸ¥è¯†è¡¨ç¤ºæ–¹é¢ä»¥åŠä¸äººè„‘æ´»åŠ¨çš„å¯¹é½ç¨‹åº¦ä¸Šè¡¨ç°æ›´ä½³ã€‚è¿™ä¸ºæœªæ¥LLMsçš„å‘å±•æä¾›äº†å°†ä¸–ç•ŒçŸ¥è¯†é›†æˆåˆ°æ¨¡å‹ä¸­çš„é‡è¦è§†è§’ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00575v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.00575.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses**<br><sub>æœºæ„: Google<br>æœ¬æ–‡ä»‹ç»äº†æ¢ç´¢LLMç³»ç»ŸExploreLLMï¼Œå®ƒé€šè¿‡ç»“åˆåŸºäºæç¤ºçš„ä»»åŠ¡åˆ†è§£æ–¹æ³•å’Œå…¨æ–°çš„ç±»ä¼¼å›¾å¼çš„å›¾å½¢ç”¨æˆ·ç•Œé¢ï¼ˆUIï¼‰ï¼Œåœ¨ç”¨æˆ·å’ŒLLMåŠ©æ‰‹ä¹‹é—´æä¾›äº†ä¸€ç§å…¨æ–°çš„äº¤äº’æ¨¡å¼ã€‚è¯¥ç³»ç»Ÿé€šè¿‡åœ¨ç»“æ„åŒ–å’Œäº¤äº’å¼ç•Œé¢ä¸­è¡¨ç¤ºç”Ÿæˆå­ä»»åŠ¡ï¼Œæ—¨åœ¨å‡è½»ç”¨æˆ·å®Œæˆå¤æ‚ä»»åŠ¡æ—¶çš„è®¤çŸ¥è´Ÿæ‹…ï¼ŒåŒæ—¶æé«˜ä¸ªæ€§åŒ–å“åº”çš„æ°´å¹³ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00763v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.00763.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Leveraging Large Language Models to Improve REST API Testing**<br><sub>æœºæ„: Georgia Institute of Technology, IBM Research<br>RESTGPTé€šè¿‡åˆ©ç”¨LLMsï¼Œç‰¹åˆ«æ˜¯GPT-3.5 Turboçš„é«˜æ•ˆå‡†ç¡®æ€§å’Œå°‘é‡ç¤ºä¾‹å­¦ä¹ çš„ç²¾å‡†æ€§ï¼Œè§£å†³äº†ç°æœ‰æ–¹æ³•åœ¨æå–è‡ªç„¶è¯­è¨€æè¿°ä¸­è§„åˆ™å’Œç”Ÿæˆæœ‰æ•ˆå€¼æ—¶çš„é™åˆ¶ï¼Œæ˜¾è‘—æå‡äº†REST APIæµ‹è¯•çš„è´¨é‡å’Œå‡†ç¡®åº¦ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00894v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.00894.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs**<br><sub>æœºæ„: Singapore Management University, National Sun Yat-sen University<br>ç ”ç©¶ç»“æœæ˜¾ç¤ºï¼ŒLLMsèƒ½å¤Ÿé€šè¿‡å…¶å†…éƒ¨çŸ¥è¯†å›¾æˆåŠŸå¤„ç†çŸ¥è¯†å›¾æ¨ç†ä»»åŠ¡ï¼Œå¹¶èƒ½ä»ä¸Šä¸‹æ–‡ä¸­æ¨æ–­å‡ºçŸ¥è¯†å›¾å…³ç³»ï¼Œå±•ç¤ºäº†LLMsåœ¨çŸ¥è¯†å›¾æ¨ç†ä¸­çš„æ½œåŠ›åŠåº”ç”¨ä»·å€¼ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00353v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.00353.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Improve Supervised Representation Learning with Masked Image Modeling**<br><sub>æœºæ„: Google Research, OpenAI  <br>è¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§èåˆç›‘ç£è¡¨ç¤ºå­¦ä¹ å’ŒMIMçš„æ–°è®­ç»ƒè®¾ç½®ï¼Œè¯¥è®¾ç½®åœ¨ä¸å¢åŠ æ˜¾è‘—çš„è®­ç»ƒæˆ–æ¨ç†å¼€é”€çš„å‰æä¸‹ï¼Œæ˜¾è‘—æé«˜äº†ä¸‹æ¸¸ä»»åŠ¡å¦‚åˆ†ç±»ã€å›¾åƒæ£€ç´¢å’Œè¯­ä¹‰åˆ†å‰²çš„è¡¨ç¤ºå­¦ä¹ è´¨é‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00950v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.0095.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Learning from One Continuous Video Stream**<br><sub>è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºä»å•ä¸€è¿ç»­è§†é¢‘æµä¸­è¿›è¡Œåœ¨çº¿å­¦ä¹ ï¼Œè¿™ä¸€æ¡†æ¶ä¾§é‡äºé€‚åº”æ€§ä¸æ³›åŒ–çš„è¯„ä¼°ï¼Œå¹¶æå‡ºäº†ä¸€ç³»åˆ—æœªæ¥é¢„æµ‹ä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒã€‚ç ”ç©¶æ˜¾ç¤ºï¼Œåœ¨è¿™ç§å­¦ä¹ ç¯å¢ƒä¸‹ï¼Œä¼˜åŒ–ç­–ç•¥éœ€è¦è°ƒæ•´ï¼Œé€šè¿‡å‡å°‘åŠ¨é‡å’Œè°ƒæ•´æƒé‡æ›´æ–°é¢‘ç‡å¯ä»¥æ”¹å–„æ¨¡å‹çš„é€‚åº”æ€§å’Œæ³›åŒ–èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00598v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.00598.md)  |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback**<br><sub>RLHF-Væ˜¯ä¸€ä¸ªé€šè¿‡ç»†ç²’åº¦æ ¡æ­£å‹äººç±»åé¦ˆæ ¡æ­£MLLMè¡Œä¸ºçš„æ–°æ¡†æ¶ï¼Œé€šè¿‡æ”¶é›†é«˜è´¨é‡çš„äººç±»åå¥½æ•°æ®ä¸ºMLLMsæä¾›äººç±»å¯¹é½çš„å­¦ä¹ ä¿¡å·ï¼Œå¹¶é€šè¿‡å…¨é¢çš„å®éªŒéªŒè¯äº†è¯¥æ¡†æ¶çš„æœ‰æ•ˆæ€§ã€‚è¯¥ç ”ç©¶å¯èƒ½åœ¨æé«˜å¤§å‹å¤šæ¨¡æ€è¯­è¨€æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­çš„å¯é æ€§å’Œå®ç”¨æ€§æ–¹é¢å–å¾—é‡è¦è¿›å±•ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00849v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-12/2312.00849.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/RLHF-V/RLHF-V)</div> |

---

### 11æœˆ

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **TaskBench: Benchmarking Large Language Models for Task Automation**<br><sub>æœºæ„: Zhejiang University<br>è¯¥æ–‡çŒ®æå‡ºäº†TaskBenchåŸºå‡†æµ‹è¯•å’ŒTASKEVALè¯„ä¼°ç³»ç»Ÿï¼Œé€šè¿‡æ•°æ®ç”Ÿæˆå’Œé‡åŒ–è¯„ä¼°ç³»ç»Ÿï¼Œæœ‰æ•ˆåœ°è§£å†³äº†åœ¨ä»»åŠ¡è‡ªåŠ¨åŒ–é¢†åŸŸå¯¹LLMsçš„è¯„ä¼°é—®é¢˜ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18760v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.1876.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **Applying Large Language Models and Chain-of-Thought for Automatic Scoring**<br><sub>æœºæ„: University of Georgia<br>æœ¬æ–‡å±•ç¤ºäº†LLMsåœ¨ä¿ƒè¿›è‡ªåŠ¨è¯„åˆ†æ–¹é¢çš„æ½œåŠ›ï¼Œå¹¶å¼ºè°ƒCoTåœ¨é…åˆé¡¹èŒå’Œè¯„åˆ†æ ‡å‡†ä½¿ç”¨æ—¶èƒ½æ˜¾è‘—å¢å¼ºè¯„åˆ†çš„å‡†ç¡®åº¦ã€‚é€šè¿‡ç»“åˆLLMså’ŒCoTçš„æ–¹æ³•ï¼Œå¯ä»¥é™ä½è‡ªåŠ¨è¯„åˆ†æ¨¡å‹æ„å»ºçš„å¤æ‚æ€§å’ŒäººåŠ›æˆæœ¬ï¼Œå¹¶å¯èƒ½æä¾›æ›´æ¥è¿‘äººç±»è¯„åˆ†ç»“æœçš„è¯„åˆ†ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03748v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2312.03748.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions**<br><sub>æœºæ„: Huawei Poisson Lab<br>IAGæ¡†æ¶é€šè¿‡å½’çº³æç¤ºæ³•åŠ å¼ºçŸ¥è¯†é™ˆè¿°çš„çœŸå®æ€§ï¼Œå¹¶ä¸”ä¼˜åŒ–äº†çŸ¥è¯†èåˆæœºåˆ¶å’Œå­¦ç”Ÿå½’çº³æ¨¡å‹ï¼Œä»¥è§£å†³ç°æœ‰åŸºäºæ£€ç´¢çš„æ–¹æ³•åœ¨éšæ€§æ¨ç†é—®ç­”ä»»åŠ¡ä¸Šçš„ä¸è¶³ã€‚ç ”ç©¶æˆæœè¡¨æ˜ï¼ŒIAGåœ¨å›ç­”æ¶‰åŠéšæ€§æ¨ç†çš„é—®ç­”ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18397v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.18397.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **Autonomous Agents in Software Development: A Vision Paper**<br><sub>æœºæ„: Tampere University<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªå…³äºåˆ©ç”¨å¤šä¸ª GPT ä»£ç†æ¥è‡ªåŠ¨æ‰§è¡Œè½¯ä»¶å·¥ç¨‹ä»»åŠ¡çš„æ„¿æ™¯ï¼Œå¹¶æ¼”ç¤ºäº†åœ¨ç®€å•è½¯ä»¶ä»»åŠ¡ä¸Šæ‰€å–å¾—çš„åˆæ­¥æˆåŠŸã€‚è¿™é¡¹å·¥ä½œæœ‰å¯èƒ½å½»åº•æ”¹å˜è½¯ä»¶å¼€å‘çš„æ–¹å¼ï¼Œå¹¶ç¼©çŸ­å¼€å‘æ—¶é—´ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18440v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.1844.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation**<br><sub>æœºæ„: University of Science and Technology of China, Microsoft Research Asia<br>MicroCinemaä»¥å…¶åˆ›æ–°çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆä¸¤é˜¶æ®µæµç¨‹å’Œæœ‰æ•ˆçš„Appearance Injection NetworkåŠAppearance Noise Prioræœºåˆ¶ï¼Œåœ¨è§†é¢‘ç”Ÿæˆè´¨é‡ä¸Šå®ç°äº†æ–°çš„çªç ´ï¼Œä¸ºåç»­å·¥ä½œæä¾›äº†å¯å€Ÿé‰´çš„èŒƒä¾‹ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18829v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.18829.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **PoseGPT: Chatting about 3D Human Pose**<br><sub>æœºæ„: Max Planck Institute for Intelligent Systems, Meshcapade<br>PoseGPTæ˜¯ä¸€ä¸ªæ–°å‹æ¡†æ¶ï¼Œå®ƒé€šè¿‡åœ¨LLMä¸­åµŒå…¥SMPLå§¿æ€æ ‡è®°ï¼Œä½¿æ¨¡å‹å¯ä»¥ç›´æ¥ä»æ–‡æœ¬å’Œè§†è§‰è¾“å…¥ç”Ÿæˆä¸‰ç»´äººä½“å§¿æ€ï¼Œå¹¶åœ¨è§£é‡Šä¸‰ç»´äººä½“å§¿æ€æ–¹é¢å®ç°äº†ä¸€å®šç¨‹åº¦çš„åˆ›æ–°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18836v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.18836.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation**<br><sub>æœºæ„: UC Berkeley, Microsoft Azure AI, ZOOM<br>CoDi-2æ˜¯ä¸€ç§å…·æœ‰å‰æ²¿èƒ½åŠ›çš„å¤šæ¨¡æ€ç”Ÿæˆæ¨¡å‹ï¼Œå¯ä»¥å¤„ç†å¤æ‚çš„å¤šæ¨¡æ€è¾“å…¥ã€åœ¨ä¸Šä¸‹æ–‡ä¸­æŒ‡å¯¼ç”Ÿæˆã€é€šè¿‡å¤šè½®äº¤äº’ä¸ç”¨æˆ·äº’åŠ¨ï¼Œå¹¶å®ç°äº†ä¼˜ç§€çš„é›¶æ ·æœ¬å’Œå°‘æ ·æœ¬æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18775v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.18775.md)  |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations**<br><sub>æœºæ„: Comcast Applied AI, University of Waterloo<br>ä½œè€…ä»¬æå‡ºäº†ä¸€ä¸ªæ–°å‹æ¢é’ˆæ¥æ£€æµ‹LLMsè¡¨ç¤ºä¸­çš„å†…éšå…³è”åè§ï¼Œå¹¶é€šè¿‡å®éªŒåœ¨åå¥½æ£€æµ‹ä¸­è¾¾åˆ°äº†æœ€æ–°æ°´å¹³ã€‚ç ”ç©¶è¿˜å‘ç°äº†å¤šä¸ªæŒ‡ä»¤éµå¾ªå‹å’Œâ€œä¼ ç»Ÿâ€çš„LLMsä¸­çš„æ˜¾è‘—åè§ï¼Œè¿™äº›åè§å­˜åœ¨äºå›½ç±ã€æ”¿æ²»ã€å®—æ•™å’Œæ€§åˆ«ç­‰æ–¹é¢ï¼Œå°½ç®¡LLMså·²ç»ç»è¿‡æ˜ç¡®çš„å®‰å…¨æŒ‡å¯¼è°ƒæ•´ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18812v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.18812.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/castorini/biasprobe)</div> |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text**<br><sub>æœºæ„: The University of Tokyo<br>ç ”ç©¶å±•ç¤ºäº†GPT-4å¤„ç†æ··æ·†æ–‡æœ¬çš„å¼ºå¤§èƒ½åŠ›ï¼Œè®¾ç½®äº†ä¸¤é¡¹æ–°æŒ‡æ ‡RRå’ŒRPGï¼Œå¹¶é€šè¿‡å®ƒä»¬éªŒè¯äº†GPT-4åœ¨ä¸åŒæ··æ·†åœºæ™¯å’Œæ¯”ç‡ä¸‹çš„ç¨³å®šè¡¨ç°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18805v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.18805.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering**<br><sub>æœºæ„: Sun Yat-Sen University<br>è¿™é¡¹å·¥ä½œé€šè¿‡åˆ›æ–°æ€§åœ°ç»“åˆä¸‰ä¸ªä»£ç†æ¥æ¨¡æ‹Ÿäººç±»è®¤çŸ¥ä¸­çš„è‡ªé¡¶å‘ä¸‹æ¨ç†è¿‡ç¨‹ï¼Œå¹¶å¼•å…¥äº†å¤šè§†è§’çŸ¥è¯†åº“çš„æ¦‚å¿µï¼Œæ˜¾è‘—æå‡äº†VQAæ¨¡å‹çš„è¡¨ç°åŠ›å’Œè§£é‡Šèƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17331v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.17331.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Understanding and Improving In-Context Learning on Vision-language Models**<br><sub>æœºæ„: LMU Munich, University of Oxford<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºè§†è§‰-è¯­è¨€æ¨¡å‹åœ¨èƒŒæ™¯å­¦ä¹ ä¸­é€‰æ‹©ç¤ºèŒƒçš„æ–°æ–¹æ³•MMICESï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—å®éªŒå±•ç¤ºäº†å…¶åœ¨ä¸åŒæ¨¡å‹å’Œæ•°æ®é›†ä¸Šçš„è‰¯å¥½æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18021v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.18021.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Zero-shot Conversational Summarization Evaluations with small Large Language Models**<br><sub>æœºæ„: Intel labs<br>æ–‡ç« ä»¥å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä¼šè¯æ‘˜è¦ä»»åŠ¡ä¸­çš„åº”ç”¨ä½œä¸ºç„¦ç‚¹ï¼Œæ·±å…¥æ¢è®¨äº†ä¸åŒæŒ‡ä»¤å¯¹æ¨¡å‹æ‰§è¡Œæ•ˆæœçš„å½±å“ï¼Œå¹¶ç ”ç©¶äº†åœ¨æœ‰é™ç¡¬ä»¶ä¸‹ä½¿ç”¨å‹ç¼©æ¨¡å‹çš„ä¼˜åŒ–æ–¹æ³•ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18041v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.18041.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation**<br><sub>æœºæ„: The Education University of Hong Kong<br>è¿™ç¯‡è®ºæ–‡ä»£è¡¨äº†ä¸€æ¬¡å¼€åˆ›æ€§çš„å°è¯•ï¼Œæ„å»ºäº†ä¸€ä¸ªå¯ä»¥é€‚åº”ä»»ä½•å­¦ç§‘å¹¶æä¾›é«˜è´¨é‡çš„å®šåˆ¶åŒ–æ•™è‚²æ”¯æŒçš„AIå¯¼å¸ˆç³»ç»Ÿã€‚è¿™ä¸ä»…èƒ½ä¿ƒè¿›AIæ•™è‚²æŠ€æœ¯çš„åº”ç”¨ï¼Œè€Œä¸”ä¸ºAIæ•™å­¦ç³»ç»Ÿçš„å‘å±•å¼€è¾Ÿäº†æ–°è·¯å¾„ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17696v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.17696.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **TaskWeaver: A Code-First Agent Framework**<br><sub>æœºæ„: Microsoft<br>TaskWeaveræ˜¯ä¸ºæ„å»ºåŸºäºLLMçš„è‡ªæ²»ä»£ç†è€Œè®¾è®¡çš„ä»£ç ä¼˜å…ˆæ¡†æ¶ï¼Œå®ç°äº†å¯¹å¤æ‚æ•°æ®çš„é«˜æ•ˆå¤„ç†ä»¥åŠæ’ä»¶çš„çµæ´»ä½¿ç”¨ï¼Œå¹¶å°†ç‰¹å®šåŸŸçŸ¥è¯†æˆåŠŸæ•´åˆå…¥ç³»ç»Ÿä¸­ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17541v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.17541.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/microsoft/TaskWeaver)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models**<br><sub>æœºæ„: Harbin Institute of Technology<br>TIMEBENCHåŸºå‡†çš„æå‡ºæ˜¯å¯¹å¤§å‹è¯­è¨€æ¨¡å‹æ—¶é—´æ¨ç†èƒ½åŠ›ç»¼åˆè¯„ä¼°çš„é‡è¦æ­¥éª¤ï¼Œå®ƒå±•ç¤ºäº†å½“å‰æ¨¡å‹ä¸äººç±»åœ¨è¿™æ–¹é¢çš„å·®è·ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æä¾›äº†æŒ‡å¼•ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17667v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.17667.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/zchuz/TimeBench)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Large Language Models for Networking: Applications, Enabling Techniques, and Challenges**<br><sub>æœºæ„: BUPT<br>è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ•´åˆå¤§å‹è¯­è¨€æ¨¡å‹ä¸ç½‘ç»œæŠ€æœ¯çš„æ–°æ¡†æ¶ChatNetï¼Œå¹¶æ¢ç©¶äº†å®ƒåœ¨ç½‘ç»œè§„åˆ’ä¸­çš„åº”ç”¨ã€‚ç ”ç©¶è¡¨æ˜ï¼ŒChatNetå¯ä»¥æœ‰æ•ˆæå‡ç½‘ç»œä»»åŠ¡çš„è‡ªåŠ¨åŒ–å’Œæ™ºèƒ½åŒ–æ°´å¹³ï¼Œå°½ç®¡åœ¨éƒ¨ç½²å‰ä»éœ€è§£å†³å¤šæ¨¡æ€æ•°æ®æ•´åˆå’Œæ’ä»¶å¼€å‘ç­‰æŒ‘æˆ˜ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17474v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.17474.md)  |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Are Large Language Models Good Fact Checkers: A Preliminary Study**<br><sub>æœºæ„: Chinese Academy of Sciences<br>è¿™ç¯‡æ–‡ç« é€šè¿‡ç³»ç»Ÿè¯„ä¼°LLMsåœ¨æ•´ä¸ªäº‹å®æ ¸æŸ¥æµç¨‹ä¸­çš„æ½œåŠ›ï¼Œå‘ç°å°½ç®¡LLMsåœ¨æŸäº›æ–¹é¢è¡¨ç°å‡ºæ½œåŠ›ï¼Œä½†ä¾ç„¶éœ€è¦æ›´å¤šç ”ç©¶å’Œå°è¯•æ¥æå‡å®ƒä»¬åœ¨äº‹å®æ ¸æŸ¥ä»»åŠ¡ä¸Šçš„è¡¨ç°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17355v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.17355.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Graph Prompt Learning: A Comprehensive Survey and Beyond**<br><sub>æœºæ„: The Chinese University of Hong Kong, Hong Kong University of Science and Technology, Fudan University  <br>è®ºæ–‡æ˜¯å…³äºå›¾æç¤ºå­¦ä¹ çš„ç»¼åˆæ€§è°ƒç ”ï¼Œæ¶µç›–äº†AGIåœ¨å›¾æ•°æ®å¤„ç†æ–¹é¢é¢ä¸´çš„æŒ‘æˆ˜ä»¥åŠå¦‚ä½•é€šè¿‡å›¾æç¤ºå­¦ä¹ æ¥å®ç°AGIæŠ€æœ¯çš„è·¨æ¨¡æ€ã€è·¨åŸŸå’Œè·¨ä»»åŠ¡é€‚ç”¨æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16534v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.16534.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/WxxShirley/Awesome-Graph-Prompt)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation**<br><sub>æœºæ„: Alibaba Group<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåˆ©ç”¨æ‰©æ•£æ¨¡å‹è¿›è¡Œè§’è‰²åŠ¨ç”»çš„æ–°æ¡†æ¶â€œAnimate Anyoneâ€ã€‚è¯¥æ¡†æ¶é€šè¿‡ReferenceNetä¿æŒå¤–è§‚ä¸€è‡´æ€§ï¼Œå¹¶é€šè¿‡å§¿æ€å¼•å¯¼å™¨ä¸æ—¶é—´å±‚ç¡®ä¿åŠ¨ç”»çš„å¯æ§æ€§ä¸è¿ç»­æ€§ï¼Œå–å¾—äº†å…ˆè¿›çš„è§’è‰²åŠ¨ç”»ç”Ÿæˆç»“æœã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17117v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.17117.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/HumanAIGC/AnimateAnyone)</div><div style='min-width:85px;'>[![Blog](https://img.shields.io/badge/Blog-Posts-yellow?logo=rss)](https://humanaigc.github.io/animate-anyone/)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?**<br><sub>æœºæ„: Nanyang Technological University<br>è¿™ç¯‡ç»¼è¿°æ–‡ç« æä¾›äº†å¯¹å¼€æºLLMsåœ¨å¤šä»»åŠ¡é¢†åŸŸç›¸è¾ƒChatGPTçš„æ€§èƒ½è¯„ä¼°çš„è€ƒå¯Ÿï¼Œçªå‡ºäº†ç›®å‰å¼€æºLLMsçš„å¼ºé¡¹å’Œæ½œåœ¨é—®é¢˜ï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶å’Œå¼€å‘æä¾›äº†å¯ç¤ºã€‚æ­¤å¤–ï¼Œæ–‡ç« è¿˜æ€»ç»“äº†ä¼—å¤šçš„æœ€ä½³å®è·µå’ŒæŒ‘æˆ˜ï¼Œæ˜¾ç¤ºå‡ºå¼€æºé¢†åŸŸåœ¨ä¸€å®šç¨‹åº¦ä¸Šæœ‰æœ›ç¼©å°ä¸å•†ä¸šæ¨¡å‹ä¹‹é—´çš„å·®è·ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16989v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.16989.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Training Chain-of-Thought via Latent-Variable Inference**<br><sub>æœºæ„: Google<br>æœ¬è®ºæ–‡å¼€å‘äº†ä¸€ç§åŸºäºMCMC-EMçš„å¾®è°ƒç­–ç•¥ï¼Œé€šè¿‡å¹³å‡ç†ç”±å¸®åŠ©LLMsç”Ÿæˆæ­£ç¡®çš„ç­”æ¡ˆï¼Œå…·æœ‰æ½œåœ¨çš„æ¨å¹¿åº”ç”¨çš„æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02179v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2312.02179.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine**<br><sub>æœºæ„: Microsoft<br>æœ¬æ–‡é€šè¿‡ç³»ç»Ÿçš„æç¤ºå·¥ç¨‹æ–¹æ³•æ¢è®¨äº†åœ¨æ— éœ€ä¸“å®¶ç›‘ç£çš„æƒ…å†µä¸‹ï¼Œå¦‚ä½•æŒ‡å¯¼é€šç”¨çš„åŸºç¡€æ¨¡å‹åœ¨ä¸“ä¸šä»»åŠ¡ä¸Šå‘æŒ¥ä¸“å®¶çº§åˆ«çš„èƒ½åŠ›ï¼Œå…·ä½“ä»¥åŒ»å­¦é¢†åŸŸä¸ºæ¡ˆä¾‹ç ”ç©¶ã€‚æ‰€æå‡ºçš„Medpromptç­–ç•¥è¯æ˜äº†å…¶åœ¨å¢å¼ºåŸºç¡€æ¨¡å‹ä¸“ä¸šèƒ½åŠ›æ–¹é¢çš„æ˜¾è‘—ä¼˜åŠ¿ï¼Œå¹¶å±•ç¤ºäº†å¹¿æ³›é€‚ç”¨äºå¤šä¸ªå­¦ç§‘çš„å¯èƒ½æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16452v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.16452.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **LLaFS: When Large-Language Models Meet Few-Shot Segmentation**<br><sub>æœºæ„: Singapore University of Technology and Design, Zhejiang University <br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„å°æ ·æœ¬å›¾åƒåˆ†å‰²æ¡†æ¶ï¼Œå¹¶è§£å†³äº†è®©LLMsç†è§£å’Œæ‰§è¡Œè§†è§‰ä»»åŠ¡çš„æ ¸å¿ƒæŒ‘æˆ˜ã€‚é€šè¿‡å®šåˆ¶æŒ‡å¯¼å’Œç»†ç²’åº¦ä¸Šä¸‹æ–‡æŒ‡å¯¼ç›¸ç»“åˆçš„æ–¹æ³•ï¼Œå®ç°äº†é«˜è´¨é‡çš„å°æ ·æœ¬åˆ†å‰²ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16926v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.16926.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/lanyunzhu99/LLaFS)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **RELIC: Investigating Large Language Model Responses using Self-Consistency**<br><sub>æœºæ„: ETH Zurich<br>RELICæ˜¯ä¸€ä¸ªäº¤äº’å¼ç³»ç»Ÿï¼Œå®ƒé€šè¿‡å¤šæ ·æœ¬çš„äº‹å®ä¸€è‡´æ€§æ£€éªŒï¼Œå¸®åŠ©ç”¨æˆ·éªŒè¯å’ŒæŒ‡å¯¼LLMsç”Ÿæˆçš„æ–‡æœ¬ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16842v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.16842.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **RankingGPT: Empowering Large Language Models in Text Ranking with Progressive Enhancement**<br><sub>æœºæ„: Alibaba Group<br>æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºæ–‡æœ¬æ’åºçš„äºŒé˜¶æ®µè®­ç»ƒæ¨¡å‹ï¼Œç»“åˆäº†å¼±ç›‘ç£é¢„è®­ç»ƒå’Œç›‘ç£ç»†åŒ–è®­ç»ƒï¼Œé€šè¿‡åœ¨ä¸æŸå®³é¢„è®­ç»ƒç›Šå¤„çš„åŸºç¡€ä¸Šå¢å¼ºæ¨¡å‹ç»†åŒ–è®­ç»ƒæ€§èƒ½ï¼Œå®Œæˆäº†ä»é¢„è®­ç»ƒåˆ°ç»†åŒ–è®­ç»ƒçš„å¹³æ»‘è¿‡æ¸¡ï¼Œå¹¶åœ¨å®éªŒä¸­æ˜¾è‘—ä¼˜äºç°æœ‰æŠ€æœ¯ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16720v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.1672.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond**<br><sub>ç ”ç©¶æå‡ºäº†ä¸€ä¸ªåˆ›æ–°çš„ï¼Œä¸€ä½“åŒ–çš„æ¡†æ¶AvatarGPTï¼Œç”¨äºå¤„ç†ç†è§£ã€è§„åˆ’ä»¥åŠç”Ÿæˆäººç±»åŠ¨ä½œç›¸å…³çš„é«˜çº§å’Œä½çº§ä»»åŠ¡ï¼Œå±•ç°å‡ºé•¿æ—¶é—´è¿åŠ¨åˆæˆçš„èƒ½åŠ›å’Œå‡å°‘æ‰‹åŠ¨å¹²é¢„çš„å¯èƒ½æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16468v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.16468.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization**<br><sub>æœºæ„: Shanghai AI Laboratory<br>æ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°é¢–çš„ç­–ç•¥æ¥ä¼˜åŒ–LVLMså¹¶å‡å°‘å¹»è§‰ç°è±¡ï¼ŒåŒæ—¶ä»‹ç»äº†ä¸€ç§æ–°çš„è¯„ä¼°æ–¹æ³•æ¥æ›´å…¨é¢åœ°è¡¡é‡å¹»è§‰ç°è±¡ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†æ‰€ææ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16839v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.16839.md)  |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Prompting in Autoregressive Large Language Models**<br><sub>æœºæ„: George Mason University<br>æœ¬è®ºæ–‡ä¸ºè‡ªå›å½’å¤§å‹è¯­è¨€æ¨¡å‹çš„æç¤ºæŠ€æœ¯é¢†åŸŸæä¾›äº†ä¸€ä¸ªç´§å‡‘çš„æ–‡çŒ®ç»¼è¿°ï¼Œå¹¶æŒ‡å‡ºäº†ä¸€äº›å°šæœªè§£å†³çš„æŒ‘æˆ˜å’Œå¼€æ”¾æ€§é—®é¢˜ï¼Œä¸ºæœªæ¥ç ”ç©¶æä¾›äº†æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03740v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2312.0374.md)  |
| <span style='display: inline-block; width: 42px;'>11-27</span> | **RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks**<br><sub>æœºæ„: Chinese Academy of Sciences, Peking University<br>æ–‡ç« æå‡ºäº†ä¸€ä¸ªåä¸ºRoboGPTçš„æ™ºèƒ½ä½“ï¼Œè¯¥æ™ºèƒ½ä½“ç”¨äºåˆ¶å®šæ‰§è¡Œæ—¥å¸¸æŒ‡ä»¤ä»»åŠ¡çš„é•¿æœŸå†³ç­–ã€‚è¯¥æ™ºèƒ½ä½“é€šè¿‡ä¸€é¡¹æ–°çš„æœºå™¨äººæ•°æ®é›†ï¼Œç»“åˆäº†LLMsçš„é€šç”¨çŸ¥è¯†å’Œæœºå™¨äººé¢†åŸŸçš„ä¸“ä¸šçŸ¥è¯†ï¼Œå¹¶å¼•å…¥äº†Re-Planæ¨¡å—å’ŒRoboSkillæ¨¡å—ä»¥å¢å¼ºä»»åŠ¡è§„åˆ’çš„é€»è¾‘æ€§å’Œé€‚åº”æ€§ã€‚åœ¨ALFREDåŸºå‡†æµ‹è¯•å’Œæ³›åŒ–ä»»åŠ¡ä¸Šï¼ŒRoboGPTä¼˜äºç°æœ‰çš„å…ˆè¿›æ–¹æ³•ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.15649v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.15649.md)  |
| <span style='display: inline-block; width: 42px;'>11-25</span> | **Faster Minimum Bayes Risk Decoding with Confidence-based Pruning**<br><sub>æœºæ„: University of Cambridge<br>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç”¨äºMBRè§£ç çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•é€šè¿‡åœ¨æ ·æœ¬ä¼°è®¡ä¸­é€æ¸å¢åŠ æ ·æœ¬æ•°é‡å¹¶ä½¿ç”¨ç½®ä¿¡åº¦å‰ªææ¥å‡å°‘ç”¨æˆ·å‡½æ•°è°ƒç”¨ã€‚åœ¨ä¿æŒå‡†ç¡®åº¦çš„åŒæ—¶ï¼Œè¯¥ç®—æ³•æ˜¾è‘—é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶é€šè¿‡ä¸‰ç§è¯­è¨€å¯¹çš„NMTå®éªŒå¾—åˆ°äº†éªŒè¯ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.14919v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.14919.md)  |
| <span style='display: inline-block; width: 42px;'>11-24</span> | **Calibrated Language Models Must Hallucinate**<br><sub>æœºæ„: Microsoft Research<br>è¯¥æ–‡ç« å±•ç¤ºäº†é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹åœ¨å……åˆ†æ ¡å‡†çš„æ¡ä»¶ä¸‹ï¼Œå¿…ç„¶äº§ç”Ÿå¹»è§‰çš„ç»Ÿè®¡æ ¹æºï¼Œå¹¶ä»‹ç»äº†é¢„æµ‹æ€§èƒ½è‰¯å¥½çš„æ¨¡å‹å›ºæœ‰çš„å¹»è§‰äº§ç”Ÿæœºåˆ¶ã€‚åŒæ—¶ï¼Œæ–‡ç« è¿˜æä¾›äº†å¹»è§‰äº§ç”Ÿç‡çš„ä¸‹ç•Œä¼°ç®—ï¼Œå¹¶æ¢è®¨äº†ä¸åŒç±»å‹äº‹å®äº§ç”Ÿå¹»è§‰çš„å¯èƒ½æ€§ï¼ŒæŒ‡å‡ºäº†æœªæ¥å‡è½»ç‰¹å®šç±»å‹å¹»è§‰çš„å¯èƒ½æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.14648v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.14648.md)  |
| <span style='display: inline-block; width: 42px;'>11-24</span> | **Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language**<br><sub>æœºæ„: Amazon<br>æ–‡ç« æå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„CnRæ–¹æ³•ï¼Œå®ƒèƒ½å¤Ÿé€šè¿‡ä½¿ç”¨è‡ªç„¶è¯­è¨€çš„ç²¾ç»†åé¦ˆå’Œå“åº”ä¿®æ­£ï¼Œé«˜æ•ˆåœ°æ ¡å‡†LLMsä»¥ç¬¦åˆäººç±»é¢„æœŸã€‚é€šè¿‡ç›¸å¯¹è¾ƒå°‘çš„äººç±»åé¦ˆæ•°æ®ï¼Œæ­¤æ–¹æ³•å¯ä»¥æ˜¾è‘—æ”¹å–„å³ä½¿æ˜¯é¡¶å°–LLMsçš„å“åº”è´¨é‡ï¼Œå¦‚ChatGPTã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.14543v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.14543.md)  |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach**<br><sub>æœºæ„: Chinese Academy of Sciences<br>LLaMACæ¡†æ¶å±•ç¤ºäº†åŸºäºLLMçš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿåœ¨é•¿æœŸè§„åˆ’ã€æ•°å­¦æ¨ç†ã€ä¼˜åŒ–é—®é¢˜å’Œç©ºé—´æ¨ç†æ–¹é¢çš„å“è¶Šè¡¨ç°ï¼Œå¹¶ä¸”å‡å°‘äº†å¤§è§„æ¨¡å¤šæ™ºèƒ½ä½“åä½œçš„è®¿é—®æˆæœ¬ã€‚éšç€LLMçš„è¿›ä¸€æ­¥æå‡å’Œæ›´å¤šåä½œæ¡†æ¶çš„å‡ºç°ï¼Œå¤šæ™ºèƒ½ä½“åä½œé¢†åŸŸå°†è¿æ¥æ–°çš„å‘å±•æœºé‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13884v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.13884.md)  |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs**<br><sub>æœºæ„: Google Research<br>æ–‡ç« æå‡ºäº†ä¸€ç§åä¸ºZipLoRAçš„æ–°ç­–ç•¥ï¼Œæ—¨åœ¨é€šè¿‡ä¸€ä¸ªä¼˜åŒ–è¿‡ç¨‹æœ‰æ•ˆåœ°åˆå¹¶ç‹¬ç«‹è®­ç»ƒçš„ä¸»é¢˜å’Œé£æ ¼LoRAsï¼Œä»è€Œèƒ½å¤Ÿç”Ÿæˆä»»ä½•ç”¨æˆ·æä¾›çš„ä¸»é¢˜é£æ ¼çš„ç»„åˆã€‚ZipLoRAå¯¹ç”Ÿæˆä»»ä½•ç‰¹å®šä¸»é¢˜å’Œé£æ ¼çš„å›¾åƒè¿™ä¸€å¼€æ”¾æ€§ç ”ç©¶é—®é¢˜æä¾›äº†åˆ›æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œä¸”ç”±äºå…¶æ— éœ€æ‰‹åŠ¨è¶…å‚æ•°è°ƒæ•´ï¼Œä½¿ç”¨èµ·æ¥æ›´åŠ ç®€ä¾¿é«˜æ•ˆã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•åœ¨ä¿æŒä¸»é¢˜å’Œé£æ ¼çœŸå®æ€§çš„åŒæ—¶ï¼Œç›¸æ¯”äºç°æœ‰æ–¹æ³•å’Œå…¶ä»–åŸºæœ¬æ–¹æ³•è€Œè¨€ï¼Œå…·æœ‰æ›´å¥½çš„ç”Ÿæˆè´¨é‡å’Œé²æ£’æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13600v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.136.md)  |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **Diffusion Model Alignment Using Direct Preference Optimization**<br><sub>æœºæ„: Nikhil Naik, Stanford University<br>æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºDiffusion-DPOçš„æ–¹æ³•ï¼Œå…¶é€šè¿‡ç›´æ¥ä¼˜åŒ–åŸºäºäººç±»æ¯”è¾ƒæ•°æ®çš„æ¨¡å‹æ¥å®ç°å¯¹æ‰©æ•£æ¨¡å‹ä¸äººç±»åå¥½çš„å¯¹é½ã€‚æ­¤å¤–ï¼Œæ–‡ç« ä¹Ÿæ¢ç´¢äº†åŸºäºAIåé¦ˆçš„è®­ç»ƒï¼Œå–å¾—äº†ä¸åŸºäºäººç±»åå¥½è®­ç»ƒç›¸åª²ç¾çš„æˆç»©ã€‚è¿™æ˜æ˜¾æå‡äº†æ¨¡å‹åœ¨è§†è§‰å¸å¼•åŠ›å’Œæ–‡æœ¬å¯¹é½æ–¹é¢çš„æ€§èƒ½ï¼Œä¸ºåˆ©ç”¨AIåé¦ˆæ‰©å±•æ‰©æ•£æ¨¡å‹å¯¹é½æ–¹æ³•æä¾›äº†æ–°çš„é€”å¾„ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12908v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.12908.md)  |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes**<br><sub>æœºæ„: ASRI<br>LucidDreameræ˜¯ä¸€ä¸ªèƒ½å¤Ÿç”¨äºç”Ÿæˆé€¼çœŸè€Œä¸”åˆ†è¾¨ç‡æ›´é«˜çš„3Dåœºæ™¯çš„æ¨¡å‹ã€‚å®ƒä¼˜äºç°æœ‰çš„åœºæ™¯ç”Ÿæˆæ¨¡å‹ï¼Œå› ä¸ºå®ƒä¸ä¾èµ–ç‰¹å®šçš„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶èƒ½å¤Ÿé€‚åº”å¤šç§è¾“å…¥æ ·å¼ã€‚LucidDreameré€šè¿‡çº¦æŸç‚¹äº‘çš„ç§»åŠ¨å’Œä½¿ç”¨æ’å€¼ç®—æ³•ï¼Œå…‹æœäº†å½¢çŠ¶æ‰­æ›²å’Œç‚¹äº‘ä¸å›¾åƒé”™ä½çš„é—®é¢˜ï¼Œä»è€Œåœ¨æ“çºµ3Dç©ºé—´ä¸­çš„ç‚¹äº‘æ—¶ä¿æŒäº†åœºæ™¯çš„çœŸå®æ„Ÿå’Œä¸€è‡´æ€§ã€‚åœ¨å®éªŒä¸­æ˜æ˜¾å±•ç¤ºäº†å…¶ä¼˜è¶Šæ€§å’Œé«˜æ³›åŒ–èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13384v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.13384.md)  |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **GAIA: a benchmark for General AI Assistants**<br><sub>æœºæ„: FAIR, Meta<br>GAIA æ˜¯ä¸€é¡¹é’ˆå¯¹é€šç”¨äººå·¥æ™ºèƒ½åŠ©ç†çš„åŸºå‡†æµ‹è¯•ï¼Œå…¶ç›®çš„åœ¨äºæå‡ºçœŸå®ä¸–ç•Œçš„æŒ‘æˆ˜æ€§é—®é¢˜ï¼Œå¹¶é¿å¼€ä¼ ç»Ÿ LLMs è¯„ä»·ä¸­çš„è®¸å¤šé™·é˜±ã€‚è¯¥åŸºå‡†æµ‹è¯•å¼ºè°ƒä»»åŠ¡å¯¹äººç±»ç®€å•è€Œå¯¹AIéš¾åº¦è¾ƒå¤§ï¼Œä»¥æ­¤æ¥è¯„ä¼°AIçš„æ‰§è¡Œå¤æ‚è¡ŒåŠ¨åºåˆ—çš„å‡†ç¡®èƒ½åŠ›ï¼Œè¿™äº›ä»»åŠ¡åœ¨è®¾è®¡ä¸Šæ— æ³•ç®€å•åœ°é€šè¿‡æš´åŠ›æ–¹æ³•å¾—ä»¥è§£å†³ã€‚GAIA è¿˜è€ƒè™‘äº†å¦‚ä½•æ‰©å±•åŸºå‡†æµ‹è¯•ï¼Œå¹¶æ¢è®¨äº†ä¸€äº›æœ€å…ˆè¿›çš„åŠ©ç†çš„æˆåŠŸä¸çŸ­æ¿ï¼Œå±•ç¤ºäº†å¢å¼º LLMs çš„æ½œåŠ›ã€‚æœ€ç»ˆï¼Œæ–‡ç« æ—¨åœ¨è®¾ç«‹ä¸€ä¸ªå¼€å‘è€…é—®é¢˜é›†ï¼Œä¸ºäººå·¥æ™ºèƒ½ç ”ç©¶æä¾›ä¸€ä¸ªå¯æ‰©å±•çš„åŸºå‡†æµ‹è¯•å¹³å°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12983v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.12983.md)  |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline**<br><sub>æœºæ„: Sber AI<br>æ€»ä½“è€Œè¨€ï¼Œè¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªæ–°å‹ä¸¤é˜¶æ®µæ½œåœ¨æ‰©æ•£çš„æ–‡æœ¬åˆ°è§†é¢‘ç”Ÿæˆæ¶æ„ï¼Œè§£å†³äº†å…³é”®å¸§åˆæˆå’Œæ’å€¼å¸§ç”Ÿæˆä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œé€šè¿‡ä½¿ç”¨ç‹¬ç«‹çš„æ—¶åŸŸå—å’Œæœ‰æ•ˆçš„æ’å€¼æ¶æ„ï¼Œå‡å°‘äº†è®¡ç®—æˆæœ¬ï¼Œå¹¶åœ¨å¤šä¸ªè´¨é‡æŒ‡æ ‡ä¸Šå–å¾—äº†ä¼˜äºç°æœ‰æŠ€æœ¯çš„è¡¨ç°ã€‚æ­¤å¤–ï¼Œè®ºæ–‡è¿˜é’ˆå¯¹è§†é¢‘è§£ç å™¨è®¾è®¡äº†ä¸åŒçš„æ¶æ„é€‰é¡¹ï¼Œè¿›ä¸€æ­¥ä¼˜åŒ–äº†è§†é¢‘çš„ä¸€è‡´æ€§å’Œæ•´ä½“è´¨é‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13073v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.13073.md)  |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions**<br><sub>æœºæ„: Tsinghua University<br>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°é¢–çš„æ¦‚ç‡æ ‘çŠ¶æ¨ç†ï¼ˆProbTreeï¼‰æ–¹æ³•ï¼Œé€šè¿‡æ¢ç´¢LLMåœ¨å›ç­”çŸ¥è¯†å¯†é›†å‹å¤æ‚é—®é¢˜æ—¶çš„èƒ½åŠ›ï¼Œå¹¶å°†ä¸ç¡®å®šæ€§å¼•å…¥æ¨ç†è¿‡ç¨‹ï¼Œåœ¨ç»Ÿä¸€æ¡†æ¶ä¸­æ•´åˆäº†å¤–éƒ¨å’Œå‚æ•°çŸ¥è¯†ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13982v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.13982.md)  |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **Visual In-Context Prompting**<br><sub>æœºæ„: HKUST, Microsoft Research<br>æœ¬è®ºæ–‡æå‡ºäº†DINOvï¼Œä¸€ä¸ªæ–°çš„è§†è§‰ä¸Šä¸‹æ–‡å†…æç¤ºæ¡†æ¶ï¼Œèƒ½å¤Ÿæœ‰æ•ˆå¤„ç†å¤šæ ·åŒ–çš„è§†è§‰æç¤ºï¼Œä½¿ç”¨æ— æ ‡ç­¾æ•°æ®ï¼Œå¹¶åœ¨å¤šä¸ªä»»åŠ¡ä¸­è¾¾åˆ°å¾ˆå¥½çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13601v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.13601.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/UX-Decoder/DINOv)</div> |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **Enhancing Summarization Performance through Transformer-Based Prompt Engineering in Automated Medical Reporting**<br><sub>æœºæ„: Utrecht University<br>è¿™é¡¹ç ”ç©¶éªŒè¯äº†åœ¨è‡ªåŠ¨åŒ–åŒ»ç–—æŠ¥å‘Šä¸­åº”ç”¨åŸºäºè½¬æ¢å™¨çš„æç¤ºå·¥ç¨‹å¯ä»¥æé«˜æ‘˜è¦æ€§èƒ½ã€‚å°½ç®¡å­˜åœ¨ä¸€äº›å±€é™æ€§ï¼Œä½†ç ”ç©¶æå‡ºçš„æ–¹æ³•è¯æ˜äº†åœ¨æç¤ºåˆ¶å®šæ—¶åŠ å…¥ç¤ºä¾‹å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„æ•ˆç”¨ï¼Œå¹¶ä¸”æŒ‡å‡ºäº†æœªæ¥å·¥ä½œçš„æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13274v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.13274.md)  |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **XAGen: 3D Expressive Human Avatars Generation**<br><sub>æœºæ„: National University of Singapore, ByteDance<br>ç ”ç©¶æå‡ºäº†XAGenæ¨¡å‹ï¼Œå®ƒæ˜¯é¦–ä¸ªèƒ½å¤Ÿç”Ÿæˆå…¨é¢å¯æ§3Däººç±»åŒ–èº«çš„GANæ¨¡å‹ã€‚XAGenåœ¨ç»†ç²’åº¦å±æ€§æ§åˆ¶ä¸Šå…·æœ‰ç‹¬ç«‹çš„èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å¤šå°ºåº¦å’Œå¤šéƒ¨åˆ†çš„3Dè¡¨ç¤ºä¸æ¸²æŸ“æŠ€æœ¯æå‡äº†é¢éƒ¨å’Œæ‰‹éƒ¨çš„ç”Ÿæˆè´¨é‡ã€‚å®éªŒç»“æœè¯æ˜XAGenåœ¨å¤–è§‚è´¨é‡ã€æ§åˆ¶èƒ½åŠ›å’Œæ•°æ®åˆ©ç”¨ç‡æ–¹é¢éƒ½è¶…è¿‡äº†ç°æœ‰æœ€å…ˆè¿›çš„æ–¹æ³•ï¼Œæ¨è¿›äº†3Dè™šæ‹ŸåŒ–èº«ç”ŸæˆæŠ€æœ¯çš„å‘å±•ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13574v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.13574.md)  |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms**<br><sub>æœºæ„: Princeton University<br>æœ¬è®ºæ–‡çš„ä¸»è¦è´¡çŒ®åŒ…æ‹¬ï¼šåœ¨å¼€æºæ¨¡å‹ä¸Šå¾®è°ƒä¸åŒå¤§å°å’Œé£æ ¼çš„æŒ‡ä»¤æ•°æ®é›†ï¼Œè¯„ä¼°å¾®è°ƒæ¨¡å‹åœ¨ä¸åŒçš„è¯„ä¼°èŒƒå¼ä¸‹çš„è¡¨ç°ï¼Œå¹¶ä¸”å‘ç°è¾ƒå°‘çš„æ ·æœ¬ï¼ˆç‰¹åˆ«æ˜¯å½“è¿™äº›æ ·æœ¬ç»“åˆäº†ä¸åŒæ¥æºå’Œé£æ ¼æ—¶ï¼‰è¶³ä»¥åœ¨ä¸åŒç±»å‹çš„è¯„ä¼°ä¸­è·å¾—è‰¯å¥½çš„æ€§èƒ½ã€‚è¿™è¡¨æ˜åœ¨åŸ¹å…»LLMsçš„æŒ‡ä»¤éµä»èƒ½åŠ›æ—¶ï¼Œâ€œå°‘å³æ˜¯å¤šâ€ï¼Œä¸”é€šè¿‡ç²¾å¿ƒé€‰æ‹©å¾®è°ƒæ ·æœ¬ï¼Œå¯ä»¥ä½¿æ¨¡å‹åœ¨æ‰§è¡ŒæŒ‡ä»¤èƒ½åŠ›ä¸Šå¾—åˆ°æ˜¾è‘—æå‡ã€‚è¿™ä¸€å‘ç°å¯¹äºå¦‚ä½•æœ‰æ•ˆåœ°å¾®è°ƒLLMsä»¥åŠå¦‚ä½•è¯„ä¼°å®ƒä»¬çš„å®ç”¨æ€§å…·æœ‰é‡è¦æ„ä¹‰ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13133v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.13133.md)  |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **How Capable Can a Transformer Become? A Study on Synthetic, Interpretable Tasks**<br><sub>æœºæ„: University of Pennsylvania, MIT<br>æœ¬æ–‡é€šè¿‡è®¾è®¡åˆæˆæ•°æ®ç”Ÿæˆè¿‡ç¨‹å’Œç³»ç»Ÿæ€§å®éªŒï¼Œä»¥è¯„ä¼°å’Œç†è§£è‡ªå›å½’Transformeræ¨¡å‹åœ¨ç»„åˆå…¶åŸå§‹èƒ½åŠ›æ–¹é¢çš„æ½œåŠ›ã€‚ç ”ç©¶ç»“æœçªæ˜¾äº†æ¨¡å‹å­¦ä¹ ç»„åˆç»“æ„çš„èƒ½åŠ›ï¼Œæ­ç¤ºäº†è®­ç»ƒæ•°æ®å¯¹æ­¤èƒ½åŠ›çš„å½±å“ä»¥åŠæ¨¡å‹å†…éƒ¨æ³¨æ„åŠ›å±‚åœ¨ç»„åˆå­¦ä¹ è¿‡ç¨‹ä¸­çš„é‡è¦æ€§ã€‚è¿™æˆ–è®¸ä¸ºè¯„ä¼°å’Œæé«˜ç°ä»£ç¥ç»ç½‘ç»œå¯¹çœŸå®ä¸–ç•Œæ•°æ®çš„ç†è§£å’Œåº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶å¯èƒ½é¢ä¸´å‰æ‰€æœªè§çš„ä»»åŠ¡æ—¶ï¼Œæä¾›äº†æ–°çš„è§è§£ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12997v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.12997.md)  |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?**<br><sub>æœºæ„: University of Auckland<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•ä»¥è¯„ä»·å°å‹è¯­è¨€æ¨¡å‹åœ¨é—®ç­”ä»»åŠ¡ä¸­ç­”æ¡ˆçš„ç”Ÿæˆæ˜¯å¦ä¸ºè®°å¿†æˆ–æ¦‚æ‹¬èƒ½åŠ›çš„ç»“æœã€‚é€šè¿‡è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æï¼Œç¡®å®šäº†ä¸å¤ªå¯èƒ½è¢«æ¨¡å‹è®°ä½ç­”æ¡ˆçš„è¯„ä¼°æ ·æœ¬ï¼Œå¹¶ç”¨å¢åŠ é¢å¤–è®­ç»ƒæ•°æ®é›†çš„æ–¹å¼ï¼Œé’ˆå¯¹ç‰¹å®šè¯„ä¼°å­é›†è¿›è¡Œäº†æ¨¡å‹æ€§èƒ½çš„ä¼˜åŒ–ã€‚æœ€ç»ˆï¼Œç ”ç©¶ç»“æœæ˜¾ç¤ºå¢åŠ äº†æ•°æ®é›†çš„æ¨¡å‹åœ¨ç‰¹å®šè¯„ä¼°æ•°æ®é›†ä¸Šæœ‰äº†æ˜¾è‘—æå‡ï¼Œå¹¶æ¨æ–­è¿™ç§æ”¹å–„ä¸æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›æœ‰å…³ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12337v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.12337.md)  |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **AcademicGPT: Empowering Academic Research**<br><sub>æœºæ„: International Digital Economy Academy<br>AcademicGPTé’ˆå¯¹å­¦æœ¯ç ”ç©¶çš„ç‰¹å®šéœ€æ±‚è¿›è¡Œäº†ä¼˜åŒ–ï¼Œé€šè¿‡ç»“åˆé’ˆå¯¹æ€§å¼ºçš„è®­ç»ƒæ•°æ®å’Œå¤šæ–¹é¢çš„åº”ç”¨å¼€å‘ï¼Œä¸ºå­¦æœ¯é¢†åŸŸæä¾›äº†å®è´¨æ€§çš„æ”¯æŒå’Œå·¥å…·ã€‚å®ƒæ ‡å¿—ç€å¤§å‹è¯­è¨€æ¨¡å‹ä¸ªæ€§åŒ–ä¸ä¸“ä¸šåŒ–å‘å±•çš„ä¸€ä¸ªé‡è¦æ­¥éª¤ï¼Œå¹¶æœ‰æœ›å¯¹å­¦æœ¯ç¤¾åŒºäº§ç”Ÿæ·±è¿œçš„å½±å“ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12315v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.12315.md)  |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey**<br><sub>æœºæ„: Nanjing University<br>æ–‡ç« ä¸ºäº†è§£å†³LLMsåœ¨åº”å¯¹é•¿ä¸Šä¸‹æ–‡æ—¶çš„æŒ‘æˆ˜ï¼Œæå‡ºäº†ä¸€ç³»åˆ—æ–¹æ³•å’Œç»¼åˆåˆ†ç±»ä½“ç³»ï¼Œæé«˜äº†LLMsåœ¨æ³¨æ„åŠ›æœºåˆ¶ã€è®°å¿†æ•ˆç‡å’Œæœ€å¤§é•¿åº¦å¤„ç†ä¸Šçš„æ€§èƒ½ã€‚é€šè¿‡ç»¼åˆå›é¡¾å’Œåˆ†ç±»å­¦ç•Œæœ€è¿‘çš„è¿›å±•ï¼Œæœ¬æ–‡ä¸ºæœªæ¥çš„LLMsæ¶æ„è®¾è®¡å’Œä¼˜åŒ–æä¾›äº†æ¸…æ™°çš„æŒ‡å¯¼æ–¹å‘ã€‚ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12351v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.12351.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/Strivin0311/long-llms-learning)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **A Survey on Multimodal Large Language Models for Autonomous Driving**<br><sub>æœºæ„: Purdue University<br>è¯¥è®ºæ–‡å…¨é¢å›é¡¾äº†MLLMsåœ¨è‡ªåŠ¨é©¾é©¶é¢†åŸŸçš„åº”ç”¨ï¼Œè¡¨æ˜MLLMså…·å¤‡è§£æéæ–‡æœ¬æ•°æ®å’Œèåˆå¤šç§æ¨¡æ€ï¼ˆå¦‚è§†è§‰ã€è¯­è¨€ï¼‰çš„èƒ½åŠ›ï¼Œè¿™äº›èƒ½åŠ›å¯¹äºè¡Œä¸ºé¢„æµ‹å’ŒåŠ¨ä½œè§„åˆ’å°¤ä¸ºé‡è¦ã€‚é€šè¿‡åœ¨ä¸åŒçš„è‡ªåŠ¨é©¾é©¶ç¯èŠ‚ä¸­éƒ¨ç½²MLLMsï¼ˆå¦‚ç†è§£äº¤é€šåœºæ™¯ã€è§„åˆ’æ§åˆ¶ã€æ¨¡å¼ç”Ÿæˆï¼‰ï¼Œå¯ä»¥æ”¹å–„å†³ç­–æµç¨‹ï¼Œå¹¶å®ç°ç±»ä¼¼äººç±»çš„é©¾é©¶ç›´è§‰å’Œå†³ç­–æ¨¡å¼ï¼ŒåŒæ—¶æé«˜è½¦è¾†å¯¼èˆªå’Œè§„åˆ’çš„æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚æ­¤å¤–ï¼Œæ¨¡å‹é€šè¿‡ä¸ºå¤šä¸ªä»»åŠ¡çš„é¢„è®­ç»ƒæä¾›äº†ä¸€ç§æ–°çš„å¯èƒ½æ€§ï¼Œè¿™å¯èƒ½ä¼šæ¨åŠ¨æŠŠæ™ºèƒ½ç³»ç»Ÿæ¨å‘äººå·¥æ™®éæ™ºèƒ½ï¼ˆAGIï¼‰çš„å‘å±•è·¯å¾„ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12320v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.1232.md)  |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Oasis: Data Curation and Assessment System for Pretraining of Large Language Models**<br><sub>æœºæ„: Chinese Academy of Sciences<br>æœ¬æ–‡æå‡ºçš„Oasisç³»ç»Ÿæ˜¯é’ˆå¯¹å¤§å‹è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒçš„æ•°æ®æ•´ç†å’Œè¯„ä¼°é—®é¢˜çš„è§£å†³æ–¹æ¡ˆã€‚Oasisé€šè¿‡å…¶äº¤äº’å¼çš„è‡ªå®šä¹‰æ•°æ®æ•´ç†æ¨¡å—ã€é’ˆå¯¹åå·®çš„æ¨¡å‹è¿‡æ»¤å™¨å’Œå…¨é¢çš„æ•°æ®è¯„ä¼°ç³»ç»Ÿï¼Œæ—¨åœ¨æé«˜æ•°æ®é›†çš„è´¨é‡å’Œå¤šæ ·æ€§ï¼ŒåŒæ—¶é™ä½å†…å­˜éœ€æ±‚å’Œèµ„æºæ¶ˆè€—ã€‚ç³»ç»Ÿçš„å®ç°ç«‹è¶³äºæå‡æ•°æ®å¤„ç†çš„çµæ´»æ€§å’Œè¯„ä¼°çš„å‡†ç¡®æ€§ï¼Œå¡«è¡¥äº†ç°æœ‰å·¥ä½œåœ¨å…¨é¢æ€§å’Œå¤šç»´åº¦è¯„ä¼°æ–¹é¢çš„ç©ºç™½ã€‚é€šè¿‡ç»¼åˆä½¿ç”¨äººç±»è¯„ä¼°ã€å¯å‘å¼åº¦é‡å’Œæœ€æ–°çš„å¤§å‹è¯­è¨€æ¨¡å‹å¦‚GPT-4è¿›è¡Œè´¨é‡è¯„ä¼°ï¼ŒOasiså±•ç°äº†å¯¹é¢„è®­ç»ƒæ•°æ®é›†è¿›è¡Œå…¨æ–¹ä½ä¼˜åŒ–çš„èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12537v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.12537.md)  |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks**<br><sub>æœºæ„: University of Cambridge<br>æœ¬æ–‡é’ˆå¯¹å¾®è°ƒå¯¹é¢„å®šä¹‰èƒ½åŠ›çš„å½±å“å¼€å±•äº†ä¸€é¡¹å…¨é¢çš„åˆ†æå’Œè¯„ä¼°ã€‚é€šè¿‡Tracrç¼–è¯‘å¼çš„èƒ½åŠ›è®¾è®¡å’ŒåŸºäºPCFGçš„å­¦ä¹ å¼èƒ½åŠ›è®¾è®¡ï¼Œæ–‡ç« è¯¦ç»†æ¢è®¨äº†å¾®è°ƒè¿‡ç¨‹ä¸­åµŒå…¥ç‰¹å¾çš„ç›¸å…³æ€§ï¼Œæå‡ºäº†reFTæ¥å¼ºåŒ–åˆ†æå¾®è°ƒå½±å“çš„æ·±åº¦ã€‚æœ¬ç ”ç©¶çš„å‘ç°æ”¹è¿›äº†å¯¹å¾®è°ƒå½±å“æœºç†çš„ç†è§£ï¼Œå¹¶ä¸ºåç»­çš„æ¨¡å‹è®¾è®¡å’Œå¾®è°ƒç­–ç•¥æä¾›äº†å®è¯æ”¯æŒã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12786v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.12786.md)  |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Latent Lab: Large Language Models for Knowledge Exploration**<br><sub>æœºæ„: Department of Electrical Engineering and Computer Science, MIT<br>Latent Labä½œä¸ºä¸€ç§æ¢ç´¢å¤§å‹æ•°æ®é›†ä¸­ç›¸äº’è”ç³»å…³ç³»çš„åˆ›æ–°å’Œå¼ºå¤§å·¥å…·ï¼Œé€šè¿‡åˆ©ç”¨LLMså’Œè§†è§‰å¼•äººæ³¨ç›®çš„æ¥å£ï¼Œå®ƒè¶…è¶Šäº†å¸¸è§„æœç´¢çš„å±€é™æ€§ï¼Œæä¾›äº†ä¸€ä¸ªè¯­ä¹‰ä¸Šæœ‰æ„ä¹‰å’Œæƒ…å¢ƒæ„ŸçŸ¥çš„ä½“éªŒã€‚å¼ºè°ƒæ¢ç´¢çš„ä»·å€¼å’Œè¿­ä»£è®¾è®¡ï¼Œåœ¨ç›´è§‚åœ°è®¿é—®å¤§é‡ç›¸äº’è¿æ¥çš„ä¿¡æ¯æ–¹é¢å®ç°äº†ä¿¡æ¯æŠ€æœ¯ä¸“å®¶çš„é•¿æœŸè¿½æ±‚ï¼Œå¹¶é€šè¿‡AIè¾…åŠ©æ¢ç´¢å°†è¿™ä¸€æ„¿æ™¯å˜ä¸ºç°å®ï¼Œä¸ºæœªæ¥äººå·¥æ™ºèƒ½å…±åˆ›ç³»ç»Ÿçš„å‘å±•å¥ å®šäº†åŸºç¡€ï¼Œå¹¶ä¿ƒè¿›äº†æ›´ç›´è§‚å’Œé«˜æ•ˆçš„åˆä½œï¼Œæœ‰èƒ½åŠ›äº§ç”Ÿæ–°é¢–å’Œæœ‰å½±å“åŠ›çš„åˆ›é€ ç‰©ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13051v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.13051.md)  |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Prompting Frameworks for Large Language Models: A Survey**<br><sub>æœºæ„: Zhejiang University<br>è¿™é¡¹ç ”ç©¶æä¾›äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå®ƒé€šè¿‡å®ç°æ–°çš„æŠ€æœ¯æ‰‹æ®µæ¥å¢å¼ºä¸LLMsçš„äº¤äº’ï¼ŒåŒ…æ‹¬æ”¹å–„ä¸ç¼–ç¨‹è¯­è¨€çš„å…¼å®¹æ€§ï¼Œä½¿èƒ½LLMsä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼Œå¹¶ç»´æŠ¤å†å²äº¤äº’ä¿¡æ¯ï¼Œå¹¶ä»¥æ­¤æŒ‡å¯¼æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12785v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.12785.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/lxx0628/Prompting-Framework-Survey)</div> |
| <span style='display: inline-block; width: 42px;'>11-20</span> | **Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents**<br><sub>æœºæ„: Shanghai Jiao Tong University<br>æœ¬æ–‡ä½œä¸ºé¦–ç¯‡ç³»ç»Ÿæ€§æ¢è®¨CoTåŸºæ­¥æœºåˆ¶ã€èŒƒå¼è½¬å˜ï¼Œä»¥åŠCoTä¸ä»£ç†é—´å¤æ‚äº¤äº’çš„å·¥ä½œï¼Œæä¾›äº†ä¸€äº›å…³é”®è§è§£ã€‚æ–‡ç« æ­ç¤ºäº†CoTåœ¨ç‰¹å®šæ¡ä»¶ä¸‹æ˜¾ç¤ºå‡ºçš„æœ‰æ•ˆæ€§ï¼ŒæŒ‡å‡ºäº†ä½¿CoTå·¥ä½œçš„å¤šä¸ªæ¡ä»¶ï¼Œä»¥åŠç†è®ºå’Œå®è¯ç ”ç©¶ä¸ºå…¶æˆåŠŸæä¾›äº†ä½•ç§è§£é‡Šã€‚æ–‡ç« è¿˜å¯¹CoTç†è®ºè¿›è¡Œäº†æ·±å…¥åˆ†æï¼Œæå‡ºäº†CoTå¯¹äºLLMsåœ¨å¤šä¸ªé¢†åŸŸçš„ä¼˜åŒ–å’Œé©æ–°å¯èƒ½å…·æœ‰é‡è¦çš„è´¡çŒ®ï¼Œå¹¶æŒ‡å‡ºå°½ç®¡LLMsã€CoTæ¨ç†å’Œè¯­è¨€ä»£ç†å¿«é€Ÿå‘å±•ï¼Œä½†ä»å­˜åœ¨æœªè§£å†³çš„æŒ‘æˆ˜ï¼Œå¦‚å¯¹æœªè§é¢†åŸŸçš„æ³›åŒ–ã€æé«˜äº¤äº’æ•ˆç‡ã€ä»£ç†å®šåˆ¶åŒ–ã€ä»£ç†æ‰©å±•åŠä»£ç†å®‰å…¨æ€§ç­‰ã€10â€ æºã€‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11797v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.11797.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/Zoeyyao27/CoT-Igniting-Agent)</div> |
| <span style='display: inline-block; width: 42px;'>11-20</span> | **GPQA: A Graduate-Level Google-Proof Q&A Benchmark**<br><sub>æœºæ„: New York University<br>GPQA æ•°æ®é›†æä¾›äº†ä¸€ä¸ªç”¨äºæµ‹è¯• AI ç³»ç»Ÿåœ¨å¤„ç†éœ€æ·±åº¦ç†è§£å’Œæ¨ç†èƒ½åŠ›çš„å¤æ‚é—®é¢˜ä¸Šçš„èƒ½åŠ›çš„åŸºå‡†ã€‚é€šè¿‡ä¸¥æ ¼çš„é—®é¢˜è´¨é‡æ§åˆ¶å’Œä¸“å®¶çº§åˆ«çš„éš¾åº¦ï¼Œå®ƒå¯èƒ½ä¿ƒè¿›äººç±»ä¸“å®¶ä¸ AI ç³»ç»Ÿåˆä½œçš„æ–¹æ³•å‘å±•ï¼Œå¹¶æ¨åŠ¨ AI ç³»ç»Ÿè®¾è®¡çš„è¿›æ­¥ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12022v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.12022.md)  |
| <span style='display: inline-block; width: 42px;'>11-20</span> | **Continual Learning: Applications and the Road Forward**<br><sub>æœºæ„: KU Leuven<br>è®ºæ–‡ç»¼è¿°äº†å½“å‰çš„æŒç»­å­¦ä¹ ç ”ç©¶ç°çŠ¶ï¼ŒæŒ‡å‡ºäº†å…¶åœ¨è®°å¿†é™åˆ¶æ¡ä»¶ä¸‹ç ”ç©¶è¾ƒå¤šè€Œå¿½è§†è®¡ç®—æˆæœ¬çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†å››ä¸ªæœ‰å‰é€”çš„ç ”ç©¶æ–¹å‘ã€‚è¿™äº›æ–¹å‘åŒ…æ‹¬ï¼š1) çœŸå®ä¸–ç•Œæ•°æ®å¤„ç†çš„æŒ‘æˆ˜ï¼Œ2) è®¡ç®—æˆæœ¬çš„è€ƒè™‘ï¼Œä»¥åŠå…¶ä»–å¦‚ä½•è·å–æ•°æ®å’Œç†è®ºç†è§£æ–¹é¢çš„å…³æ³¨ç‚¹ã€‚è®ºæ–‡ä¸»å¼ æœªæ¥çš„CLç®—æ³•åº”åœ¨å‡å°‘å¯¹å®Œå…¨æ ‡è®°å’Œå°é—­ä¸–ç•Œå‡è®¾çš„ä¾èµ–ä¸Šåšå‡ºå®è´¨æ€§çš„è¿›å±•ï¼Œä»¥ä½¿CLæˆä¸ºè§£å†³å®é™…æœºå™¨å­¦ä¹ é—®é¢˜çš„ä¸€ä¸ªæœ‰æ•ˆå·¥å…·ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11908v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.11908.md)  |
| <span style='display: inline-block; width: 42px;'>11-20</span> | **Assessing Prompt Injection Risks in 200+ Custom GPTs**<br><sub>æœºæ„: Northwestern University<br>è¯¥è®ºæ–‡ç€é‡ç ”ç©¶äº†è‡ªå®šä¹‰GPTæ¨¡å‹ä¸­çš„å®‰å…¨é£é™©ï¼Œå°¤å…¶æ˜¯æç¤ºæ³¨å…¥æ”»å‡»ã€‚ç ”ç©¶è€…ä»¬æå‡ºäº†ä¸€ä¸ªåŒ…å«æ‰«æã€æ³¨å…¥æ•Œæ„æç¤ºå’Œæå–ç›®æ ‡ä¿¡æ¯ä¸‰ä¸ªæ­¥éª¤çš„æ”»å‡»æ–¹æ³•ï¼Œå¹¶é€šè¿‡å®æ–½è¯„ä¼°å‘ç°è‡ªå®šä¹‰GPTæ¨¡å‹å­˜åœ¨ä¸¥é‡çš„ç³»ç»Ÿæç¤ºæå–å’Œæ–‡ä»¶æ³„éœ²æ¼æ´ã€‚è¿™äº›å‘ç°çªå‡ºäº†è‡ªå®šä¹‰GPTæ¨¡å‹ä¸­çš„å…³é”®å®‰å…¨ç¼ºé™·ï¼Œå¹¶æŒ‡å‡ºäº†æå‡è¿™äº›æ¨¡å‹å®‰å…¨æ€§ç»“æ„çš„å¿…è¦æ€§ã€‚æ­¤å¤–ï¼Œçº¢é˜Ÿè¯„ä¼°æ¸…æ¥šåœ°æ˜¾ç¤ºå‡ºï¼Œç°æœ‰é˜²æŠ¤æªæ–½å¹¶ä¸è¶³å¤Ÿå¼ºå¤§ï¼Œç”šè‡³æœ‰æ—¶å€™æ˜ç¡®æŒ‡å‡ºä¸åº”è¯¥åˆ†äº«çš„ä¿¡æ¯ä¹Ÿèƒ½è¢«æå–å‡ºæ¥ï¼Œè¿™è¡¨æ˜äºŸéœ€è¿›ä¸€æ­¥åŠ å¼ºå¯¹æŠ—æç¤ºæ³¨å…¥æ”»å‡»çš„é˜²å¾¡æœºåˆ¶ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11538v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.11538.md)  |
| <span style='display: inline-block; width: 42px;'>11-19</span> | **TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems**<br><sub>æœºæ„: SenseTime Researc<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11315v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.11315.md)  |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning**<br><sub>æœºæ„: Technical University of Darmstadt, University of Cambridge<br>è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿä¸€çš„åº“â€”â€”Adaptersï¼Œå®ƒæ•´åˆå¹¶æ‰©å±•äº†å‚æ•°é«˜æ•ˆå’Œæ¨¡å—åŒ–è¿ç§»å­¦ä¹ æ–¹æ³•ï¼Œå®ç°äº†ä¸Transformersåº“çš„ç´§å¯†æ•´åˆï¼Œé€šè¿‡å¤šä¸ªNLPä»»åŠ¡çš„å¯¹æ¯”å®éªŒï¼Œå±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11077v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.11077.md)  |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **An Embodied Generalist Agent in 3D World**<br><sub>æœºæ„: Beijing Institute for General Artificial Intelligence <br>LEOæ˜¯ä¸€ä¸ªæ–°å‹çš„èº«ä½“åŒ–ã€å¤šæ¨¡æ€ã€å¤šä»»åŠ¡çš„é€šç”¨å‹æ™ºèƒ½ä½“ï¼Œä¸“æ³¨äºåœ¨3Dä¸–ç•Œä¸­çš„æ„ŸçŸ¥ã€åŸºç¡€ã€æ¨ç†ã€è§„åˆ’å’Œè¡ŒåŠ¨ã€‚é€šè¿‡å¯¹3Dè§†è§‰-è¯­è¨€å¯¹é½å’Œè§†è§‰-è¯­è¨€-åŠ¨ä½œæŒ‡ä»¤è°ƒä¼˜çš„è®­ç»ƒï¼ŒLEOèƒ½åœ¨3Dä¸–ç•Œä¸­æ‰§è¡Œä¸€ç³»åˆ—ä»»åŠ¡ã€‚æ–‡ç« é€šè¿‡ä¸€ç³»åˆ—ä¸¥æ ¼å®éªŒå’Œæ¶ˆèå®éªŒçš„ç»“æœï¼Œè¯å®äº†LEOåœ¨ä¸€ç³»åˆ—ä»»åŠ¡ä¸Šçš„é«˜æ•ˆæ€§èƒ½ï¼Œå¹¶ä¸ºæœªæ¥èº«ä½“åŒ–é€šç”¨å‹æ™ºèƒ½ä½“çš„å‘å±•æä¾›äº†å®è´µæ´è§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12871v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.12871.md)  |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability**<br><sub>æœºæ„: University of Science and Technology of China<br>æ–‡ç« é’ˆå¯¹æ¨èæ¨¡å‹è§£é‡Šæ€§çš„ç ”ç©¶æå‡ºäº†ä¸€ç§æ–°å‹çš„æ–¹æ³•ï¼Œå³é€šè¿‡å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œå¯¹é½ï¼Œä»¥æé«˜è§£é‡Šçš„è´¨é‡å’Œå‡†ç¡®æ€§ã€‚æ–‡ç« ä»‹ç»äº†ä¸‰ç§ä¸åŒçš„å¯¹é½æ–¹æ³•ï¼Œå¹¶é€šè¿‡ä¸€ç³»åˆ—ä»»åŠ¡è®­ç»ƒLLMä»¥æ¨¡ä»¿æ¨èæ¨¡å‹çš„é€»è¾‘ã€‚è®ºæ–‡é‡‡ç”¨äº†å¤šç§è¯„ä¼°ç­–ç•¥å’Œè¯„åˆ†ä½“ç³»ï¼ŒåŒ…æ‹¬ä½¿ç”¨æœ€æ–°çš„GPT-4æ¨¡å‹å’Œäººç±»è¯„åˆ†æ¥éªŒè¯æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶åœ¨ä¸‰ä¸ªä¸åŒçš„æ•°æ®é›†ä¸Šè¿›è¡Œäº†æµ‹è¯•ï¼Œæ˜¾ç¤ºå‡ºå…¶åœ¨æé«˜æ¨èæ¨¡å‹è§£é‡Šæ€§æ–¹é¢çš„æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10947v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.10947.md)  |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **Orca 2: Teaching Small Language Models How to Reason**<br><sub>æœºæ„: Microsoft Research<br>æ–‡ç« é€šè¿‡ä»‹ç»ä¸€ä¸ªæ–°çš„å°å‹è¯­è¨€æ¨¡å‹Orca 2ï¼Œå¹¶å±•ç¤ºå…¶åœ¨å¤šç§æ¨ç†ä»»åŠ¡ä¸Šèƒ½å¤Ÿä¸æ›´å¤§çš„æ¨¡å‹ç›¸åŒ¹æ•Œæˆ–è¶…è¶Šå®ƒä»¬çš„æ€§èƒ½ï¼Œå¯¹å½“å‰å°å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚æ¨ç†ä»»åŠ¡ä¸­è¡¨ç°ä¸ä½³çš„é—®é¢˜æå‡ºäº†æœ‰æ•ˆçš„è§£å†³æ–¹æ¡ˆã€‚Orca 2çš„å¼€å‘ä¾èµ–äºå¯¹è®­ç»ƒæ•°æ®å’Œè®­ç»ƒç­–ç•¥çš„ç²¾å¿ƒè®¾è®¡ï¼Œè¯æ˜äº†å³ä½¿æ˜¯å°å‹æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ”¹è¿›è®­ç»ƒæ–¹æ³•æ¥å¢å¼ºå…¶ç†è§£å’Œæ¨ç†èƒ½åŠ›ã€‚æ–‡ç« è¿˜æä¾›äº†Orca 2åœ¨å„ç§æ ‡å‡†æµ‹è¯•ä¸­çš„å“è¶Šæ€§èƒ½ç»“æœï¼ŒéªŒè¯äº†å…¶æ–¹æ³•è®ºåœ¨å®é™…åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11045v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.11045.md)  |
| <span style='display: inline-block; width: 42px;'>11-17</span> | **Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2**<br><sub>æœºæ„: Allen Institute for AI <br>TÃœLU 2é€šè¿‡é‡‡ç”¨æ–°çš„åŸºç¡€æ¨¡å‹å’Œè°ƒæ•´ç­–ç•¥ï¼Œåœ¨å¤šä¸ªæ€§èƒ½æŒ‡æ ‡ä¸Šå®ç°äº†çªç ´ï¼Œå¯¹è¿›ä¸€æ­¥ç†è§£å’Œæ”¹è¿›é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹çš„é€‚é…å…·æœ‰é‡è¦æ„ä¹‰ã€‚é€šè¿‡å¼•å…¥æ–°çš„æ•°æ®æ··åˆç‰©å’Œå…ˆè¿›çš„è®­ç»ƒæ–¹æ³•ï¼ˆå¦‚DPOï¼‰ï¼ŒTÃœLU 2æé«˜äº†æ¨¡å‹åœ¨å„ç§æ¨ç†å’ŒçŸ¥è¯†æ¢æµ‹ä»»åŠ¡ä¸Šçš„æ€§èƒ½ï¼Œå¹¶åœ¨å¼€æ”¾å¼ç”ŸæˆæŒ‡æ ‡ä¸Šå–å¾—äº†æ˜¾è‘—çš„æå‡ã€‚æ­¤å¤–ï¼Œç ”ç©¶è€…ä»¬é€šè¿‡å…¬å¼€ç›¸å…³æ¨¡å‹ã€æ•°æ®å’Œä»£ç ï¼Œæ¨åŠ¨äº†è¯­è¨€æ¨¡å‹é€‚é…æ–¹æ³•çš„å¼€æ”¾ç ”ç©¶å’Œå‘å±•ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10702v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.10702.md)  |
| <span style='display: inline-block; width: 42px;'>11-17</span> | **Exploring the Relationship between In-Context Learning and Instruction Tuning**<br><sub>æœºæ„: HKUST<br>è®ºæ–‡æä¾›äº†ICLä¸ITä¹‹é—´å¯†åˆ‡ç›¸å…³çš„å®è¯è¯æ®ï¼Œå³ä½¿ICLä¸­ä¸æ›´æ”¹æ¨¡å‹å‚æ•°ï¼ŒäºŒè€…æ‰€ä½¿ç”¨çš„æŒ‡ä»¤å’Œç¤ºä¾‹éƒ½é©±åŠ¨æ¨¡å‹æœç€æ”¶æ•›çš„éšè—çŠ¶æ€å‰è¿›ã€‚è¿™ä¸€å‘ç°å¯¹äºå¦‚ä½•è®¾è®¡é«˜æ•ˆçš„æ•°æ®é›†å’Œä»»åŠ¡ä»¥æ¨è¿›åŸºç¡€æ¨¡å‹åœ¨ä¸‹æ¸¸åº”ç”¨çš„å‘å±•å’Œå¯¹é½å…·æœ‰å¯ç¤ºä½œç”¨ã€‚ç ”ç©¶ç»“æœè¿˜å¯ä»¥å¸®åŠ©ç†è§£ç¤ºä¾‹åœ¨ICLå’ŒITä¸­çš„ä½œç”¨ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨è¿™äº›è§è§£æ¥è®¾è®¡æœ‰æ•ˆçš„ç¤ºä¾‹ä»»åŠ¡å’Œæ•°æ®é›†ï¼Œä»è€Œæå‡LLMçš„æ€§èƒ½ã€‚è®ºæ–‡ä¸­ç”³æ˜å°†ä¼šæä¾›å®éªŒä»£ç ä»¥ä¾›å¤ç°ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10367v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.10367.md)  |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **Automatic Engineering of Long Prompts**<br><sub>æœºæ„: Google<br>æœ¬æ–‡é’ˆå¯¹è¯­è¨€æ¨¡å‹é•¿æŒ‡ä»¤å·¥ç¨‹ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œæå‡ºäº†ä¸€ç§æ–°çš„ç®—æ³•æ¡†æ¶ï¼Œå¹¶è§£å†³äº†è´ªå©ªç®—æ³•æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜å’Œé—ä¼ ç®—æ³•åˆæœŸæ”¶æ•›æ…¢çš„é—®é¢˜ã€‚é€šè¿‡å¯¹æŒ‡ä»¤çš„æ¯ä¸ªå¥å­è¿›è¡Œè¯­ä¹‰ä¿æŒé‡è¿°ï¼Œå¹¶åˆ©ç”¨æ³¢æŸæœç´¢æ¥ç»´æŠ¤å’Œä¼˜åŒ–å€™é€‰æŒ‡ä»¤é›†åˆï¼Œä½¿ç®—æ³•åœ¨æœ‰é™è®­ç»ƒæ•°æ®ä¸Šè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½å’Œè¾ƒå¿«çš„æ”¶æ•›é€Ÿåº¦ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10117v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.10117.md)  |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **Crafting In-context Examples according to LMs' Parametric Knowledge**<br><sub>æœºæ„: The University of Texas at Austin<br>æœ¬æ–‡çš„é‡ç‚¹ç ”ç©¶æ˜¯å¦‚ä½•æ ¹æ®LMçš„å‚æ•°çŸ¥è¯†æœ‰æ•ˆåœ°åˆ›å»ºä¸Šä¸‹æ–‡ç¤ºä¾‹ï¼šé€‰æ‹©æœ€ä¼˜çš„ç¤ºä¾‹ï¼ˆå·²çŸ¥ä¸æœªçŸ¥çš„æ¯”è¾ƒï¼‰ä»¥åŠåœ¨ä¸Šä¸‹æ–‡ç¤ºä¾‹ä¸­å¦‚ä½•æ’åºç­”æ¡ˆã€‚å®éªŒç»“æœæ”¯æŒäº†åŠå·²çŸ¥ç¤ºä¾‹çš„æœ‰æ•ˆæ€§ä»¥åŠåŸºäºå‚æ•°çŸ¥è¯†çš„ç­”æ¡ˆæ’åºæ–¹æ³•ï¼Œè¿™äº›å‘ç°ä¸ºæé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤šç­”æ¡ˆç”Ÿæˆä»»åŠ¡ä¸­çš„æ€§èƒ½æä¾›äº†å¯è¡Œçš„æŠ€æœ¯é€”å¾„ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09579v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.09579.md)  |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **MacGyver: Are Large Language Models Creative Problem Solvers?**<br><sub>æœºæ„: University of California, Princeton University<br>æœ¬ç ”ç©¶é€šè¿‡åˆ›é€ MACGYVERæ•°æ®é›†ï¼Œæ¢ç´¢äº†LLMsåœ¨è§£å†³éä¼ ç»Ÿé—®é¢˜ä¸Šçš„èƒ½åŠ›ï¼Œå¹¶é€šè¿‡äººç±»è¯„ä¼°å‘˜å¯¹GPT-4çš„è¡¨ç°è¿›è¡Œäº†è¯„ä»·ã€‚ç ”ç©¶ç»“æœå±•ç¤ºäº†LLMsåœ¨è¿™ç±»ä»»åŠ¡ä¸Šçš„å±€é™æ€§ï¼ŒåŒæ—¶æå‡ºäº†æé«˜å…¶è¡¨ç°çš„æ–°æ–¹æ³•ã€‚ç ”ç©¶å¼ºè°ƒäº†åˆ›é€ æ€§é—®é¢˜è§£å†³èƒ½åŠ›åœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­çš„é‡è¦æ€§ï¼Œå¹¶å°è¯•é€šè¿‡LLMsè¡¥å……äººç±»çš„åˆ›é€ æ€§æ€ç»´ï¼Œä»¥æœŸæé«˜è§£å†³é—®é¢˜çš„èƒ½åŠ›å’Œæ•ˆç‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09682v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.09682.md)  |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **Predictive Minds: LLMs As Atypical Active Inference Agents**<br><sub>æœºæ„: Charles University<br>æœ¬è®ºæ–‡å°†æ´»åŠ¨æ¨æ–­çš„æ¦‚å¿µåº”ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰ï¼Œä»ä¸€ä¸ªæ–°çš„è§†è§’åˆ†æäº†LLMsçš„è¡Œä¸ºå’Œå­¦ä¹ æœºåˆ¶ã€‚è®ºæ–‡æå‡ºï¼Œå°½ç®¡LLMsåœ¨ç‰©ç†ä¸Šæ— æ³•ç›´æ¥ä¸ç¯å¢ƒäº’åŠ¨ï¼Œä½†å®ƒä»¬é€šè¿‡ç”Ÿæˆæ–‡æœ¬åœ¨è™šæ‹Ÿç¯å¢ƒä¸­çš„â€œè¡ŒåŠ¨â€é—´æ¥å½±å“ä¸–ç•Œï¼Œå¹¶æœ‰å¯èƒ½å°†è¿™äº›å½±å“åé¦ˆåˆ°æ¨¡å‹çš„è®­ç»ƒä¸­ã€‚ç ”ç©¶æŒ‡å‡ºï¼Œå¢å¼ºLLMsä¸ç”¨æˆ·äº¤äº’çš„åé¦ˆå¾ªç¯ï¼Œå°†æœ‰åŠ©äºæå‡æ¨¡å‹çš„è‡ªæˆ‘æ„è¯†ï¼Œè®©å…¶æ›´å¥½åœ°é€‚åº”å’Œå“åº”ç¯å¢ƒå˜åŒ–ï¼Œè¿™å°†å¸¦æ¥é‡å¤§çš„ç¤¾ä¼šå½±å“å’Œæ½œåœ¨çš„é£é™©ã€‚è®ºæ–‡ä¸ºç†è§£å’Œæ”¹è¿›LLMsåœ¨å®é™…éƒ¨ç½²æ—¶çš„è¡Œä¸ºæä¾›äº†é‡è¦çš„ç†è®ºåŸºç¡€ï¼Œé¢„æµ‹äº†è¿™äº›ç³»ç»Ÿæœªæ¥å¯èƒ½çš„å‘å±•æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10215v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.10215.md)  |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Memory Augmented Language Models through Mixture of Word Experts**<br><sub>æœºæ„: Google Research<br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªç§°ä¸ºMoWEçš„æ–°å‹æ¶æ„ï¼Œå®ƒé€šè¿‡èåˆç¨€ç–æ¨¡å‹çš„æ•ˆç‡å’Œå¤§å‹è¯­è¨€æ¨¡å‹çš„æ€§èƒ½ï¼Œå‡ºè‰²åœ°å¤„ç†äº†æ€§èƒ½ä¸è®¡ç®—æˆæœ¬ä¹‹é—´çš„å¹³è¡¡ã€‚é€šè¿‡é‡‡å–åˆ›æ–°çš„è®¾è®¡åŸåˆ™ï¼Œå¹¶ä¸”åœ¨NLPå¤šç§ä»»åŠ¡ä¸­éªŒè¯äº†å…¶è¶…è¶Šä¼ ç»Ÿæ¨¡å‹å¦‚T5å’ŒMoEçš„æ€§èƒ½ï¼ŒMoWEå±•ç¤ºäº†åœ¨å­¦æœ¯å’Œå®é™…åº”ç”¨é¢†åŸŸçš„æ½œåŠ›ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡æ—¶ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10768v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.10768.md)  |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Exponentially Faster Language Modelling**<br><sub>æœºæ„: ETH Zurich<br>æœ¬æ–‡ä»‹ç»äº†UltraFastBERTï¼Œè¿™æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡è¯­è¨€æ¨¡å‹çš„å˜ç§ï¼Œå®ƒæ˜¾è‘—å‡å°‘äº†åœ¨æ¨ç†æ—¶éœ€è¦ä½¿ç”¨çš„ç¥ç»å…ƒæ•°é‡ï¼Œå¹¶é€šè¿‡ä½¿ç”¨å¿«é€Ÿå‰é¦ˆç½‘ç»œæ¥æé«˜è®¡ç®—æ•ˆç‡ã€‚å°½ç®¡ä¸å…·å¤‡åŸç”Ÿçš„é«˜æ•ˆå®ç°ï¼Œä½†è¯¥æ¨¡å‹æä¾›äº†ä¸€ä¸ªèƒ½å¤Ÿæ˜¾è‘—åŠ é€Ÿæ¨ç†è¿‡ç¨‹çš„CPUä»£ç å®ç°ï¼Œå¹¶åœ¨æ ‡å‡†ä¸‹æ¸¸ä»»åŠ¡ä¸­è¡¨ç°è‰¯å¥½ã€‚è¿™ä¸€å·¥ä½œå±•ç¤ºäº†æ¡ä»¶ç¥ç»æ‰§è¡Œåœ¨è¯­è¨€å»ºæ¨¡é¢†åŸŸå·¨å¤§çš„æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10770v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.1077.md)  |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **ToolTalk: Evaluating Tool-Usage in a Conversational Setting**<br><sub>æœºæ„: Microsoft Corporation<br>ToolTalk æ˜¯ä¸€ä¸ªè‡´åŠ›äºè¯„ä¼°å’Œæé«˜ LLM åœ¨å¯¹è¯ç¯å¢ƒä¸­ä½¿ç”¨å¤šæ­¥éª¤å¤–éƒ¨å·¥å…·æ€§èƒ½çš„åŸºå‡†ã€‚å®ƒé€šè¿‡åˆ›æ–°çš„è¯„ä¼°æ–¹æ³•å’ŒçœŸå®åœºæ™¯æ¨¡æ‹Ÿï¼ŒæŒ‘æˆ˜å’Œæ‰©å±•äº†ç°æœ‰ LLMs çš„èƒ½åŠ›è¾¹ç•Œï¼Œå¹¶ä¸ºæœªæ¥çš„ç ”ç©¶æŒ‡å‡ºäº†æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10775v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.10775.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/microsoft/ToolTalk)</div> |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Contrastive Chain-of-Thought Prompting**<br><sub>æœºæ„: DAMO Academy, Alibaba Group<br>æœ¬è®ºæ–‡æå‡ºäº†å¯¹æ¯”å¼é“¾å¼æ€ç»´æ–¹æ³•ï¼Œä»¥è§£å†³ä¼ ç»Ÿé“¾å¼æ€ç»´ä¸­å­˜åœ¨çš„é—®é¢˜ï¼Œå³ç¼ºä¹å¯¹é”™è¯¯é¿å…çš„æŒ‡å¯¼ä»¥åŠå®ç°æ¨ç†æ•ˆæœçš„ä¸ç¡®å®šæ€§ã€‚é€šè¿‡æä¾›æœ‰æ•ˆå’Œæ— æ•ˆçš„æ¨ç†ç¤ºä¾‹ï¼Œæ–°æ–¹æ³•æ—¨åœ¨å¼•å¯¼æ¨¡å‹å‡å°‘æ¨ç†é”™è¯¯å¹¶ä¸€æ­¥æ­¥æ¨ç†ï¼ŒåŒæ—¶è¯¥æ–¹æ³•æä¾›äº†è‡ªåŠ¨åŒ–æ„å»ºå¯¹æ¯”ç¤ºä¾‹çš„æŠ€æœ¯ä»¥ä¾¿æ³›åŒ–åˆ°å„ç§ä»»åŠ¡ã€‚å®éªŒç»“æœè¯å®ï¼Œè¯¥æ–¹æ³•èƒ½å¤Ÿä½œä¸ºä¸€ç§é€šç”¨å¢å¼ºæ‰‹æ®µï¼Œæ˜¾è‘—æå‡é“¾å¼æ€ç»´çš„æ€§èƒ½ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09277v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.09277.md)  |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models**<br><sub>æœºæ„: Tecent AI Lab<br>è®ºæ–‡æå‡ºçš„CHAIN-OF-NOTEï¼ˆCONï¼‰æ¡†æ¶æ—¨åœ¨æé«˜RALMsçš„é²æ£’æ€§ï¼Œä¸»è¦é€šè¿‡å¼•å…¥ç»“æ„åŒ–çš„é˜…è¯»ç¬”è®°è¿‡ç¨‹æ¥æ‰¹åˆ¤æ€§åœ°è¯„ä¼°æ£€ç´¢åˆ°çš„æ–‡æ¡£ã€‚å®éªŒç»“æœè¡¨æ˜ï¼Œè¯¥æ¡†æ¶æé«˜äº†æ¨¡å‹åœ¨å™ªå£°æ•°æ®å’ŒæœªçŸ¥æƒ…å†µä¸‹çš„å¥å£®æ€§ï¼Œæ”¹å–„äº†æ•´ä½“QAæ€§èƒ½ï¼Œå¹¶åœ¨æ£€ç´¢æ–‡æ¡£å¤±è´¥è¿˜æ˜¯æˆåŠŸæ—¶å‡æé«˜äº†æ¨¡å‹çš„æ€§èƒ½ã€‚CONæ¡†æ¶é€šè¿‡ç”Ÿæˆè¯»å–ç¬”è®°å’Œæœ€ç»ˆå›ç­”ï¼Œæé«˜äº†æ¨¡å‹å¯¹å™ªå£°çš„é²æ£’æ€§ï¼Œå¹¶åœ¨ç¼ºä¹ä¿¡æ¯æ—¶èƒ½å¤Ÿç»™å‡ºâ€œæœªçŸ¥â€çš„å›ç­”ï¼Œå¢å¼ºäº†æ¨¡å‹çš„é€‚åº”æ€§å’Œå¯é æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09210v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.0921.md)  |
| <span style='display: inline-block; width: 42px;'>11-14</span> | **Instruction-Following Evaluation for Large Language Models**<br><sub>æœºæ„: Google, Yale University<br>æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹çš„æŒ‡ä»¤éµå¾ªèƒ½åŠ›çš„æ–°æ–¹æ³•â€”â€”IFEvalï¼Œå®ƒé€šè¿‡åˆæˆé€»è¾‘ä¸€è‡´çš„æŒ‡ä»¤å’Œè®¡ç®—æŒ‡ä»¤éµå¾ªå‡†ç¡®æ€§çš„æ–°å‡†åˆ™æ¥è§£å†³è¯„ä¼°è¿‡ç¨‹ä¸­çš„æŒ‘æˆ˜ã€‚æ­¤æ–¹æ³•ä¸ºè‡ªåŠ¨åŒ–ä¸”æ— åè§ï¼Œå®ƒé€šè¿‡å¤šæ­¥éª¤è¿‡ç¨‹é¿å…æŒ‡ä»¤é—´çš„æ½œåœ¨å†²çªï¼Œå¹¶å¼•å…¥äº†ä¸¥æ ¼å’Œå®½æ¾çš„å‡†ç¡®æ€§è¯„ä»·æ ‡å‡†æ¥å‡å°‘è¯¯åˆ¤ï¼ŒåŒæ—¶è®¤ä¸ºæœªæ¥å¯ä»¥é€šè¿‡å¢åŠ å¤šæ ·åŒ–å’Œä½¿ç”¨å¤šæ¨¡æ€æŒ‡ä»¤æ¥æ”¹è¿›è¯¥æ–¹æ³•ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.07911v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.07911.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/google-research/google-research/tree/master/instruction_following_eval)</div> |
| <span style='display: inline-block; width: 42px;'>11-14</span> | **Learning to Filter Context for Retrieval-Augmented Generation**<br><sub>æœºæ„: Carnegie Mellon University<br>æœ¬æ–‡æå‡ºçš„FILCOæ–¹æ³•é’ˆå¯¹å¼€æ”¾é¢†åŸŸé—®ç­”å’Œäº‹å®éªŒè¯ç­‰çŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ï¼Œé€šè¿‡æ”¹å–„æä¾›ç»™ç”Ÿæˆæ¨¡å‹çš„ä¸Šä¸‹æ–‡è´¨é‡æ¥è§£å†³ç”Ÿæˆè¾“å‡ºæ—¶é¢ä¸´çš„é—®é¢˜ã€‚é€šè¿‡ç»“åˆè¯æ±‡å’Œä¿¡æ¯è®ºæ–¹æ³•æ¥è¯†åˆ«æœ‰ç”¨ä¸Šä¸‹æ–‡ï¼Œå¹¶è®­ç»ƒæ¨¡å‹ä»¥åœ¨æµ‹è¯•æ—¶è¿‡æ»¤æ£€ç´¢ä¸Šä¸‹æ–‡ï¼Œå¾ˆå¥½åœ°è§£å†³äº†ä»¥å‰æ–¹æ³•çš„å±€é™æ€§ã€‚å®éªŒç»“æœæ˜¾ç¤ºï¼Œç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•ï¼ŒFILCOåœ¨å¤šä¸ªçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡ä¸Šéƒ½å–å¾—äº†æ˜¾è‘—çš„æ€§èƒ½æ”¹è¿›ï¼Œå¹¶ä¸”åœ¨ä¸Šä¸‹æ–‡è¿‡æ»¤è®­ç»ƒä¸Šæ˜¾ç¤ºå‡ºå…¶æœ‰æ•ˆæ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.08377v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.08377.md)  |
| <span style='display: inline-block; width: 42px;'>11-14</span> | **KTRL+F: Knowledge-Augmented In-Document Search**<br><sub>æœºæ„: KAIST AI, Samsung Research<br>æ–‡ç« æå‡ºäº†ä¸€ä¸ªæ–°çš„é—®é¢˜â€”â€”KTRL+Fï¼Œä»¥è§£å†³æ–‡çŒ®æœç´¢ä¸­çš„å®æ—¶ã€å‡†ç¡®æ€§ã€å¼•å…¥å¤–éƒ¨çŸ¥è¯†çš„éœ€æ±‚ã€‚é€šè¿‡åˆ†æç°æœ‰åŸºçº¿ï¼Œæ–‡ç« å‘ç°å®ƒä»¬å­˜åœ¨å±€é™æ€§ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šæå‡ºäº†Knowledge-Augmented Phrase Retrievalæ¨¡å‹ã€‚è¯¥æ¨¡å‹æœ‰æ•ˆåœ°åœ¨çŸ­è¯­æ£€ç´¢ä¸­æ•´åˆäº†å¤–éƒ¨çŸ¥è¯†ï¼Œé€šè¿‡ç®€å•çš„æ‰©å±•ä¿æŒäº†å¿«é€Ÿå“åº”ï¼Œæ— éœ€é¢å¤–è®­ç»ƒã€‚é€šè¿‡ç”¨æˆ·ç ”ç©¶ï¼Œè¯æ˜äº†è¯¥æ¨¡å‹èƒ½å¤Ÿæå‡ç”¨æˆ·æœç´¢ä½“éªŒï¼Œå‡å°‘æœç´¢æ—¶é—´å’Œå¤–éƒ¨ä¿¡æ¯æ£€ç´¢é‡ã€‚ä½œè€…é¼“åŠ±ç ”ç©¶ç¤¾åŒºå…³æ³¨KTRL+Fè¿™ä¸€ç‹¬ç‰¹æŒ‘æˆ˜ï¼Œæé«˜æ–‡çŒ®ä¿¡æ¯è®¿é—®çš„æ•ˆç‡å’Œæ•ˆæœã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.08329v3)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.08329.md)  |
| <span style='display: inline-block; width: 42px;'>11-13</span> | **Can LLMs Patch Security Issues?**<br><sub>æœºæ„: School of Computer Science Atlanta<br>æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„ä»£ç ä¿®æ­£æ–¹æ³•FDSSï¼Œé€šè¿‡ä¸é™æ€ä»£ç åˆ†æå·¥å…·Bandité›†æˆï¼Œèƒ½æ˜¾è‘—æé«˜LLMsè§£å†³ä»£ç ä¸­å®‰å…¨é—®é¢˜çš„èƒ½åŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00024v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2312.00024.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/Kamel773/LLM-code-refine)</div> |
| <span style='display: inline-block; width: 42px;'>11-11</span> | **In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering**<br><sub>æœºæ„: Stanford University<br>æœ¬è®ºæ–‡æå‡ºçš„ICVæ–¹æ³•ä¸ºå¤§å‹è¯­è¨€æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ æä¾›äº†ä¸€ç§æ–°é¢–ä¸”æ›´åŠ æœ‰æ•ˆçš„æ›¿ä»£æ–¹æ¡ˆã€‚é€šè¿‡å°†æ¼”ç¤ºç¤ºä¾‹çš„å…³é”®ä¿¡æ¯é›†æˆåˆ°ä¸€ä¸ªå¯ä»¥æ§åˆ¶çš„å‘é‡ä¸­ï¼ŒICVæ–¹æ³•æé«˜äº†ä»»åŠ¡æŒ‡å¯¼çš„ç²¾ç¡®åº¦å’Œæ•ˆæœï¼Œå¹¶æ˜¾è‘—ä¼˜äºç°æœ‰çš„æ–¹æ³•ã€‚å®éªŒç»“æœè¡¨æ˜ï¼ŒICVåœ¨å¤šé¡¹ä»»åŠ¡ä¸­å±•ç°äº†è¾ƒé«˜çš„æ€§èƒ½ï¼ŒåŒ…æ‹¬åœ¨ä¸åŒçš„LLMsä¸Šè¿›è¡Œè¯­è¨€æ¨¡å‹è§£æ¯’ã€é£æ ¼è½¬æ¢å’Œè§’è‰²æ‰®æ¼”ã€‚ICVæ–¹æ³•çš„è®¡ç®—å¼€é”€ä½ï¼Œå¹¶ä¸”æ˜“äºæ§åˆ¶ï¼Œæœ‰åŠ©äºæå‡è¯­è¨€æ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­çš„é€‚ç”¨æ€§å’Œå¼¹æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.06668v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.06668.md)  |
| <span style='display: inline-block; width: 42px;'>11-10</span> | **Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking**<br><sub>æœºæ„: Helvia.ai<br>è®ºæ–‡é¦–æ¬¡å°†å¤šç§åœ¨èµ„æºå—é™ç¯å¢ƒä¸‹çš„æ–¹æ³•è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼ŒåŒ…æ‹¬æˆæœ¬åˆ†æã€RAGæ–¹æ³•å’Œåˆ©ç”¨GPT-4çš„æ•°æ®å¢å¼ºï¼Œä¸ºé‡‘èè¡Œä¸šæä¾›äº†æ–°çš„æ–¹æ³•ç”¨ä»¥åº”å¯¹æ•°æ®å’Œé¢„ç®—é™åˆ¶çš„æŒ‘æˆ˜ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.06102v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.06102.md)  |
| <span style='display: inline-block; width: 42px;'>11-05</span> | **ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs**<br><sub>æœºæ„: Cornell University, Microsoft Research<br>æœ¬æ–‡æä¾›äº†ä¸€ä¸ªä½¿ç”¨å¼€æºLLMså¢å¼ºåœ¨çº¿æ•™è‚²QAå¹³å°çš„æ–°æ–¹æ¡ˆï¼Œå¹¶å¯¹å…¶è¿›è¡Œäº†å¹¿æ³›çš„è¯„ä¼°å’Œæµ‹è¯•ã€‚é€šè¿‡å°†RAGã€SFTå’ŒDPOç­‰æŠ€æœ¯ç»“åˆåº”ç”¨ï¼Œç¡®ä¿äº†å›ç­”è´¨é‡çš„æ˜¾è‘—æå‡å’Œæ•°æ®éšç§çš„ä¿æŠ¤ï¼Œå¯¹äºå¼€å‘æ™ºèƒ½QAåŠ©æ‰‹å…·æœ‰é‡è¦çš„æ„ä¹‰ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.02775v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.02775.md)  |
| <span style='display: inline-block; width: 42px;'>11-01</span> | **LLMRec: Large Language Models with Graph Augmentation for Recommendation**<br><sub>æœºæ„: University of Hong Kong, Baidu<br>LLMRecä½œä¸ºå¼€åˆ›æ€§çš„å·¥ä½œï¼Œå®ƒå¼•å…¥LLMsæ¥å¢å¼ºå›¾æ¨èç³»ç»Ÿï¼ŒæˆåŠŸåœ°è§£å†³äº†äº¤äº’æ•°æ®çš„ç¨€ç–æ€§å’Œä½è´¨é‡ä¾§ä¿¡æ¯çš„é—®é¢˜ï¼Œå¹¶é€šè¿‡å¼ºåŒ–ç”¨æˆ·-é¡¹ç›®äº¤äº’è¾¹ã€é¡¹ç›®èŠ‚ç‚¹å±æ€§ä»¥åŠç”¨æˆ·ç”»åƒç­‰æ‰‹æ®µæå‡äº†æ¨èç³»ç»Ÿçš„æ€§èƒ½ï¼Œç¡®ä¿äº†æ¨èè´¨é‡çš„åŒæ—¶é™ä½äº†æ•°æ®å™ªå£°çš„å½±å“ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.00423v5)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-11/2311.00423.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/HKUDS/LLMRec.git)</div> |

---

### 10æœˆ

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>10-20</span> | **The History and Risks of Reinforcement Learning and Human Feedback**<br><sub>æœºæ„: Berkeley<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.13595v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-10/2310.13595.md)  |
| <span style='display: inline-block; width: 42px;'>10-17</span> | **Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection**<br><sub>æœºæ„: University of Washington<br>è®ºæ–‡æ¨å‡ºäº†SELF-RAGï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„æ¡†æ¶ï¼Œé€šè¿‡æŒ‰éœ€æ£€ç´¢å’Œè‡ªæˆ‘åæ€æ¥å¢åŠ LLMsçš„è´¨é‡å’Œäº‹å®æ€§ã€‚å®ƒé€šè¿‡ç”Ÿæˆåæ€æ ‡è®°è®©LMåœ¨æ¨ç†é˜¶æ®µå˜å¾—å¯æ§ï¼Œå¯ä»¥æ»¡è¶³å¤šæ ·åŒ–çš„ä»»åŠ¡è¦æ±‚ã€‚SELF-RAGåœ¨å¤šä¸ªä»»åŠ¡ä¸Šæ˜¾è‘—è¶…è¶Šäº†ç°æœ‰LLMså’ŒRAGæ¨¡å‹ï¼Œå¹¶é€šè¿‡å®šåˆ¶çš„è§£ç ç®—æ³•å’Œåæ€æ ‡è®°ï¼Œä¸ºæ¨¡å‹è‡ªæˆ‘è¯„ä¼°å’Œå®šåˆ¶æä¾›äº†æ–°çš„æ–¹æ¡ˆã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.11511v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-10/2310.11511.md)  |
| <span style='display: inline-block; width: 42px;'>10-11</span> | **OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models**<br><sub>æœºæ„: Tsinghua University, Chinese Academy of Sciences<br>OpsEval ä½œä¸ºä¸€ä¸ªå…¨é¢çš„ AIOps ä»»åŠ¡å¯¼å‘å‹åŸºå‡†æµ‹è¯•ï¼Œä¸ä»…è¯„ä¼°äº†å¤§å‹è¯­è¨€æ¨¡å‹çš„ç»¼åˆæ€§èƒ½ã€æ¨ç†å’Œå®é™…åº”ç”¨èƒ½åŠ›ï¼Œè¿˜å¯èƒ½æ”¹å˜æœªæ¥å¤§è§„æ¨¡è´¨é‡è¯„ä¼°ä¸­ä½¿ç”¨çš„è¯„ä»·æŒ‡æ ‡ã€‚å®ƒæä¾›äº†ä¸€ä¸ªç”¨äºæŒç»­ç ”ç©¶å’Œä¼˜åŒ–AIOpsé¢†åŸŸå¤§å‹è¯­è¨€æ¨¡å‹çš„åšå®åŸºç¡€ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.07637v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-10/2310.07637.md)  |
| <span style='display: inline-block; width: 42px;'>10-10</span> | **GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models**<br><sub>æœºæ„: Microsoft Research<br>æœ¬ç ”ç©¶å±•ç¤ºäº†åœ¨å†œä¸šé¢†åŸŸä½¿ç”¨LLMsè¿›è¡Œé—®é¢˜å›ç­”çš„æ–°æ–¹æ³•ï¼Œç‰¹åˆ«æ˜¯é€šè¿‡Ensemble Refinementç­–ç•¥ï¼Œå¤§å¹…æå‡äº†LLMsåœ¨å¤šé€‰é¢˜ç›®ä¸Šçš„è¡¨ç°ï¼Œå¹¶æ˜¾ç¤ºå‡ºåœ¨å¤„ç†ä¸“ä¸šé¢†åŸŸé—®é¢˜æ—¶çš„å¹¿æ³›æ½œåŠ›ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.06225v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-10/2310.06225.md)  |

---

### 09æœˆ

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>09-04</span> | **Benchmarking Large Language Models in Retrieval-Augmented Generation**<br><sub>æœºæ„: Chinese Information Processing Laboratory <br>æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºå®é™…æ–°é—»æ–‡ç« çš„æ£€ç´¢å¢å¼ºç”ŸæˆåŸºå‡†æµ‹è¯•ï¼Œç”¨ä»¥å½»åº•è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä¿¡æ¯ç¯å¢ƒä¸­çš„å¤šé¡¹èƒ½åŠ›ï¼Œå¹¶é€šè¿‡å®éªŒç»“æœå±•ç°äº†ç°æœ‰LLMsåœ¨è¿™äº›æ–¹é¢çš„å±€é™æ€§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2309.01431v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-09/2309.01431.md)  |

---

### 08æœˆ

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>08-18</span> | **Learning Representations on Logs for AIOps**<br><sub>æœºæ„: IBM Research<br>æœ¬æ–‡æå‡ºçš„BERTOpsæ¨¡å‹é€šè¿‡ä½¿ç”¨LLMsä¸­çš„é€šç”¨è¡¨ç¤ºï¼Œå¹¶ç»“åˆä¸“é—¨é’ˆå¯¹AIOpsæ—¥å¿—æ•°æ®çš„é¢„è®­ç»ƒï¼Œæœ‰æ•ˆåœ°æé«˜äº†è‡ªåŠ¨åŒ–æ—¥å¿—åˆ†æä»»åŠ¡çš„æ€§èƒ½ï¼Œå¹¶å±•ç¤ºäº†æ˜¾è‘—çš„æ”¹è¿›ã€‚BERTOpsä¸ä»…ä¼˜äºç°æœ‰æ¨¡å‹ï¼Œåœ¨å¤šä¸ªä¸‹æ¸¸ä»»åŠ¡ä¸­ä¹Ÿè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ï¼Œæœ‰åŠ©äºåŠ é€ŸAIOpsçš„å®è·µåº”ç”¨ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2308.11526v1)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-08/2308.11526.md)  |

---

### 07æœˆ

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>07-11</span> | **Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps**<br><sub>æœºæ„: UNIVERSITY OF MARYLAND<br>æœ¬ç ”ç©¶ä½¿ç”¨å¯¹æ¯”ç¤ºä¾‹å’Œæ˜¾è‘—å›¾åˆ†ææ³•æ¥æ¢ç©¶å¤§å‹è¯­è¨€æ¨¡å‹ä¸­ä¸Šä¸‹æ–‡å­¦ä¹ çš„å†…åœ¨æœºåˆ¶ï¼Œæ­ç¤ºäº†æ ‡ç­¾ç¿»è½¬ã€è¾“å…¥å˜åŒ–ã€å’Œè¡¥å……æ€§è§£é‡Šå¯¹é¢„æµ‹çš„ä¸åŒå½±å“ï¼Œå¹¶ä¸ºå®è·µè€…æä¾›äº†å¦‚ä½•ç­–åˆ’ç¤ºä¾‹çš„æ´è§ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2307.05052v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-07/2307.05052.md) <div style='min-width:85px;'>[![GitHub](https://img.shields.io/badge/GitHub-View-brightgreen?logo=github)](https://github.com/paihengxu/XICL)</div> |

---

### 05æœˆ

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>05-24</span> | **In-Context Demonstration Selection with Cross Entropy Difference**<br><sub>æœºæ„: Microsoft Cognitive Service Research<br>æ–‡ç« æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºäº¤å‰ç†µå·®å¼‚ï¼ˆCEDï¼‰çš„ä¸Šä¸‹æ–‡ç¤ºä¾‹é€‰æ‹©æ–¹æ³•ï¼Œå¹¶æä¾›äº†ç†è®ºä¸Šçš„è§£é‡Šï¼Œå®ç°äº†å¯¹ä¸åŒå¤§å°å’Œç±»å‹çš„å¤§å‹è¯­è¨€æ¨¡å‹æ€§èƒ½çš„æå‡ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2305.14726v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-05/2305.14726.md)  |
| <span style='display: inline-block; width: 42px;'>05-23</span> | **Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning**<br><sub>æœºæ„: Lean Wang, Lei Li, Damai Dai, Deli Chen, Hao Zhou, Fandong Meng, Jie Zhou, Xu Sun<br>æœ¬è®ºæ–‡é€šè¿‡ä¿¡æ¯æµè§†è§’ç ”ç©¶äº†å¤§å‹è¯­è¨€æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆICLï¼‰çš„å†…éƒ¨æœºåˆ¶ï¼Œå‘ç°äº†æ ‡ç­¾è¯åœ¨ä¿¡æ¯æµä¸­ä½œä¸ºé”šç‚¹çš„ç°è±¡ï¼Œæå‡ºäº†æ–°å‡è®¾ï¼Œå¹¶é€šè¿‡å®éªŒéªŒè¯äº†å…¶æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œä½¿ç”¨æ‰€å¾—æ´è§æå‡ºäº†æé«˜ICLæ€§èƒ½çš„æ–¹æ³•ï¼Œä¸ºæœªæ¥ç›¸å…³ç ”ç©¶æä¾›äº†ç†è®ºåŸºç¡€å’Œå®è·µæŒ‡å¯¼ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2305.14160v2)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-05/2305.1416.md)  |
| <span style='display: inline-block; width: 42px;'>05-19</span> | **How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings**<br><sub>æœºæ„: The Ohio State University<br>æœ¬ç ”ç©¶æ­ç¤ºå‡ºæœ‰æ•ˆæç¤ºæ„é€ çš„å…³é”®æ•°æ®åº“çŸ¥è¯†å’Œæœ€ä¼˜è¡¨è¿°ï¼Œä¸ºLLMsåœ¨text-to-SQLä»»åŠ¡ä¸­çš„åº”ç”¨æä¾›æŒ‡å¯¼ï¼Œå¹¶æŒ‡å‡ºåœ¨è·¨åŸŸè®¾ç½®ä¸­å¯¹äºæç¤ºé•¿åº¦å­˜åœ¨ä¸€ä¸ªâ€œç”œèœœç‚¹â€ã€‚æœ¬ç ”ç©¶çš„å‘ç°å¯èƒ½å¯¹äºç‰¹å®šæ•°æ®åº“ä¸æ€»æ˜¯é€‚ç”¨ï¼Œç‰¹åˆ«æ˜¯å¦‚æœè¯¥æ•°æ®åº“ä¸Spideræ•°æ®åº“æ˜¾è‘—ä¸åŒã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2305.11853v3)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-05/2305.11853.md)  |

---

### 03æœˆ

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>03-31</span> | **A Survey of Large Language Models**<br><sub>æœºæ„: Renmin University of China<br>æ€»çš„æ¥è¯´ï¼Œè¿™ç¯‡ç»¼è¿°æ–‡ç« ä»‹ç»äº†LLMsé¢†åŸŸçš„æœ€æ–°è¿›å±•ï¼Œç‰¹åˆ«æ˜¯OpenAIæ¨å‡ºçš„ChatGPTå’ŒGPT-4æ¨¡å‹ï¼Œå¹¶å¼ºè°ƒäº†è¿™äº›äº§å“å¯¹äººå·¥æ™ºèƒ½ç ”ç©¶çš„é‡å¤§å½±å“ï¼Œç‰¹åˆ«æŒ‡å‡ºäº†å®ƒä»¬åœ¨äººæœºäº¤æµã€å¤šæ¨¡æ€ç†è§£å’Œç”Ÿæˆã€ä»¥åŠäººå·¥æ™ºèƒ½å¯¹é½å’Œå®‰å…¨æ€§æ–¹é¢çš„çªç ´ã€‚åŒæ—¶ï¼Œæ–‡ç« è®¤è¯†åˆ°å°½ç®¡å–å¾—äº†å·¨å¤§çš„æŠ€æœ¯è¿›å±•ï¼Œä½†åœ¨å®‰å…¨æ€§ã€ç”Ÿæˆè´¨é‡å’Œå¤šæ¨¡æ€æ€§åŠŸèƒ½æ–¹é¢ä»é¢ä¸´æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç³»åˆ—çš„æŠ€æœ¯å’Œç­–ç•¥æ¥ç¼“è§£è¿™äº›é—®é¢˜ã€‚é€šè¿‡è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°ç†è§£LLMsçš„å‘å±•æ–¹å‘ä»¥åŠå¯¹æœªæ¥äººå·¥æ™ºèƒ½åº”ç”¨å’Œç ”ç©¶çš„æ½œåœ¨å½±å“ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2303.18223v13)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-03/2303.18223.md)  |

---

### 02æœˆ

| &nbsp;Date&nbsp;&nbsp; | Paper | Links & Summary |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>02-08</span> | **A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity**<br><sub>æœºæ„: Centre for Artificial Intelligence Research<br>æ–‡ç« é€šè¿‡æ›´ç»†ç²’åº¦çš„æ–¹å¼è¯„ä¼°äº†ChatGPTçš„æ¨ç†èƒ½åŠ›ï¼Œå¹¶ä¸”æ‰¾åˆ°äº†LLMsä¸­çš„ä¸€ä¸ªå…³é”®é—®é¢˜ï¼Œå³åœ¨éæ–‡æœ¬è¯­ä¹‰ç†è§£æ–¹é¢çš„ä¸è¶³ã€‚è¿™ä¸€å‘ç°å¯¹äºæœªæ¥LLMsçš„æ”¹è¿›å’Œæ¨ç†èƒ½åŠ›çš„ç ”ç©¶æä¾›äº†é‡è¦çš„æ–¹å‘ã€‚</sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2302.04023v4)</div><div style='min-width:85px;'>[![Summary](https://img.shields.io/badge/Sum.-Read-blue?logo=dependabot)](summary/2023-02/2302.04023.md)  |

---

## åˆ†ç±»
<a name='Reasoning'></a>
### Reasoning

| &nbsp;Date&nbsp; | Paper | Links |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **MARIO: MAth Reasoning with code Interpreter Output -- A Reproducible Pipeline**<br><sub>**Institution:** Alibaba Group  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08190v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **The Benefits of a Concise Chain of Thought on Problem-Solving in Large Language Models**<br><sub>**Institution:** Johns Hopkins University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05618v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Evidence to Generate (E2G): A Single-agent Two-step Prompting for Context Grounded and Retrieval Augmented Reasoning**<br><sub>**Institution:** Qatar Computing Research Institute <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05787v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion**<br><sub>**Institution:** Tsinghua Shenzhen International Graduate School Tsinghua University, School of Computer Science Peking University, Baidu Inc.<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06072v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **The Critique of Critique**<br><sub>**Institution:** The Hong Kong Polytechnic University, Shanghai Jiao Tong University, Shanghai Artificial Intelligence Laboratory<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04518v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Know Your Needs Better: Towards Structured Understanding of Marketer Demands with Analogical Reasoning Augmented LLMs**<br><sub>**Institution:** Zhejiang University, Ant Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04319v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Chain-of-Table: Evolving Tables in the Reasoning Chain for Table Understanding**<br><sub>**Institution:** University of California San Diego, Google Cloud AI Research, Google Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04398v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-08</span> | **TTMs: Fast Multi-level Tiny Time Mixers for Improved Zero-shot and Few-shot Forecasting of Multivariate Time Series**<br><sub>**Institution:** IBM Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03955v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-07</span> | **Grimoire is All You Need for Enhancing Large Language Models**<br><sub>**Institution:** Beihang University, Renmin University of China<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03385v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-07</span> | **Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon**<br><sub>**Institution:** Beijing Academy of Artificial Intelligence, Renmin University of China, Nankai University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03462v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-06</span> | **Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing Short Text Classification**<br><sub>**Institution:** Aerospace Information Research Institute Chinese Academy of Sciences, Key Laboratory of Target Cognition and Application Technology, University of Chinese Academy of Sciences<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03158v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives**<br><sub>**Institution:** Zhejiang University, OPPO Research Institute<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02009v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **ICE-GRT: Instruction Context Enhancement by Generative Reinforcement based Transformers**<br><sub>**Institution:** Bytedance Inc.<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02072v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)**<br><sub>**Institution:** University of South Carolina, New Mexico State University, IBM Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02500v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)**<br><sub>**Institution:** University of South Carolina, New Mexico State University, IBM Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02500v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-01</span> | **From Prompt Engineering to Prompt Science With Human in the Loop**<br><sub>**Institution:** University of Washington<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04122v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-01</span> | **A & B == B & A: Triggering Logical Reasoning Failures in Large Language Models**<br><sub>**Institution:** The Chinese University of Hong Kong, Tencent AI Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00757v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs**<br><sub>**Institution:** Chinese University of Hong Kong, Tencent AI Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17080v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Improving In-context Learning via Bidirectional Alignment**<br><sub>**Institution:** Nanyang Technological University, Princeton University, Salesforce Research USA<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17055v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Challenge LLMs to Reason About Reasoning: A Benchmark to Unveil Cognitive Depth in LLMs**<br><sub>**Institution:** Chinese University of Hong Kong, Tencent AI Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17080v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos**<br><sub>**Institution:** Tsinghua University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17117v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **How Robust are LLMs to In-Context Majority Label Bias?**<br><sub>**Institution:** Amazon<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16549v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **Rethinking Tabular Data Understanding with Large Language Models**<br><sub>**Institution:** UC San Diego, USC, UC Davis  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16702v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Scaling Down, LiTting Up: Efficient Zero-Shot Listwise Reranking with Seq2seq Encoder-Decoder Models**<br><sub>**Institution:** University of Waterloo<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16098v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **KnowledgeNavigator: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph**<br><sub>**Institution:** Northeastern University, Neusoft AI Magic Technology Research, Neusoft Institute of Intelligent Medical Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15880v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Supervised Knowledge Makes Large Language Models Better In-context Learners**<br><sub>**Institution:** School of Engineering Westlake University, Westlake Institute for Advanced Study, Peking University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15918v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes**<br><sub>**Institution:** University of Michigan, Rutgers University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14890v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **On Task Performance and Model Calibration with Supervised and Self-Ensembled In-Context Learning**<br><sub>**Institution:** Language Technology Lab University of Cambridge<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13772v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction**<br><sub>**Institution:** MIT, Microsoft Research NYC<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13558v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **Active Preference Inference using Language Models and Probabilistic Reasoning**<br><sub>**Institution:** Cornell University, Cornell Tech<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12009v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows**<br><sub>**Institution:** University of Washington, Stanford University, Allen Institute for AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11681v2)</div> |
| <span style='display: inline-block; width: 42px;'>12-17</span> | **Mixed Distillation Helps Smaller Language Model Better Reasoning**<br><sub>**Institution:** Zhejiang University, Dalian Medical University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10730v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **ProCoT: Stimulating Critical Thinking and Writing of Students through Engagement with Large Language Models (LLMs)**<br><sub>**Institution:** LuleÃ¥ University of Technology Sweden<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09801v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning**<br><sub>**Institution:** National University of Singapore, University of Illinois Urbana-Champaign, Microsoft  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09039v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning**<br><sub>**Institution:** Hong Kong University of Science and Technology, Microsoft Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08901v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **Efficient Toxic Content Detection by Bootstrapping and Distilling Large Language Models**<br><sub>**Institution:** University of Southern California, Amazon.com Inc.<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08303v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **Comparable Demonstrations are Important in In-Context Learning: A Novel Perspective on Demonstration Selection**<br><sub>**Institution:** Shanghai Jiao Tong University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07476v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **MMICT: Boosting Multi-Modal Fine-Tuning with In-Context Examples**<br><sub>**Institution:** Xiamen University, Tencent YouTu Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06363v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **"What's important here?": Opportunities and Challenges of Using LLMs in Retrieving Information from Web Interfaces**<br><sub>**Institution:** Carnegie Mellon University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06147v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **On Meta-Prompting**<br><sub>**Institution:** Microsoft  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06562v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **A Study on the Calibration of In-context Learning**<br><sub>**Institution:** Harvard University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04021v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Cost-Effective In-Context Learning for Entity Resolution: A Design Space Exploration**<br><sub>**Institution:** Renmin University of China, Beijing Institute of Technology, HKUST (GZ)<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03987v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Prompt Optimization via Adversarial In-Context Learning**<br><sub>**Institution:** National University of Singapore, Hong Kong University of Science and Technology, Institute for Infocomm Research (I2R) A*STAR<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02614v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation**<br><sub>**Institution:** Sea AI Lab, Sun Yat-sen University, Harvard University  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02439v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Retrieval-augmented Multi-modal Chain-of-Thoughts Reasoning for Large Language Models**<br><sub>**Institution:** Xiamen University, MBZUAI, Tencent AI Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01714v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Exchange-of-Thought: Enhancing Large Language Model Capabilities through Cross-Model Communication**<br><sub>**Institution:** Fudan University, National University of Singapore, Shanghai AI Laboratory  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01823v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **On the Effectiveness of Large Language Models in Domain-Specific Code Generation**<br><sub>**Institution:** Shanghai Jiao Tong University, Chongqing University, East China Normal University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01639v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **The Unlocking Spell on Base LLMs: Rethinking Alignment via In-Context Learning**<br><sub>**Institution:** Allen Institute for Artificial Intelligence, University of Washington<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01552v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-02</span> | **Exploring and Improving the Spatial Reasoning Abilities of Large Language Models**<br><sub>**Institution:** Stanford University  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01054v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **On Exploring the Reasoning Capability of Large Language Models with Knowledge Graphs**<br><sub>**Institution:** Singapore Management University, National Sun Yat-sen University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00353v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions**<br><sub>**Institution:** Huawei Poisson Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18397v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **Applying Large Language Models and Chain-of-Thought for Automatic Scoring**<br><sub>**Institution:** University of Georgia<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03748v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Zero-shot Conversational Summarization Evaluations with small Large Language Models**<br><sub>**Institution:** Intel labs<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18041v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Understanding and Improving In-Context Learning on Vision-language Models**<br><sub>**Institution:** LMU Munich, University of Oxford<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18021v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **Probabilistic Tree-of-thought Reasoning for Answering Knowledge-intensive Complex Questions**<br><sub>**Institution:** Tsinghua University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13982v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **Enhancing Summarization Performance through Transformer-Based Prompt Engineering in Automated Medical Reporting**<br><sub>**Institution:** Utrecht University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13274v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **Visual In-Context Prompting**<br><sub>**Institution:** HKUST, Microsoft Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13601v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-20</span> | **Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents**<br><sub>**Institution:** Shanghai Jiao Tong University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11797v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-19</span> | **TPTU-v2: Boosting Task Planning and Tool Usage of Large Language Model-based Agents in Real-world Systems**<br><sub>**Institution:** SenseTime Researc<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11315v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **Orca 2: Teaching Small Language Models How to Reason**<br><sub>**Institution:** Microsoft Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11045v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-17</span> | **Exploring the Relationship between In-Context Learning and Instruction Tuning**<br><sub>**Institution:** HKUST<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10367v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **Crafting In-context Examples according to LMs' Parametric Knowledge**<br><sub>**Institution:** The University of Texas at Austin<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09579v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **Automatic Engineering of Long Prompts**<br><sub>**Institution:** Google<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10117v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Contrastive Chain-of-Thought Prompting**<br><sub>**Institution:** DAMO Academy, Alibaba Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09277v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Chain-of-Note: Enhancing Robustness in Retrieval-Augmented Language Models**<br><sub>**Institution:** Tecent AI Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09210v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-11</span> | **In-context Vectors: Making In Context Learning More Effective and Controllable Through Latent Space Steering**<br><sub>**Institution:** Stanford University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.06668v2)</div> |
| <span style='display: inline-block; width: 42px;'>10-31</span> | **Learning to Reason and Memorize with Self-Notes**<br><sub>**Institution:** Meta AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.00833.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-19</span> | **AutoMix: Automatically Mixing Language Models**<br><sub>**Institution:** Carnegie Mellon University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.12963.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-12</span> | **Re-Reading Improves Reasoning in Language Models**<br><sub>**Institution:** Institute of Information Engineering, CAS<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2309.06275.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>07-11</span> | **Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps**<br><sub>**Institution:** UNIVERSITY OF MARYLAND<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2307.05052v2)</div> |
| <span style='display: inline-block; width: 42px;'>05-26</span> | **Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models**<br><sub>**Institution:** Singapore Management University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.04091.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-26</span> | **Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models**<br><sub>**Institution:** Shanghai Jiao Tong University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.16582.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-26</span> | **MultiTool-CoT: GPT-3 Can Use Multiple External Tools with Chain of Thought Prompting**<br><sub>**Institution:** Kyoto University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.16896.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-23</span> | **Improving Factuality and Reasoning in Language Models through Multiagent Debate**<br><sub>**Institution:** MIT<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.14325.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-23</span> | **ChatCoT: Tool-Augmented Chain-of-Thought Reasoning on Chat-based Large Language Models**<br><sub>**Institution:** Gaoling School of Artificial Intelligence, Renmin University of China<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.14323.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-22</span> | **LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities**<br><sub>**Institution:** Zhejiang University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.13168.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-19</span> | **RCOT: Detecting and Rectifying Factual Inconsistency in Reasoning by Reversing Chain-of-Thought.**<br><sub>**Institution:** Nanjing University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.11499.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-19</span> | **How to Prompt LLMs for Text-to-SQL: A Study in Zero-shot, Single-domain, and Cross-domain Settings**<br><sub>**Institution:** The Ohio State University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2305.11853v3)</div> |
| <span style='display: inline-block; width: 42px;'>05-17</span> | **Tree of Thoughts: Deliberate Problem Solving with Large Language Models**<br><sub>**Institution:** Princeton University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.10601.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-10</span> | **ReAct: Synergizing Reasoning and Acting in Language Models**<br><sub>**Institution:** Princeton University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2210.03629.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-05</span> | **Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework**<br><sub>**Institution:** Nanyang Technological University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.03268.pdf)</div> |
<a name='Agent'></a>
### Agent

| &nbsp;Date&nbsp; | Paper | Links |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>01-14</span> | **Small LLMs Are Weak Tool Learners: A Multi-LLM Agent**<br><sub>**Institution:** Sun Yat-sen University, Alibaba Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07324v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction**<br><sub>**Institution:** Fudan University, Microsoft Research Asia, Zhejiang University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06201v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Bootstrapping LLM-based Task-Oriented Dialogue Agents via Self-Talk**<br><sub>**Institution:** AWS AI Labs<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05033v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **AUTOACT: Automatic Agent Learning from Scratch via Self-Planning**<br><sub>**Institution:** Zhejiang University, Alibaba Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05268v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Agent Alignment in Evolving Social Norms**<br><sub>**Institution:** Fudan University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04620v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-08</span> | **SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems**<br><sub>**Institution:** Fudan University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03945v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-07</span> | **Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects**<br><sub>**Institution:** The Chinese University of Hong Kong, DeepWisdom, Peking University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03428v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-06</span> | **CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models**<br><sub>**Institution:** Harbin Institute of Technology, Kuaishou Technology<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08438v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-05</span> | **From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models**<br><sub>**Institution:** Beike Inc.<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02777v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **GitAgent: Facilitating Autonomous Agent with GitHub by Tool Extension**<br><sub>**Institution:** Tsinghua University, Renmin University of China<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17294v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Experiential Co-Learning of Software-Developing Agents**<br><sub>**Institution:** Tsinghua University,Dalian University of Technology,Beijing University of Posts and Telecommunications<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17025v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Pangu-Agent: A Fine-Tunable Generalist Agent with Structured Reasoning**<br><sub>**Institution:** Huawei Noah's Ark Lab, University College London, University of Oxford<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14878v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **De novo Drug Design using Reinforcement Learning with Multiple GPT Agents**<br><sub>**Institution:** Tsinghua University, Microsoft Research AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06155v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-21</span> | **AppAgent: Multimodal Agents as Smartphone Users**<br><sub>**Institution:** Tencent  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13771v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation**<br><sub>**Institution:** The University of Hong Kong, Shanghai Jiao Tong University, Kingâ€™s College London<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13010v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **AgentCoder: Multi-Agent-based Code Generation with Iterative Testing and Optimisation**<br><sub>**Institution:** The University of Hong Kong, Shanghai Jiao Tong University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13010v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Agent-based Learning of Materials Datasets from Scientific Literature**<br><sub>**Institution:** University of Toronto  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11690v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Social Learning: Towards Collaborative Learning with Large Language Models**<br><sub>**Institution:** Google, EPFL<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11441v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **ReST meets ReAct: Self-Improvement for Multi-Step Reasoning LLM Agent**<br><sub>**Institution:** Google<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10003v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent**<br><sub>**Institution:** Shanghai Jiao Tong University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08926v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-08</span> | **PaperQA: Retrieval-Augmented Generative Agent for Scientific Research**<br><sub>**Institution:** RAND Corporation, Carnegie Mellon University, LangChain<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07559v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **An LLM Compiler for Parallel Function Calling**<br><sub>**Institution:** UC Berkeley, ICSI, LBNL<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04511v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia**<br><sub>**Institution:** Google DeepMind, Google Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03664v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction**<br><sub>**Institution:** Zhejiang Lab, Ant Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03022v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **Autonomous Agents in Software Development: A Vision Paper**<br><sub>**Institution:** Tampere University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18440v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **TaskWeaver: A Code-First Agent Framework**<br><sub>**Institution:** Microsoft<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17541v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering**<br><sub>**Institution:** Sun Yat-Sen University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17331v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **AvatarGPT: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16468v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-27</span> | **RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks**<br><sub>**Institution:** Chinese Academy of Sciences, Peking University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.15649v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **Controlling Large Language Model-based Agents for Large-Scale Decision-Making: An Actor-Critic Approach**<br><sub>**Institution:** Chinese Academy of Sciences<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13884v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **An Embodied Generalist Agent in 3D World**<br><sub>**Institution:** Beijing Institute for General Artificial Intelligence <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12871v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **Predictive Minds: LLMs As Atypical Active Inference Agents**<br><sub>**Institution:** Charles University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10215v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-14</span> | **KTRL+F: Knowledge-Augmented In-Document Search**<br><sub>**Institution:** KAIST AI, Samsung Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.08329v3)</div> |
| <span style='display: inline-block; width: 42px;'>11-06</span> | **MetaGPT: Meta Programming for Multi-Agent Collaborative Framework**<br><sub>**Institution:** DeepWisdom, King Abdullah University of Science and Technology<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2308.00352.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>10-16</span> | **OpenAgents: An Open Platform for Language Agents in the Wild**<br><sub>**Institution:** The University of Hong Kong, XLang Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.10634.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>10-16</span> | **Theory of Mind for Multi-Agent Collaboration via Large Language Models**<br><sub>**Institution:** University of Pittsburgh<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.10701.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-29</span> | **AutoAgents: A Framework for Automatic Agent Generation**<br><sub>**Institution:** Peking University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2309.17288.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-29</span> | **ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving**<br><sub>**Institution:** Tsinghua University, Microsoft<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2309.17452.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-14</span> | **Agents: An Open-source Framework for Autonomous Language Agents**<br><sub>**Institution:** AIWaves Inc.<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2309.07870.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>08-21</span> | **AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors**<br><sub>**Institution:** Tsinghua University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2308.10848.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>08-21</span> | **GPT-in-the-Loop: Adaptive Decision-Making for Multiagent Systems**<br><sub>**Institution:** University of Waterloo<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2308.10435.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>08-16</span> | **AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation**<br><sub>**Institution:** Microsoft Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2308.08155.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>07-25</span> | **WebArena: A Realistic Web Environment for Building Autonomous Agents**<br><sub>**Institution:** Carnegie Mellon University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2307.13854.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>07-24</span> | **A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis**<br><sub>**Institution:** Google DeepMind<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2307.12856.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>07-16</span> | **Communicative Agents for Software Development**<br><sub>**Institution:** Tsinghua University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2307.07924.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>07-14</span> | **Language models show human-like content effects on reasoning tasks**<br><sub>**Institution:** Google DeepMind<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2207.07051.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>07-10</span> | **RoCo: Dialectic Multi-Robot Collaboration with Large Language Models**<br><sub>**Institution:** Columbia University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2307.04738.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>06-13</span> | **Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control**<br><sub>**Institution:** Nanyang Technological University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2306.07863.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-23</span> | **Improving Factuality and Reasoning in Language Models through Multiagent Debate**<br><sub>**Institution:** MIT<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.14325.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-21</span> | **Augmenting Autotelic Agents with Large Language Models**<br><sub>**Institution:** MIT<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.12487.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>03-31</span> | **CAMEL: Communicative Agents for Mind Exploration of Large Language Model Society**<br><sub>**Institution:** King Abdullah University of Science and Technology<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2303.17760.pdf)</div> |
<a name='Knowledge-and-Retrieval'></a>
### Knowledge and Retrieval

| &nbsp;Date&nbsp; | Paper | Links |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>01-17</span> | **LLMs for Relational Reasoning: How Far are We?**<br><sub>**Institution:** Continental-NTU Corporate Lab, Nanyang Technological University, Singapore<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.09042v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture**<br><sub>**Institution:** Microsoft<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08406v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **Salute the Classic: Revisiting Challenges of Machine Translation in the Age of Large Language Models**<br><sub>**Institution:** Tencent AI Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08350v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-15</span> | **A Study on Large Language Models' Limitations in Multiple-Choice Question Answering**<br><sub>**Institution:** David R. Cheriton School of Computer Science<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07955v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs**<br><sub>**Institution:** Virginia Tech, Renmin University of China, UC Davis<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06373v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation**<br><sub>**Institution:** Tianyu Zheng, Shuyue Guo, Xingwei Qu, Jiawei Guo, Weixu Zhang, Xinrun Du, Chenghua Lin, Wenhao Huang, Wenhu Chen, Jie Fu, Ge Zhang<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06477v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **TOFU: A Task of Fictitious Unlearning for LLMs**<br><sub>**Institution:** Carnegie Mellon University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06121v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase**<br><sub>**Institution:** LAIR Lab Lehigh University, Huazhong University of Science and Technology  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05952v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **CASA: Causality-driven Argument Sufficiency Assessment**<br><sub>**Institution:** Peking University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05249v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Attendre: Wait To Attend By Retrieval With Evicted Queries in Memory-Based Transformers for Long Context Processing**<br><sub>**Institution:** Google Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04881v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05507v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search**<br><sub>**Institution:** Nanyang Technological University Singapore<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04514v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval**<br><sub>**Institution:** Columbia University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02369v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-02</span> | **LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01325v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-01</span> | **The Earth is Flat? Unveiling Factual Errors in Large Language Models**<br><sub>**Institution:** The Chinese University of Hong Kong, Tencent AI Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00761v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-31</span> | **Improving Text Embeddings with Large Language Models**<br><sub>**Institution:** Microsoft Corporation<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00368v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-31</span> | **BatchEval: Towards Human-like Text Evaluation**<br><sub>**Institution:** Beijing Institute of Technology, Xiaohongshu Inc  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.00437v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **Enhancing Quantitative Reasoning Skills of Large Language Models through Dimension Perception**<br><sub>**Institution:** **Institution:** Shanghai Key Laboratory of Data Science School of Computer Science Fudan University, School of Data Science Fudan University, DataGrand Co. LTD<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17532v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-28</span> | **Structured Packing in LLM Training Improves Long Context Utilization**<br><sub>**Institution:** University of Warsaw, Google DeepMind, Polish Academy of Sciences<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17296v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Think and Retrieval: A Hypothesis Knowledge Graph Enhanced Medical Large Language Models**<br><sub>**Institution:** Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education; School of Computer Science Peking University, Beijing China<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15883v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-25</span> | **ESGReveal: An LLM-based approach for extracting structured data from ESG reports**<br><sub>**Institution:** Alibaba Cloud, Tsinghua University, Sun Yat-Sen University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17264v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **VIEScore: Towards Explainable Metrics for Conditional Image Synthesis Evaluation**<br><sub>**Institution:** University of Waterloo, IN.AI Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14867v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **A Revisit of Fake News Dataset with Augmented Fact-checking by ChatGPT**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11870v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes**<br><sub>**Institution:** University of Cambridge<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12112v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **NoMIRACL: Knowing When You Don't Know for Robust Multilingual Retrieval-Augmented Generation**<br><sub>**Institution:** University of Waterloo, Huawei Noahâ€™s Ark Lab, FEEC-Unicamp Brazil<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11361v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **G-LLaVA: Solving Geometric Problem with Multi-Modal Large Language Model**<br><sub>**Institution:** Huawei Noah's Ark Lab, The University of Hong Kong, The Hong Kong University of Science and Technology<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11370v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **"Paraphrasing The Original Text" Makes High Accuracy Long-Context QA**<br><sub>**Institution:** Tsinghua University  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11193v2)</div> |
| <span style='display: inline-block; width: 42px;'>12-17</span> | **Distinguishing Translations by Human, NMT, and ChatGPT: A Linguistic and Statistical Approach**<br><sub>**Institution:** Shanghai Jiao Tong University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10750v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **RIGHT: Retrieval-augmented Generation for Mainstream Hashtag Recommendation**<br><sub>**Institution:** CAS Key Lab of Network Data Science and Technology ICT CAS, University of Chinese Academy of Sciences Beijing China<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10466v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **ProTIP: Progressive Tool Retrieval Improves Planning**<br><sub>**Institution:** Apple  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10332v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **CoAScore: Chain-of-Aspects Prompting for NLG Evaluation**<br><sub>**Institution:** GSAI Renmin University of China<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10355v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **RecPrompt: A Prompt Tuning Framework for News Recommendation Using Large Language Models**<br><sub>**Institution:** Science Foundation Ireland (SFI), JSPS KAKENHI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10463v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **Challenges with unsupervised LLM knowledge discovery**<br><sub>**Institution:** Google DeepMind, Google Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10029v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **No-Skim: Towards Efficiency Robustness Evaluation on Skimming-based Language Models**<br><sub>**Institution:** Fudan University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09494v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **KGLens: A Parameterized Knowledge Graph Solution to Assess What an LLM Does and Doesn't Know**<br><sub>**Institution:** Apple<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11539v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **Generative Context-aware Fine-tuning of Self-supervised Speech Models**<br><sub>**Institution:** ASAPP, Carnegie Mellon University, Toyota Technological Institute at Chicago<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09895v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **Faithful Persona-based Conversational Dataset Generation with Large Language Models**<br><sub>**Institution:** University of Southern California, Google, Information Sciences Institute<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10007v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Entity-Augmented Code Generation**<br><sub>**Institution:** JetBrains<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08976v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Self-Evaluation Improves Selective Generation in Large Language Models**<br><sub>**Institution:** Google DeepMind, Google Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09300v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Math-Shepherd: A Label-Free Step-by-Step Verifier for LLMs in Mathematical Reasoning**<br><sub>**Institution:** Peking University, DeepSeek-AI, The University of Hong Kong<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08935v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Towards Verifiable Text Generation with Evolving Memory and Self-Reflection**<br><sub>**Institution:** Peking University, Chinese Academy of Sciences, Baidu Inc<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09075v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **TinyGSM: achieving >80% on GSM8k with small language models**<br><sub>**Institution:** Carnegie Mellon University, Microsoft Research  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09241v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **LLMEval: A Preliminary Study on How to Evaluate Large Language Models**<br><sub>**Institution:** Fudan University, Shanghai Jiaotong University  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07398v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **diff History for Long-Context Language Agents**<br><sub>**Institution:** New York University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07540v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Dense X Retrieval: What Retrieval Granularity Should We Use?**<br><sub>**Institution:** University of Washington, Tencent AI Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06648v2)</div> |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Honeybee: Locality-enhanced Projector for Multimodal LLM**<br><sub>**Institution:** Kakao Brain<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06742v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-10</span> | **Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs**<br><sub>**Institution:** Microsoft Israel<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05934v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-08</span> | **Using Program Knowledge Graph to Uncover Software Vulnerabilities**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04818v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **CLadder: A Benchmark to Assess Causal Reasoning Capabilities of Language Models**<br><sub>**Institution:** MPI for Intelligent Systems, University of Washington<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04350v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **A Hardware Evaluation Framework for Large Language Model Inference**<br><sub>**Institution:** Princeton University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03134v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Competition-Level Problems are Effective LLM Evaluators**<br><sub>**Institution:** Microsoft Research Asia, Xiamen University, Microsoft Azure AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02143v2)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **ChatGPT as a Math Questioner? Evaluating ChatGPT on Generating Pre-university Math Questions**<br><sub>**Institution:** Nanyang Technological University, National University of Singapore<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01661v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-03</span> | **Running cognitive evaluations on large language models: The do's and the don'ts**<br><sub>**Institution:** Massachusetts Institute of Technology<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01276v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-03</span> | **TextGenSHAP: Scalable Post-hoc Explanations in Text Generation with Long Documents**<br><sub>**Institution:** University of Southern California, Google Cloud AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01279v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-03</span> | **D-Bot: Database Diagnosis System using Large Language Models**<br><sub>**Institution:** Tsinghua University, Pigsty, ModelBest<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01454v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models**<br><sub>**Institution:** University of Wisconsin - Madison<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00960v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models**<br><sub>**Institution:** University of Wisconsin - Madison<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00960v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Deciphering Digital Detectives: Understanding LLM Behaviors and Capabilities in Multi-Agent Mystery Games**<br><sub>**Institution:** Quebec AI Institute<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00746v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **TaskBench: Benchmarking Large Language Models for Task Automation**<br><sub>**Institution:** Zhejiang University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18760v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **What Do Llamas Really Think? Revealing Preference Biases in Language Model Representations**<br><sub>**Institution:** Comcast Applied AI, University of Waterloo<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18812v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Are Large Language Models Good Fact Checkers: A Preliminary Study**<br><sub>**Institution:** Chinese Academy of Sciences<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17355v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models**<br><sub>**Institution:** Harbin Institute of Technology<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17667v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-26</span> | **UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation**<br><sub>**Institution:** Renmin University of Chin<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2311.15296.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Do Smaller Language Models Answer Contextualised Questions Through Memorisation Or Generalisation?**<br><sub>**Institution:** University of Auckland<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12337v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Oasis: Data Curation and Assessment System for Pretraining of Large Language Models**<br><sub>**Institution:** Chinese Academy of Sciences<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12537v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **How Capable Can a Transformer Become? A Study on Synthetic, Interpretable Tasks**<br><sub>**Institution:** University of Pennsylvania, MIT<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12997v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-20</span> | **GPQA: A Graduate-Level Google-Proof Q&A Benchmark**<br><sub>**Institution:** New York University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12022v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-20</span> | **Continual Learning: Applications and the Road Forward**<br><sub>**Institution:** KU Leuven<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11908v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-16</span> | **MacGyver: Are Large Language Models Creative Problem Solvers?**<br><sub>**Institution:** University of California, Princeton University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.09682v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **ToolTalk: Evaluating Tool-Usage in a Conversational Setting**<br><sub>**Institution:** Microsoft Corporation<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10775v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-14</span> | **Instruction-Following Evaluation for Large Language Models**<br><sub>**Institution:** Google, Yale University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.07911v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-10</span> | **Making LLMs Worth Every Penny: Resource-Limited Text Classification in Banking**<br><sub>**Institution:** Helvia.ai<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.06102v1)</div> |
| <span style='display: inline-block; width: 42px;'>10-17</span> | **Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection**<br><sub>**Institution:** University of Washington<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.11511v1)</div> |
| <span style='display: inline-block; width: 42px;'>10-11</span> | **OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models**<br><sub>**Institution:** Tsinghua University, Chinese Academy of Sciences<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.07637v2)</div> |
| <span style='display: inline-block; width: 42px;'>10-10</span> | **A New Benchmark and Reverse Validation Method for Passage-level Hallucination Detection**<br><sub>**Institution:** Peking University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.06498.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>10-10</span> | **The Geometry of Truth: Emergent Linear Structure in Large Language Model Representations of True/False Datasets**<br><sub>**Institution:** Northeastern University, MIT<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.06824.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-26</span> | **RAGAS: Automated Evaluation of Retrieval Augmented Generation**<br><sub>**Institution:** Cardiff University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2309.15217.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-04</span> | **Benchmarking Large Language Models in Retrieval-Augmented Generation**<br><sub>**Institution:** Chinese Information Processing Laboratory <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2309.01431v1)</div> |
| <span style='display: inline-block; width: 42px;'>06-15</span> | **KoLA: Carefully Benchmarking World Knowledge of Large Language Models**<br><sub>**Institution:** Tsinghua University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2306.09296.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>06-07</span> | **Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering**<br><sub>**Institution:** KAIST, MBZUAI, Amazon<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2306.04136.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-29</span> | **G-EVAL: NLG Evaluation using GPT-4 with Better Human Alignment**<br><sub>**Institution:** Microsoft Cognitive Services Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2303.16634.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-24</span> | **In-Context Demonstration Selection with Cross Entropy Difference**<br><sub>**Institution:** Microsoft Cognitive Service Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2305.14726v2)</div> |
| <span style='display: inline-block; width: 42px;'>05-16</span> | **StructGPT: A General Framework for Large Language Model to Reason over Structured Data**<br><sub>**Institution:** Gaoling School of Artificial Intelligence, Renmin University of China.<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.09645.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>02-08</span> | **A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity**<br><sub>**Institution:** Centre for Artificial Intelligence Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2302.04023v4)</div> |
<a name='Alignment-and-Hallucination'></a>
### Alignment and Hallucination

| &nbsp;Date&nbsp; | Paper | Links |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models**<br><sub>**Institution:** Google Research, Tel Aviv University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06102v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-06</span> | **The Dawn After the Dark: An Empirical Study on Factuality Hallucination in Large Language Models**<br><sub>**Institution:** Renmin University of China, UniversitÃ© de MontrÃ©al<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.03205v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **Aligning Large Language Models with Human Preferences through Representation Engineering**<br><sub>**Institution:** Fudan University  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15997v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-25</span> | **Alleviating Hallucinations of Large Language Models through Induced Hallucinations**<br><sub>**Institution:** Soochow University, Tencent AI Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.15710v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Large Language Model (LLM) Bias Index -- LLMBI**<br><sub>**Institution:** University of Oxford, University Canada West, Amazon Web Services (AWS)<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14769v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Reasons to Reject? Aligning Language Models with Judgments**<br><sub>**Institution:** Tencent AI Lab, The Chinese University of Hong Kong<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14591v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **WEAK-TO-STRONG GENERALIZATION: ELICITING STRONG CAPABILITIES WITH WEAK SUPERVISION**<br><sub>**Institution:** OpenAI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://cdn.openai.com/papers/weak-to-strong-generalization.pdf)</div><div style='min-width:85px;'>[![Blog](https://img.shields.io/badge/Blog-Posts-yellow?logo=rss)](https://mp.weixin.qq.com/s/f6YW-CxnLhnfMWTLg4M4Cw)</div> |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Unlocking Anticipatory Text Generation: A Constrained Approach for Faithful Decoding with Large Language Models**<br><sub>**Institution:** Salesforce AI Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06149v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Context Tuning for Retrieval Augmented Generation**<br><sub>**Institution:** Apple  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05708v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-02</span> | **Axiomatic Preference Modeling for Longform Question Answering**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02206v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Instruction-tuning Aligns LLMs to the Human Brain**<br><sub>**Institution:** EPFL<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00575v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Nash Learning from Human Feedback**<br><sub>**Institution:** Google DeepMind<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00886v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **RELIC: Investigating Large Language Model Responses using Self-Consistency**<br><sub>**Institution:** ETH Zurich<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16842v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization**<br><sub>**Institution:** Shanghai AI Laboratory<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16839v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-24</span> | **Data-Efficient Alignment of Large Language Models with Human Feedback Through Natural Language**<br><sub>**Institution:** Amazon<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.14543v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-24</span> | **Calibrated Language Models Must Hallucinate**<br><sub>**Institution:** Microsoft Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.14648v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **ZipLoRA: Any Subject in Any Style by Effectively Merging LoRAs**<br><sub>**Institution:** Google Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13600v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **RecExplainer: Aligning Large Language Models for Recommendation Model Interpretability**<br><sub>**Institution:** University of Science and Technology of China<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10947v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-14</span> | **Learning to Filter Context for Retrieval-Augmented Generation**<br><sub>**Institution:** Carnegie Mellon University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.08377v1)</div> |
| <span style='display: inline-block; width: 42px;'>10-24</span> | **Correction with Backtracking Reduces Hallucination in Summarization**<br><sub>**Institution:** Google DeepMind, Cornell University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.16176.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>10-20</span> | **The History and Risks of Reinforcement Learning and Human Feedback**<br><sub>**Institution:** Berkeley<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.13595v2)</div> |
| <span style='display: inline-block; width: 42px;'>10-19</span> | **Automatic Hallucination Assessment for Aligned Large Language Models via Transferable Adversarial Attacks**<br><sub>**Institution:** University of Pennsylvania, Microsoft Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.12516.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>10-19</span> | **Large Language Models Help Humans Verify Truthfulness -- Except When They Are Convincingly Wrong**<br><sub>**Institution:** Stanford University, University of Maryland<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.12558.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>10-05</span> | **Evaluating Hallucinations in Chinese Large Language Models**<br><sub>**Institution:** Fudan University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.03368.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>10-02</span> | **Tool-Augmented Reward Modeling**<br><sub>**Institution:** Zhejiang University, Baidu<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.01045.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>10-02</span> | **LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples**<br><sub>**Institution:** Peking University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.01469.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-30</span> | **AutoHall: Automated Hallucination Dataset Generation for Large Language Models**<br><sub>**Institution:** Shanghai Jiao Tong University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2310.00259.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-28</span> | **Hallucination Reduction in Long Input Text Summarization**<br><sub>**Institution:** Jadavpur University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2309.16781.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-25</span> | **Aligning Large Multimodal Models with Factually Augmented RLHF**<br><sub>**Institution:** UC Berkeley, CMU<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2309.14525.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-20</span> | **Chain-of-Verification Reduces Hallucination in Large Language Models**<br><sub>**Institution:** Meta AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2309.11495.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-18</span> | **Summarization is (Almost) Dead**<br><sub>**Institution:** Peking University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2309.09558.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>08-22</span> | **Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models**<br><sub>**Institution:** University of Pittsburgh, Pittsburgh, TikTok<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2308.11764v2.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>07-31</span> | **Evaluating Correctness and Faithfulness of Instruction-Following Models for Question Answering**<br><sub>**Institution:** Jadavpur University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2307.16877.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>06-09</span> | **Judging LLM-as-a-judge with MT-Bench and Chatbot Arena**<br><sub>**Institution:** UC Berkeley<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2306.05685.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-26</span> | **Training Socially Aligned Language Models on Simulated Social Interactions**<br><sub>**Institution:** Google DeepMind<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.16960.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-24</span> | **Trusting Your Evidence: Hallucinate Less with Context-aware Decoding**<br><sub>**Institution:** University of Washington<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.14739.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-22</span> | **LM vs LM: Detecting Factual Errors via Cross Examination**<br><sub>**Institution:** Google DeepMind<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.13281.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>05-18</span> | **LIMA: Less Is More for Alignment**<br><sub>**Institution:** Meta AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.11206.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>03-23</span> | **FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation**<br><sub>**Institution:** University of Washington<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.14251.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>03-08</span> | **HistAlign: Improving Context Dependency in Language Generation by Aligning with History**<br><sub>**Institution:** UNC Chapel Hill<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.04782.pdf)</div> |
<a name='Application'></a>
### Application

| &nbsp;Date&nbsp; | Paper | Links |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>01-17</span> | **Vlogger: Make Your Dream A Vlog**<br><sub>**Institution:** Shanghai Jiao Tong University, Shanghai AI Laboratory, Shenzhen Institute of Advanced Technology Chinese Academy of Sciences<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.09414v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **SpecGen: Automated Generation of Formal Program Specifications via Large Language Models**<br><sub>**Institution:** Nanjing University, Nanyang Technological University, Singapore Management University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08807v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **TestSpark: IntelliJ IDEA's Ultimate Test Generation Companion**<br><sub>**Institution:** JetBrains Research, Delft University of Technology<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06580v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape**<br><sub>**Institution:** Tsinghua University, University of Maryland, Beijing Xicheng Educational Research Institute  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06431v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **Teaching Code LLMs to Use Autocompletion Tools in Repository-Level Code Generation**<br><sub>**Institution:** Nanyang Technological University, Fudan University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06391v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Prompting Large Language Models for Recommender Systems: A Comprehensive Framework and Empirical Analysis**<br><sub>**Institution:** Renmin University of China, Beijing Key Laboratory of Big Data Management and Analysis Methods, Meituan Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04997v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-10</span> | **Leveraging Print Debugging to Improve Code Generation in Large Language Models**<br><sub>**Institution:** Zhejiang University, ByteDance<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05319v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-08</span> | **MARG: Multi-Agent Review Generation for Scientific Papers**<br><sub>**Institution:** Northwestern University, The Hebrew University of Jerusalem, Allen Institute for AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04259v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-05</span> | **Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache**<br><sub>**Institution:** Alibaba Group, Shanghai Jiao Tong University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02669v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **Using LLM to select the right SQL Query from candidates**<br><sub>**Institution:** Peking University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02115v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-04</span> | **LLM Augmented LLMs: Expanding Capabilities through Composition**<br><sub>**Institution:** Google Research, Google DeepMind<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.02412v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-03</span> | **MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries**<br><sub>**Institution:** Indian Institute of Technology Patna, Stanford University, Amazon GenAI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01596v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-03</span> | **Social Media Ready Caption Generation for Brands**<br><sub>**Institution:** Adobe Research India<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01637v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **DB-GPT: Empowering Database Interactions with Private Large Language Models**<br><sub>**Institution:** Alibaba Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17449v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **Building Efficient Universal Classifiers with Natural Language Inference**<br><sub>**Institution:** Vrije Universiteit Amsterdam, University of London Royal Holloway, Hugging Face<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17543v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-29</span> | **The Right Prompts for the Job: Repair Code-Review Defects with Large Language Model**<br><sub>**Institution:** Ant Group, Nanjing University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17485v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **Conversational Question Answering with Reformulations over Knowledge Graph**<br><sub>**Institution:** University of Illinois at Urbana-Champaign, Amazon<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.17269v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-27</span> | **Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges**<br><sub>**Institution:** Shanghai Jiao Tong University (SJTU)<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08664v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **RecRanker: Instruction Tuning Large Language Model as Ranker for Top-k Recommendation**<br><sub>**Institution:** City University of Hong Kong, The Chinese University of Hong Kong, Hangdian University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16018v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **YAYI 2: Multilingual Open-Source Large Language Models**<br><sub>**Institution:** Beijing Wenge Technology Co. Ltd., Institute of Automation Chinese Academy of Sciences<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14862v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Generative Multimodal Models are In-Context Learners**<br><sub>**Institution:** Beijing Academy of Artificial Intelligence, Tsinghua University, Peking University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13286v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Lampr: Boosting the Effectiveness of Language-Generic Program Reduction via Large Language Models**<br><sub>**Institution:** University of Waterloo, The Hong Kong University of Science and Technology, Concordia University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13064v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-19</span> | **Text-Conditioned Resampler For Long Form Video Understanding**<br><sub>**Institution:** University of Oxford, Google, Google DeepMind<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11897v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Towards Better Serialization of Tabular Data for Few-shot Classification with Large Language Models**<br><sub>**Institution:** Carnegie Mellon University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12464v2)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **MAC-SQL: Multi-Agent Collaboration for Text-to-SQL**<br><sub>**Institution:** Beihang University, Tencent Cloud AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11242v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **GSVA: Generalized Segmentation via Multimodal Large Language Models**<br><sub>**Institution:** Tsinghua University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10103v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Auto MC-Reward: Automated Dense Reward Design with Large Language Models for Minecraft**<br><sub>**Institution:** CUHK-SenseTime Joint Laboratory, Shanghai AI Laboratory, Tsinghua University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09238v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **CogAgent: A Visual Language Model for GUI Agents**<br><sub>**Institution:** Tsinghua University, Zhipu AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08914v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **StemGen: A music generation model that listens**<br><sub>**Institution:** SAMI, ByteDance Inc.<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08723v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention**<br><sub>**Institution:** The Swiss AI Lab IDSIA USI & SUPSI, AI Initiative KAUST, Center for Brain Science Harvard University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07987v2)</div> |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **E&V: Prompting Large Language Models to Perform Static Analysis by Pseudo-code Execution and Verification**<br><sub>**Institution:** UC Riverside, Microsoft Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08477v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-13</span> | **Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision**<br><sub>**Institution:** Peking University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08056v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **LLM in a flash: Efficient Large Language Model Inference with Limited Memory**<br><sub>**Institution:** Apple<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.11514v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Oracle-based Protocol Testing with Eywa**<br><sub>**Institution:** Microsoft Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06875v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Can Large Language Models Serve as Rational Players in Game Theory? A Systematic Analysis**<br><sub>**Institution:** Shanghai Jiao Tong University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05488v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-07</span> | **Generating Illustrated Instructions**<br><sub>**Institution:** GenAI Meta, Columbia University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.04552v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Holmes: Towards Distributed Training Across Clusters with Heterogeneous NIC Environment**<br><sub>**Institution:** Zhejiang Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03549v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **OneLLM: One Framework to Align All Modalities with Language**<br><sub>**Institution:** MMLab The Chinese University of Hong Kong, Shanghai Artificial Intelligence Laboratory<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03700v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education**<br><sub>**Institution:** Carnegie Mellon University  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03173v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **LLMs Accelerate Annotation for Medical Information Extraction**<br><sub>**Institution:** Google Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02296v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-02</span> | **Large Language Models Are Zero-Shot Text Classifiers**<br><sub>**Institution:** Florida Atlantic University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01044v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Beyond ChatBots: ExploreLLM for Structured Thoughts and Personalized Model Responses**<br><sub>**Institution:** Google<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00763v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-01</span> | **Improve Supervised Representation Learning with Masked Image Modeling**<br><sub>**Institution:** Google Research, OpenAI  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00950v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **Unnatural Error Correction: GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text**<br><sub>**Institution:** The University of Tokyo<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18805v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **MicroCinema: A Divide-and-Conquer Approach for Text-to-Video Generation**<br><sub>**Institution:** University of Science and Technology of China, Microsoft Research Asia<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18829v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-30</span> | **PoseGPT: Chatting about 3D Human Pose**<br><sub>**Institution:** Max Planck Institute for Intelligent Systems, Meshcapade<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.18836v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **Large Language Models for Networking: Applications, Enabling Techniques, and Challenges**<br><sub>**Institution:** BUPT<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17474v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-29</span> | **How to Build an AI Tutor that Can Adapt to Any Course and Provide Accurate Answers Using Large Language Model and Retrieval-Augmented Generation**<br><sub>**Institution:** The Education University of Hong Kong<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17696v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **LLaFS: When Large-Language Models Meet Few-Shot Segmentation**<br><sub>**Institution:** Singapore University of Technology and Design, Zhejiang University <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16926v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation**<br><sub>**Institution:** Alibaba Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.17117v1)</div><div style='min-width:85px;'>[![Blog](https://img.shields.io/badge/Blog-Posts-yellow?logo=rss)](https://humanaigc.github.io/animate-anyone/)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Can Generalist Foundation Models Outcompete Special-Purpose Tuning? Case Study in Medicine**<br><sub>**Institution:** Microsoft<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16452v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **ChatGPT's One-year Anniversary: Are Open-Source Large Language Models Catching up?**<br><sub>**Institution:** Nanyang Technological University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16989v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **LucidDreamer: Domain-free Generation of 3D Gaussian Splatting Scenes**<br><sub>**Institution:** ASRI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13384v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **FusionFrames: Efficient Architectural Aspects for Text-to-Video Generation Pipeline**<br><sub>**Institution:** Sber AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13073v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **XAGen: 3D Expressive Human Avatars Generation**<br><sub>**Institution:** National University of Singapore, ByteDance<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13574v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **AcademicGPT: Empowering Academic Research**<br><sub>**Institution:** International Digital Economy Academy<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12315v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **A Survey on Multimodal Large Language Models for Autonomous Driving**<br><sub>**Institution:** Purdue University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12320v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-13</span> | **Can LLMs Patch Security Issues?**<br><sub>**Institution:** School of Computer Science Atlanta<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.00024v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-05</span> | **ChaTA: Towards an Intelligent Question-Answer Teaching Assistant using Open-Source LLMs**<br><sub>**Institution:** Cornell University, Microsoft Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.02775v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-01</span> | **LLMRec: Large Language Models with Graph Augmentation for Recommendation**<br><sub>**Institution:** University of Hong Kong, Baidu<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.00423v5)</div> |
| <span style='display: inline-block; width: 42px;'>10-10</span> | **GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models**<br><sub>**Institution:** Microsoft Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2310.06225v2)</div> |
| <span style='display: inline-block; width: 42px;'>08-18</span> | **Learning Representations on Logs for AIOps**<br><sub>**Institution:** IBM Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2308.11526v1)</div> |
<a name='Pre-training-and-Instruction-Fine-tuning'></a>
### Pre-training and Instruction Fine-tuning

| &nbsp;Date&nbsp; | Paper | Links |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>01-17</span> | **ReFT: Reasoning with Reinforced Fine-Tuning**<br><sub>**Institution:** ByteDance Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08967v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-16</span> | **Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation**<br><sub>**Institution:** Johns Hopkins University, Microsoft<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.08417v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-15</span> | **MAPLE: Multilingual Evaluation of Parameter Efficient Finetuning of Large Language Models**<br><sub>**Institution:** Microsoft Research India<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07598v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **APAR: LLMs Can Do Auto-Parallel Auto-Regressive Decoding**<br><sub>**Institution:** Tsinghua University, Zhipu AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06761v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-12</span> | **An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models**<br><sub>**Institution:** University of Washington Seattle, University of Wisconsin-Madison, Stanford University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06692v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint**<br><sub>**Institution:** Gaoling School of Artificial Intelligence, Renmin University of China; School of Information, Renmin University of China; Kuaishou Technology, Beijing China.<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.06081v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-26</span> | **A Prompt Learning Framework for Source Code Summarization**<br><sub>**Institution:** Nanyang Technological University, Tencent Inc., Nanjing University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.16066v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Plan, Posture and Go: Towards Open-World Text-to-Motion Generation**<br><sub>**Institution:** Tsinghua University, Microsoft Research Asia<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14828v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **Generative AI Beyond LLMs: System Implications of Multi-Modal Generation**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14385v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Time is Encoded in the Weights of Finetuned Language Models**<br><sub></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.13401v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Mini-GPTs: Efficient Large Language Models through Contextual Pruning**<br><sub>**Institution:** Massachusetts Institute of Technology<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12682v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-20</span> | **Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy**<br><sub>**Institution:** Ant Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.12728v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-15</span> | **The Art of Balancing: Revolutionizing Mixture of Experts for Maintaining World Knowledge in Language Model Alignment**<br><sub>**Institution:** NLP Group Fudan University, Hikvision Inc  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.09979v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-14</span> | **Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention**<br><sub>**Institution:** Tencent AI Lab Seattle<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.08618v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-12</span> | **VILA: On Pre-training for Visual Language Models**<br><sub>**Institution:** NVIDIA, MIT  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.07533v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-11</span> | **Federated Full-Parameter Tuning of Billion-Sized Language Models with Communication Cost under 18 Kilobytes**<br><sub>**Institution:** Zhejiang University, Alibaba Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.06353v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Sim-GPT: Text Similarity via GPT Annotated Data**<br><sub>**Institution:** Shannon.AI, Zhejiang University, Bytedance<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05603v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **Agile-Quant: Activation-Guided Quantization for Faster Inference of LLMs on the Edge**<br><sub>**Institution:** Northeastern University, Oracle<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05693v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Controllable Human-Object Interaction Synthesis**<br><sub>**Institution:** Stanford University, FAIR Meta<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03913v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-05</span> | **RankZephyr: Effective and Robust Zero-Shot Listwise Reranking is a Breeze!**<br><sub>**Institution:** University of Waterloo<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02724v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Training Chain-of-Thought via Latent-Variable Inference**<br><sub>**Institution:** Google<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02179v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **RankingGPT: Empowering Large Language Models in Text Ranking with Progressive Enhancement**<br><sub>**Institution:** Alibaba Group<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16720v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Prompting in Autoregressive Large Language Models**<br><sub>**Institution:** George Mason University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03740v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-23</span> | **Diffusion Model Alignment Using Direct Preference Optimization**<br><sub>**Institution:** Nikhil Naik, Stanford University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12908v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-22</span> | **LIMIT: Less Is More for Instruction Tuning Across Evaluation Paradigms**<br><sub>**Institution:** Princeton University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.13133v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey**<br><sub>**Institution:** Nanjing University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12351v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks**<br><sub>**Institution:** University of Cambridge<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12786v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-18</span> | **Adapters: A Unified Library for Parameter-Efficient and Modular Transfer Learning**<br><sub>**Institution:** Technical University of Darmstadt, University of Cambridge<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.11077v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-17</span> | **Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2**<br><sub>**Institution:** Allen Institute for AI <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10702v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Exponentially Faster Language Modelling**<br><sub>**Institution:** ETH Zurich<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10770v2)</div> |
| <span style='display: inline-block; width: 42px;'>11-15</span> | **Memory Augmented Language Models through Mixture of Word Experts**<br><sub>**Institution:** Google Research<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.10768v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-14</span> | **Fine-tuning Language Models for Factuality**<br><sub>**Institution:** Stanford University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2311.08401.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>07-12</span> | **Instruction Mining: When Data Mining Meets Large Language Model Finetuning**<br><sub>**Institution:** Carnegie Mellon University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2307.06290.pdf)</div> |
<a name='Survey'></a>
### Survey

| &nbsp;Date&nbsp; | Paper | Links |
| --- | --- | --- |
| <span style='display: inline-block; width: 42px;'>01-15</span> | **The What, Why, and How of Context Length Extension Techniques in Large Language Models -- A Detailed Survey**<br><sub>**Institution:** Technology Innovation Institute UAE, Islamic University of Technology Bangladesh, Stanford University, Amazon GenAI, AI Institute University of South Carolina<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.07872v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-11</span> | **Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems**<br><sub>**Institution:** Zhongguancun Laboratory, Tsinghua University, Institute of Information Engineering Chinese Academy of Sciences<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.05778v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-09</span> | **Large Language Models for Robotics: Opportunities, Challenges, and Perspectives**<br><sub>**Institution:** Northwestern Polytechnical University, University of Georgia, Shaanxi Normal University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.04334v1)</div> |
| <span style='display: inline-block; width: 42px;'>01-02</span> | **A Comprehensive Survey of Hallucination Mitigation Techniques in Large Language Models**<br><sub>**Institution:** Islamic University of Technology Bangladesh, University of South Carolina, Stanford University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2401.01313v2)</div> |
| <span style='display: inline-block; width: 42px;'>12-22</span> | **A Survey of Reinforcement Learning from Human Feedback**<br><sub>**Institution:** LMU Munich, Duke Kunshan University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.14925v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **From Google Gemini to OpenAI Q-Star: A Survey of Reshaping the Generative Artificial Intelligence (AI) Research Landscape**<br><sub>**Institution:** Cyberstronomy Pty Ltd, Academies Australasia Polytechnic, Massey University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10868v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-18</span> | **Retrieval-Augmented Generation for Large Language Models: A Survey**<br><sub>**Institution:** Shanghai Research Institute for Intelligent Autonomous Systems, Tongji University, Fudan University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10997v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-16</span> | **A Survey on Robotic Manipulation of Deformable Objects: Recent Advances, Open Challenges and New Frontiers**<br><sub>**Institution:** Tongji University, National Natural Science Foundation of China, Shanghai Municipal Science and Technology Major Project<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.10419v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-09</span> | **NLLG Quarterly arXiv Report 09/23: What are the most influential current AI Papers?**<br><sub>**Institution:** University of Mannheim, University of Bielefeld<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.05688v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-06</span> | **Efficient Large Language Models: A Survey**<br><sub>**Institution:** The Ohio State University, Google Research, Amazon AWS AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.03863v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **Data Management For Large Language Models: A Survey**<br><sub>**Institution:** Peking University, Huawei Noahâ€™s Ark Lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.01700v1)</div> |
| <span style='display: inline-block; width: 42px;'>12-04</span> | **A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly**<br><sub>**Institution:** Elsevier<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2312.02003v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-28</span> | **Graph Prompt Learning: A Comprehensive Survey and Beyond**<br><sub>**Institution:** The Chinese University of Hong Kong, Hong Kong University of Science and Technology, Fudan University  <br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.16534v1)</div> |
| <span style='display: inline-block; width: 42px;'>11-21</span> | **Prompting Frameworks for Large Language Models: A Survey**<br><sub>**Institution:** Zhejiang University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2311.12785v1)</div> |
| <span style='display: inline-block; width: 42px;'>10-16</span> | **A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future**<br><sub>**Institution:** Harbin Institute of Technology, Huawei<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2309.15402.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>09-03</span> | **Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models**<br><sub>**Institution:** Tencent AI lab<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2309.01219.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>06-01</span> | **Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation**<br><sub>**Institution:** Carnegie Mellon University<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.00955.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>03-31</span> | **A Survey of Large Language Models**<br><sub>**Institution:** Renmin University of China<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](http://arxiv.org/pdf/2303.18223v13)</div> |
| <span style='display: inline-block; width: 42px;'>03-15</span> | **GPT-4 Technical Report**<br><sub>**Institution:** OpenAI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2303.08774.pdf)</div> |
| <span style='display: inline-block; width: 42px;'>02-15</span> | **Augmented Language Models: a Survey**<br><sub>**Institution:** Meta AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2302.07842.pdf)</div> |
### Top Conferences

| &nbsp;Conference&nbsp; | Paper | Links |
| --- | --- | --- |
| EMNLP 2023 | ğŸ† **Best Long Paper** - **Label Words are Anchors: An Information Flow Perspective for Understanding In-Context Learning**<br><sub>**Institution:** Peking University, Tencent<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2305.14160.pdf)</div> |
| EMNLP 2023 | ğŸ† **Best Short Paper** - **Faster Minimum Bayes Risk Decoding with Confidence-based Pruning**<br><sub>**Institution:** University of Cambridge<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://arxiv.org/pdf/2311.14919.pdf)</div> |
| EMNLP 2023 | ğŸ† **Best Demo Paper** - **PaperMage: A Unified Toolkit for Processing, Representing, and Manipulating Visually-Rich Scientific Documents**<br><sub>**Institution:** Allen Institute for AI, Massachusetts Institute of Technology, University of California Berkeley<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://aclanthology.org/2023.emnlp-demo.45.pdf)</div> |
| EMNLP 2023 | ğŸ† **Best Theme Paper** - **Personalized Dense Retrieval on Global Index for Voice-enabled Conversational Systems**<br><sub>**Institution:** Amazon Alexa AI<br></sub>| <div style='min-width:85px;'>[![arXiv](https://img.shields.io/badge/arXiv-Paper-%23D2691E?logo=arxiv)](https://assets.amazon.science/ec/80/1ffa1dfb4cb9a9730e1df91493f9/personalized-dense-retrieval-on-global-index-for-voice-enabled-conversational-systems.pdf)</div> |

## Star History
<picture>
<source
    media="(prefers-color-scheme: dark)"
    srcset="
    https://api.star-history.com/svg?repos=xianshang33/llm-paper-daily&type=Date&theme=dark
    "
/>
<source
    media="(prefers-color-scheme: light)"
    srcset="
    https://api.star-history.com/svg?repos=xianshang33/llm-paper-daily&type=Date
    "
/>
<img
    alt="Star History Chart"
    src="https://api.star-history.com/svg?repos=xianshang33/llm-paper-daily&type=Date"
/>
</picture>
            